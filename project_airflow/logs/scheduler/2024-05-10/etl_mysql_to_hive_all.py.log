[2024-05-10T13:37:04.507+0000] {processor.py:161} INFO - Started process (PID=32) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:04.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:04.511+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:04.511+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:04.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:04.544+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:04.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:04.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:37:25.651+0000] {processor.py:161} INFO - Started process (PID=118) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:25.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:25.656+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:25.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:25.755+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:25.711+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:25.758+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:25.789+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.150 seconds
[2024-05-10T13:37:31.285+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:31.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:31.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:31.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:31.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:31.321+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:31.329+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:31.340+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.062 seconds
[2024-05-10T13:37:36.239+0000] {processor.py:161} INFO - Started process (PID=276) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:36.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:36.243+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:36.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:36.283+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:36.277+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:36.286+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:36.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:37:41.420+0000] {processor.py:161} INFO - Started process (PID=361) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:41.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:41.423+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:41.423+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:41.461+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:41.455+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:41.463+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:41.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.062 seconds
[2024-05-10T13:37:47.957+0000] {processor.py:161} INFO - Started process (PID=440) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:47.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:47.961+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:47.961+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:47.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:47.994+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:48.000+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:48.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T13:37:54.883+0000] {processor.py:161} INFO - Started process (PID=520) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:54.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:37:54.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:54.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:54.959+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:37:54.954+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:37:54.961+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:37:54.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T13:38:00.734+0000] {processor.py:161} INFO - Started process (PID=599) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:00.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:00.738+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:00.738+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:00.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:00.775+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:00.784+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:00.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:38:07.811+0000] {processor.py:161} INFO - Started process (PID=685) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:07.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:07.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:07.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:07.852+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:07.846+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:07.854+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:07.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.062 seconds
[2024-05-10T13:38:14.295+0000] {processor.py:161} INFO - Started process (PID=765) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:14.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:14.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:14.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:14.345+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:14.338+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:14.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:14.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:38:21.128+0000] {processor.py:161} INFO - Started process (PID=844) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:21.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:21.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:21.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:21.178+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:21.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:21.180+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:21.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T13:38:28.409+0000] {processor.py:161} INFO - Started process (PID=923) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:28.410+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:28.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:28.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:28.443+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:28.438+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:28.444+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:28.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.057 seconds
[2024-05-10T13:38:36.194+0000] {processor.py:161} INFO - Started process (PID=1002) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:36.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:36.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:36.198+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:36.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:36.237+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:36.247+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:36.259+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:38:42.100+0000] {processor.py:161} INFO - Started process (PID=1087) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:42.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:42.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:42.104+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:42.148+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:42.141+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:42.151+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:42.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T13:38:48.043+0000] {processor.py:161} INFO - Started process (PID=1166) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:48.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:48.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:48.046+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:48.082+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:48.078+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:48.084+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:48.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T13:38:54.302+0000] {processor.py:161} INFO - Started process (PID=1245) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:54.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:38:54.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:54.305+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:54.345+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:38:54.339+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:38:54.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:38:54.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:39:01.113+0000] {processor.py:161} INFO - Started process (PID=1324) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:01.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:01.118+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:01.117+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:01.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:01.157+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:01.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:01.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T13:39:07.494+0000] {processor.py:161} INFO - Started process (PID=1403) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:07.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:07.498+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:07.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:07.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:07.530+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:07.539+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:07.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:39:13.601+0000] {processor.py:161} INFO - Started process (PID=1488) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:13.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:13.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:13.605+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:13.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:13.638+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:13.648+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:13.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T13:39:19.137+0000] {processor.py:161} INFO - Started process (PID=1567) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:19.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:19.140+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:19.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:19.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:19.171+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:19.178+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:19.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T13:39:24.771+0000] {processor.py:161} INFO - Started process (PID=1646) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:24.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:24.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:24.774+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:24.807+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:24.802+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:24.809+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:24.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.059 seconds
[2024-05-10T13:39:31.778+0000] {processor.py:161} INFO - Started process (PID=1725) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:31.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:31.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:31.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:31.820+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:31.813+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:31.823+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:31.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:39:38.020+0000] {processor.py:161} INFO - Started process (PID=1810) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:38.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:38.024+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:38.023+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:38.057+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:38.052+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:38.059+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:38.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T13:39:43.578+0000] {processor.py:161} INFO - Started process (PID=1889) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:43.580+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:43.582+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:43.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:43.621+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:43.614+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:43.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:43.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T13:39:49.298+0000] {processor.py:161} INFO - Started process (PID=1968) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:49.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:49.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:49.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:49.338+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:49.332+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:49.340+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:49.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.062 seconds
[2024-05-10T13:39:56.417+0000] {processor.py:161} INFO - Started process (PID=2047) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:56.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:39:56.421+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:56.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:56.456+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:39:56.449+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:39:56.458+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:39:56.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T13:40:02.399+0000] {processor.py:161} INFO - Started process (PID=2126) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:02.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:02.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:02.403+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:02.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:02.436+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:02.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:02.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T13:40:09.737+0000] {processor.py:161} INFO - Started process (PID=2211) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:09.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:09.741+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:09.741+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:09.778+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:09.772+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:09.780+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:09.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T13:40:15.825+0000] {processor.py:161} INFO - Started process (PID=2290) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:15.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:15.829+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:15.828+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:15.872+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:15.865+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:15.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:15.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T13:40:21.521+0000] {processor.py:161} INFO - Started process (PID=2369) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:21.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:21.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:21.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:21.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:21.557+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:21.565+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:21.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T13:40:27.358+0000] {processor.py:161} INFO - Started process (PID=2448) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:27.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:27.361+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:27.361+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:27.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:27.395+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:27.402+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:27.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T13:40:32.995+0000] {processor.py:161} INFO - Started process (PID=2527) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:32.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:32.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:32.999+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:33.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:33.038+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:33.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:33.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T13:40:39.255+0000] {processor.py:161} INFO - Started process (PID=2612) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:39.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:39.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:39.257+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:39.292+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:39.285+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:39.294+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:39.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.058 seconds
[2024-05-10T13:40:45.321+0000] {processor.py:161} INFO - Started process (PID=2691) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:45.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:45.325+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:45.324+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:45.370+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:45.363+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:45.372+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:45.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:40:51.099+0000] {processor.py:161} INFO - Started process (PID=2770) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:51.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:51.103+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:51.102+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:51.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:51.139+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:51.148+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:51.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T13:40:56.621+0000] {processor.py:161} INFO - Started process (PID=2849) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:56.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:40:56.625+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:56.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:56.663+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:40:56.656+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:40:56.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:40:56.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T13:41:02.100+0000] {processor.py:161} INFO - Started process (PID=2928) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:02.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:02.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:02.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:02.139+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:02.133+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:02.141+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:02.152+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T13:41:08.464+0000] {processor.py:161} INFO - Started process (PID=3013) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:08.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:08.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:08.468+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:08.518+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:08.514+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:08.520+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:08.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:41:16.338+0000] {processor.py:161} INFO - Started process (PID=3092) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:16.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:16.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:16.342+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:16.382+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:16.377+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:16.385+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:16.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:41:22.335+0000] {processor.py:161} INFO - Started process (PID=3171) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:22.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:22.339+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:22.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:22.375+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:22.370+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:22.377+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:22.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.059 seconds
[2024-05-10T13:41:27.936+0000] {processor.py:161} INFO - Started process (PID=3250) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:27.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:27.940+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:27.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:27.979+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:27.974+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:27.981+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:27.994+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T13:41:33.598+0000] {processor.py:161} INFO - Started process (PID=3329) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:33.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:33.602+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:33.601+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:33.643+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:33.635+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:33.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:33.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:41:39.547+0000] {processor.py:161} INFO - Started process (PID=3414) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:39.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:39.551+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:39.550+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:39.586+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:39.581+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:39.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:39.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T13:41:46.322+0000] {processor.py:161} INFO - Started process (PID=3493) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:46.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:46.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:46.329+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:46.379+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:46.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:46.381+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:46.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:41:53.296+0000] {processor.py:161} INFO - Started process (PID=3572) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:53.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:53.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:53.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:53.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:53.323+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:53.332+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:53.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.055 seconds
[2024-05-10T13:41:58.899+0000] {processor.py:161} INFO - Started process (PID=3651) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:58.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:41:58.903+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:58.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:58.940+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:41:58.934+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:41:58.943+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:41:58.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T13:42:05.517+0000] {processor.py:161} INFO - Started process (PID=3730) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:05.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:05.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:05.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:05.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:05.548+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:05.556+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:05.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.056 seconds
[2024-05-10T13:42:11.504+0000] {processor.py:161} INFO - Started process (PID=3816) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:11.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:11.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:11.508+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:11.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:11.541+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:11.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:11.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:42:17.680+0000] {processor.py:161} INFO - Started process (PID=3895) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:17.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:17.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:17.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:17.722+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:17.715+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:17.724+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:17.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T13:42:25.006+0000] {processor.py:161} INFO - Started process (PID=3974) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:25.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:25.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:25.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:25.056+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:25.048+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:25.059+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:25.079+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:42:31.407+0000] {processor.py:161} INFO - Started process (PID=4053) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:31.409+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:31.411+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:31.411+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:31.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:31.443+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:31.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:31.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T13:42:37.171+0000] {processor.py:161} INFO - Started process (PID=4132) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:37.172+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:37.175+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:37.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:37.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:37.207+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:37.216+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:37.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T13:42:43.063+0000] {processor.py:161} INFO - Started process (PID=4217) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:43.064+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:43.066+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:43.066+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:43.098+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:43.093+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:43.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:43.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.056 seconds
[2024-05-10T13:42:48.941+0000] {processor.py:161} INFO - Started process (PID=4296) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:48.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:48.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:48.944+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:48.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:48.980+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:48.987+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:48.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:42:55.047+0000] {processor.py:161} INFO - Started process (PID=4375) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:55.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:42:55.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:55.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:55.099+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:42:55.093+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:42:55.102+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:42:55.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T13:43:02.912+0000] {processor.py:161} INFO - Started process (PID=4454) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:02.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:02.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:02.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:02.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:02.948+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:02.957+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:02.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T13:43:11.113+0000] {processor.py:161} INFO - Started process (PID=4540) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:11.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:11.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:11.117+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:11.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:11.157+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:11.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:11.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T13:43:19.205+0000] {processor.py:161} INFO - Started process (PID=4619) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:19.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:19.211+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:19.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:19.281+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:19.272+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:19.284+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:19.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T13:43:27.424+0000] {processor.py:161} INFO - Started process (PID=4698) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:27.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:27.428+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:27.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:27.470+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:27.463+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:27.472+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:27.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T13:43:34.753+0000] {processor.py:161} INFO - Started process (PID=4777) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:34.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:34.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:34.757+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:34.801+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:34.795+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:34.804+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:34.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:43:41.870+0000] {processor.py:161} INFO - Started process (PID=4862) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:41.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:41.874+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:41.874+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:41.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:41.921+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:41.931+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:41.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T13:43:48.630+0000] {processor.py:161} INFO - Started process (PID=4941) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:48.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:48.634+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:48.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:48.680+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:48.674+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:48.683+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:48.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:43:55.092+0000] {processor.py:161} INFO - Started process (PID=5020) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:55.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:43:55.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:55.096+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:55.144+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:43:55.138+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:43:55.146+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:43:55.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T13:44:01.834+0000] {processor.py:161} INFO - Started process (PID=5099) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:01.836+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:01.838+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:01.838+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:01.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:01.876+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:01.886+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:01.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T13:44:10.096+0000] {processor.py:161} INFO - Started process (PID=5184) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:10.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:10.101+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:10.101+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:10.142+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:10.136+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:10.144+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:10.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:44:17.070+0000] {processor.py:161} INFO - Started process (PID=5263) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:17.072+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:17.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:17.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:17.115+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:17.109+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:17.117+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:17.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T13:44:23.378+0000] {processor.py:161} INFO - Started process (PID=5342) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:23.380+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:23.382+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:23.382+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:23.424+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:23.417+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:23.427+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:23.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:44:29.425+0000] {processor.py:161} INFO - Started process (PID=5421) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:29.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:29.428+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:29.428+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:29.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:29.462+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:29.471+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:29.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:44:35.580+0000] {processor.py:161} INFO - Started process (PID=5500) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:35.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:35.584+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:35.584+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:35.630+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:35.623+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:35.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:35.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:44:44.337+0000] {processor.py:161} INFO - Started process (PID=5585) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:44.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:44.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:44.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:44.383+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:44.376+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:44.385+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:44.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:44:52.200+0000] {processor.py:161} INFO - Started process (PID=5664) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:52.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:52.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:52.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:52.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:52.247+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:52.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:52.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T13:44:59.349+0000] {processor.py:161} INFO - Started process (PID=5743) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:59.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:44:59.353+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:59.353+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:59.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:44:59.392+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:44:59.402+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:44:59.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:45:06.736+0000] {processor.py:161} INFO - Started process (PID=5822) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:06.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:06.740+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:06.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:06.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:06.778+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:06.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:06.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:45:14.677+0000] {processor.py:161} INFO - Started process (PID=5907) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:14.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:14.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:14.680+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:14.723+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:14.716+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:14.725+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:14.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:45:25.172+0000] {processor.py:161} INFO - Started process (PID=5987) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:25.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:25.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:25.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:25.218+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:25.211+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:25.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:25.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T13:45:33.261+0000] {processor.py:161} INFO - Started process (PID=6066) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:33.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:33.265+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:33.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:33.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:33.297+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:33.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:33.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:45:41.455+0000] {processor.py:161} INFO - Started process (PID=6152) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:41.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:41.459+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:41.459+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:41.504+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:41.496+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:41.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:41.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:45:48.647+0000] {processor.py:161} INFO - Started process (PID=6232) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:48.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:48.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:48.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:48.701+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:48.694+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:48.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:48.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T13:45:58.270+0000] {processor.py:161} INFO - Started process (PID=6311) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:58.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:45:58.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:58.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:58.349+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:45:58.338+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:45:58.352+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:45:58.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T13:46:06.446+0000] {processor.py:161} INFO - Started process (PID=6390) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:06.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:06.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:06.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:06.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:06.503+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:06.516+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:06.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T13:46:14.339+0000] {processor.py:161} INFO - Started process (PID=6475) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:14.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:14.344+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:14.343+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:14.402+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:14.394+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:14.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:14.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T13:46:20.977+0000] {processor.py:161} INFO - Started process (PID=6554) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:20.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:20.981+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:20.981+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:21.025+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:21.017+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:21.027+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:21.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:46:28.312+0000] {processor.py:161} INFO - Started process (PID=6633) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:28.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:28.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:28.316+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:28.368+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:28.361+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:28.371+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:28.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T13:46:35.843+0000] {processor.py:161} INFO - Started process (PID=6712) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:35.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:35.848+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:35.847+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:35.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:35.884+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:35.893+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:35.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:46:44.761+0000] {processor.py:161} INFO - Started process (PID=6798) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:44.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:44.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:44.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:44.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:44.793+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:44.801+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:44.814+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T13:46:51.805+0000] {processor.py:161} INFO - Started process (PID=6878) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:51.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:51.809+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:51.808+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:51.855+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:51.848+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:51.858+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:51.871+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:46:58.404+0000] {processor.py:161} INFO - Started process (PID=6957) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:58.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:46:58.409+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:58.408+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:58.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:46:58.446+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:46:58.455+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:46:58.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T13:47:06.526+0000] {processor.py:161} INFO - Started process (PID=7036) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:06.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:06.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:06.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:06.571+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:06.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:06.573+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:06.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T13:47:16.070+0000] {processor.py:161} INFO - Started process (PID=7122) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:16.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:16.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:16.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:16.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:16.150+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:16.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:16.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T13:47:24.048+0000] {processor.py:161} INFO - Started process (PID=7201) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:24.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:24.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:24.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:24.102+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:24.095+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:24.105+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:24.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T13:47:32.784+0000] {processor.py:161} INFO - Started process (PID=7280) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:32.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:32.790+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:32.790+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:32.851+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:32.841+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:32.853+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:32.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T13:47:41.099+0000] {processor.py:161} INFO - Started process (PID=7365) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:41.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:41.103+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:41.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:41.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:41.138+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:41.148+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:41.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:47:50.150+0000] {processor.py:161} INFO - Started process (PID=7444) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:50.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:50.154+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:50.154+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:50.198+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:50.191+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:50.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:50.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:47:57.361+0000] {processor.py:161} INFO - Started process (PID=7523) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:57.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:47:57.367+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:57.367+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:57.447+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:47:57.433+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:47:57.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:47:57.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T13:48:05.051+0000] {processor.py:161} INFO - Started process (PID=7602) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:05.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:05.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:05.055+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:05.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:05.089+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:05.100+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:05.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T13:48:12.755+0000] {processor.py:161} INFO - Started process (PID=7687) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:12.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:12.759+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:12.759+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:12.802+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:12.795+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:12.804+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:12.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T13:48:20.487+0000] {processor.py:161} INFO - Started process (PID=7766) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:20.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:20.494+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:20.493+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:20.541+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:20.534+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:20.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:20.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T13:48:28.561+0000] {processor.py:161} INFO - Started process (PID=7845) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:28.563+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:28.565+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:28.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:28.614+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:28.607+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:28.617+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:28.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:48:36.655+0000] {processor.py:161} INFO - Started process (PID=7924) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:36.657+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:36.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:36.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:36.709+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:36.702+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:36.711+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:36.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:48:44.231+0000] {processor.py:161} INFO - Started process (PID=8009) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:44.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:44.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:44.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:44.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:44.275+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:44.284+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:44.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:48:53.047+0000] {processor.py:161} INFO - Started process (PID=8088) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:53.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:48:53.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:53.054+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:53.105+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:48:53.097+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:48:53.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:48:53.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T13:49:02.627+0000] {processor.py:161} INFO - Started process (PID=8167) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:02.628+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:02.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:02.631+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:02.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:02.670+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:02.678+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:02.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:49:12.978+0000] {processor.py:161} INFO - Started process (PID=8252) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:12.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:12.982+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:12.981+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:13.022+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:13.014+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:13.024+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:13.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:49:23.368+0000] {processor.py:161} INFO - Started process (PID=8331) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:23.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:23.372+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:23.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:23.421+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:23.413+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:23.424+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:23.438+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:49:35.043+0000] {processor.py:161} INFO - Started process (PID=8410) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:35.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:35.049+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:35.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:35.122+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:35.114+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:35.125+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:35.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T13:49:47.915+0000] {processor.py:161} INFO - Started process (PID=8495) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:47.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:47.920+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:47.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:47.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:47.957+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:47.966+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:47.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:49:55.952+0000] {processor.py:161} INFO - Started process (PID=8575) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:55.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:49:55.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:55.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:55.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:49:55.991+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:49:55.998+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:49:56.011+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:50:04.632+0000] {processor.py:161} INFO - Started process (PID=8654) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:04.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:04.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:04.637+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:04.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:04.676+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:04.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:04.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:50:12.807+0000] {processor.py:161} INFO - Started process (PID=8740) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:12.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:12.811+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:12.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:12.850+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:12.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:12.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:12.865+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:50:19.979+0000] {processor.py:161} INFO - Started process (PID=8819) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:19.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:19.982+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:19.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:20.024+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:20.017+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:20.026+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:20.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T13:50:26.475+0000] {processor.py:161} INFO - Started process (PID=8898) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:26.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:26.479+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:26.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:26.525+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:26.518+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:26.528+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:26.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:50:36.315+0000] {processor.py:161} INFO - Started process (PID=8977) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:36.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:36.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:36.319+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:36.366+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:36.359+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:36.369+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:36.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:50:43.654+0000] {processor.py:161} INFO - Started process (PID=9062) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:43.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:43.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:43.658+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:43.698+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:43.693+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:43.700+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:43.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:50:50.962+0000] {processor.py:161} INFO - Started process (PID=9141) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:50.963+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:50.966+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:50.965+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:51.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:51.002+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:51.013+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:51.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:50:57.672+0000] {processor.py:161} INFO - Started process (PID=9220) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:57.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:50:57.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:57.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:57.717+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:50:57.711+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:50:57.719+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:50:57.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T13:51:07.086+0000] {processor.py:161} INFO - Started process (PID=9299) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:07.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:07.090+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:07.090+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:07.136+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:07.131+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:07.139+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:07.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T13:51:15.679+0000] {processor.py:161} INFO - Started process (PID=9385) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:15.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:15.683+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:15.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:15.725+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:15.719+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:15.728+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:15.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T13:51:23.683+0000] {processor.py:161} INFO - Started process (PID=9464) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:23.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:23.687+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:23.687+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:23.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:23.724+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:23.734+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:23.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:51:30.566+0000] {processor.py:161} INFO - Started process (PID=9543) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:30.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:30.571+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:30.570+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:30.622+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:30.615+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:30.625+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:30.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T13:51:39.246+0000] {processor.py:161} INFO - Started process (PID=9628) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:39.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:39.250+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:39.250+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:39.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:39.283+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:39.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:39.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T13:51:47.595+0000] {processor.py:161} INFO - Started process (PID=9708) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:47.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:47.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:47.599+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:47.643+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:47.636+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:47.646+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:47.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:51:54.948+0000] {processor.py:161} INFO - Started process (PID=9788) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:54.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:51:54.952+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:54.951+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:54.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:51:54.992+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:51:55.002+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:51:55.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T13:52:01.788+0000] {processor.py:161} INFO - Started process (PID=9867) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:01.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:01.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:01.792+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:01.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:01.835+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:01.844+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:01.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T13:52:08.974+0000] {processor.py:161} INFO - Started process (PID=9946) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:08.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:08.978+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:08.978+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:09.032+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:09.023+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:09.035+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:09.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T13:52:16.374+0000] {processor.py:161} INFO - Started process (PID=10031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:16.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:16.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:16.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:16.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:16.423+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:16.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:16.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T13:52:24.623+0000] {processor.py:161} INFO - Started process (PID=10110) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:24.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:24.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:24.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:24.672+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:24.666+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:24.674+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:24.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:52:32.911+0000] {processor.py:161} INFO - Started process (PID=10189) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:32.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:32.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:32.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:32.961+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:32.954+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:32.963+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:32.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T13:52:40.076+0000] {processor.py:161} INFO - Started process (PID=10274) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:40.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:40.080+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:40.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:40.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:40.118+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:40.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:40.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:52:47.247+0000] {processor.py:161} INFO - Started process (PID=10353) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:47.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:47.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:47.252+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:47.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:47.297+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:47.306+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:47.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T13:52:57.230+0000] {processor.py:161} INFO - Started process (PID=10432) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:57.232+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:52:57.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:57.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:57.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:52:57.280+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:52:57.290+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:52:57.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T13:53:05.992+0000] {processor.py:161} INFO - Started process (PID=10511) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:05.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:05.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:05.997+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:06.051+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:06.041+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:06.053+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:06.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T13:53:14.004+0000] {processor.py:161} INFO - Started process (PID=10597) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:14.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:14.008+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:14.008+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:14.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:14.046+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:14.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:14.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:53:20.949+0000] {processor.py:161} INFO - Started process (PID=10676) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:20.950+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:20.955+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:20.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:21.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:20.996+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:21.006+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:21.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T13:53:28.589+0000] {processor.py:161} INFO - Started process (PID=10755) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:28.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:28.594+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:28.593+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:28.648+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:28.640+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:28.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:28.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T13:53:37.242+0000] {processor.py:161} INFO - Started process (PID=10834) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:37.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:37.247+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:37.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:37.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:37.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:37.297+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:37.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T13:53:44.899+0000] {processor.py:161} INFO - Started process (PID=10919) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:44.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:44.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:44.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:44.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:44.941+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:44.950+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:44.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T13:53:51.361+0000] {processor.py:161} INFO - Started process (PID=10998) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:51.362+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:51.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:51.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:51.406+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:51.401+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:51.408+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:51.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T13:53:58.399+0000] {processor.py:161} INFO - Started process (PID=11077) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:58.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:53:58.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:58.403+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:58.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:53:58.443+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:53:58.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:53:58.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:54:05.857+0000] {processor.py:161} INFO - Started process (PID=11156) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:05.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:05.862+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:05.862+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:05.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:05.904+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:05.913+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:05.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:54:12.943+0000] {processor.py:161} INFO - Started process (PID=11241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:12.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:12.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:12.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:12.989+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:12.982+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:12.991+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:13.005+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T13:54:20.148+0000] {processor.py:161} INFO - Started process (PID=11320) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:20.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:20.152+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:20.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:20.202+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:20.194+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:20.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:20.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:54:26.937+0000] {processor.py:161} INFO - Started process (PID=11399) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:26.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:26.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:26.941+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:26.987+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:26.979+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:26.990+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:27.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:54:34.774+0000] {processor.py:161} INFO - Started process (PID=11478) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:34.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:34.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:34.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:34.818+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:34.813+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:34.821+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:34.835+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T13:54:42.964+0000] {processor.py:161} INFO - Started process (PID=11563) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:42.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:42.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:42.975+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:43.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:43.042+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:43.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:43.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T13:54:50.358+0000] {processor.py:161} INFO - Started process (PID=11643) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:50.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:50.364+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:50.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:50.409+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:50.403+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:50.411+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:50.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:54:57.957+0000] {processor.py:161} INFO - Started process (PID=11722) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:57.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:54:57.961+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:57.961+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:58.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:54:58.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:54:58.012+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:54:58.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:55:05.276+0000] {processor.py:161} INFO - Started process (PID=11801) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:05.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:05.280+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:05.279+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:05.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:05.317+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:05.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:05.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T13:55:12.307+0000] {processor.py:161} INFO - Started process (PID=11886) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:12.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:12.314+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:12.313+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:12.367+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:12.360+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:12.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:12.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T13:55:21.518+0000] {processor.py:161} INFO - Started process (PID=11965) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:21.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:21.526+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:21.525+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:21.584+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:21.577+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:21.588+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:21.612+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T13:55:30.670+0000] {processor.py:161} INFO - Started process (PID=12044) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:30.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:30.674+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:30.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:30.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:30.721+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:30.731+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:30.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T13:55:38.136+0000] {processor.py:161} INFO - Started process (PID=12123) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:38.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:38.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:38.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:38.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:38.180+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:38.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:38.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T13:55:44.863+0000] {processor.py:161} INFO - Started process (PID=12208) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:44.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:44.867+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:44.867+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:44.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:44.908+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:44.918+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:44.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:55:52.449+0000] {processor.py:161} INFO - Started process (PID=12287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:52.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:55:52.455+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:52.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:52.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:55:52.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:55:52.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:55:52.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T13:56:00.110+0000] {processor.py:161} INFO - Started process (PID=12366) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:00.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:00.113+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:00.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:00.156+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:00.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:00.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:00.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T13:56:08.620+0000] {processor.py:161} INFO - Started process (PID=12445) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:08.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:08.625+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:08.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:08.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:08.673+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:08.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:08.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T13:56:17.481+0000] {processor.py:161} INFO - Started process (PID=12530) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:17.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:17.485+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:17.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:17.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:17.527+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:17.551+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:17.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T13:56:25.099+0000] {processor.py:161} INFO - Started process (PID=12609) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:25.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:25.105+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:25.104+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:25.149+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:25.143+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:25.152+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:25.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T13:56:32.912+0000] {processor.py:161} INFO - Started process (PID=12688) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:32.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:32.917+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:32.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:32.958+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:32.952+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:32.961+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:32.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T13:56:41.253+0000] {processor.py:161} INFO - Started process (PID=12773) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:41.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:41.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:41.257+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:41.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:41.296+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:41.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:41.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T13:56:48.559+0000] {processor.py:161} INFO - Started process (PID=12852) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:48.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:56:48.565+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:48.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:48.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:56:48.638+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:56:48.667+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:56:48.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T13:57:08.095+0000] {processor.py:161} INFO - Started process (PID=12931) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:08.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:57:08.102+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:08.101+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:08.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:08.196+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:57:08.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:08.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.177 seconds
[2024-05-10T13:57:23.874+0000] {processor.py:161} INFO - Started process (PID=13017) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:23.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:57:23.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:23.879+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:23.951+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:23.942+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:57:23.957+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:23.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T13:57:36.037+0000] {processor.py:161} INFO - Started process (PID=13096) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:36.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:57:36.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:36.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:36.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:36.081+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:57:36.089+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:36.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T13:57:53.293+0000] {processor.py:161} INFO - Started process (PID=13181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:53.295+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:57:53.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:53.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:53.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:57:53.352+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:57:53.363+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:57:53.388+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T13:58:07.719+0000] {processor.py:161} INFO - Started process (PID=13260) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:07.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:07.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:07.726+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:07.783+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:07.776+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:07.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:07.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T13:58:16.186+0000] {processor.py:161} INFO - Started process (PID=13346) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:16.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:16.191+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:16.191+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:16.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:16.238+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:16.250+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:16.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T13:58:24.504+0000] {processor.py:161} INFO - Started process (PID=13425) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:24.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:24.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:24.508+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:24.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:24.548+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:24.557+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:24.572+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T13:58:33.086+0000] {processor.py:161} INFO - Started process (PID=13504) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:33.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:33.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:33.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:33.142+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:33.135+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:33.145+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:33.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T13:58:41.064+0000] {processor.py:161} INFO - Started process (PID=13589) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:41.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:41.069+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:41.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:41.123+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:41.114+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:41.125+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:41.142+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T13:58:48.381+0000] {processor.py:161} INFO - Started process (PID=13668) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:48.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:48.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:48.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:48.435+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:48.427+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:48.437+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:48.453+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T13:58:56.121+0000] {processor.py:161} INFO - Started process (PID=13747) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:56.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:58:56.129+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:56.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:56.179+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:58:56.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:58:56.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:58:56.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T13:59:06.254+0000] {processor.py:161} INFO - Started process (PID=13826) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:06.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:06.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:06.262+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:06.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:06.318+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:06.333+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:06.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.115 seconds
[2024-05-10T13:59:16.425+0000] {processor.py:161} INFO - Started process (PID=13912) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:16.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:16.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:16.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:16.472+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:16.466+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:16.474+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:16.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T13:59:24.240+0000] {processor.py:161} INFO - Started process (PID=13992) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:24.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:24.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:24.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:24.293+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:24.284+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:24.295+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:24.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T13:59:34.011+0000] {processor.py:161} INFO - Started process (PID=14071) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:34.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:34.019+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:34.018+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:34.082+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:34.073+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:34.088+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:34.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.115 seconds
[2024-05-10T13:59:46.225+0000] {processor.py:161} INFO - Started process (PID=14159) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:46.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:46.232+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:46.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:46.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:46.280+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:46.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:46.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T13:59:56.708+0000] {processor.py:161} INFO - Started process (PID=14238) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:56.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T13:59:56.715+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:56.714+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:56.771+0000] {logging_mixin.py:188} INFO - [2024-05-10T13:59:56.763+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T13:59:56.774+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T13:59:56.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.107 seconds
[2024-05-10T14:00:06.680+0000] {processor.py:161} INFO - Started process (PID=14317) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:06.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:06.689+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:06.689+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:06.739+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:06.732+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:06.742+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:06.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:00:16.949+0000] {processor.py:161} INFO - Started process (PID=14402) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:16.951+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:16.955+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:16.954+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:17.012+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:17.004+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:17.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:17.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T14:00:26.303+0000] {processor.py:161} INFO - Started process (PID=14482) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:26.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:26.308+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:26.308+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:26.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:26.363+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:26.374+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:26.405+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.115 seconds
[2024-05-10T14:00:35.878+0000] {processor.py:161} INFO - Started process (PID=14561) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:35.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:35.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:35.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:35.925+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:35.919+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:35.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:35.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:00:43.541+0000] {processor.py:161} INFO - Started process (PID=14647) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:43.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:43.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:43.545+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:43.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:43.588+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:43.598+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:43.614+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T14:00:52.834+0000] {processor.py:161} INFO - Started process (PID=14726) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:52.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:00:52.838+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:52.837+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:52.877+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:00:52.872+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:00:52.879+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:00:52.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T14:01:01.081+0000] {processor.py:161} INFO - Started process (PID=14805) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:01.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:01.086+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:01.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:01.130+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:01.123+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:01.133+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:01.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:01:11.516+0000] {processor.py:161} INFO - Started process (PID=14891) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:11.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:11.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:11.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:11.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:11.589+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:11.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:11.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T14:01:20.895+0000] {processor.py:161} INFO - Started process (PID=14970) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:20.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:20.899+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:20.898+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:20.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:20.937+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:20.947+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:20.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:01:28.787+0000] {processor.py:161} INFO - Started process (PID=15049) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:28.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:28.792+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:28.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:28.837+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:28.830+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:28.839+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:28.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:01:37.787+0000] {processor.py:161} INFO - Started process (PID=15128) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:37.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:37.791+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:37.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:37.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:37.827+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:37.837+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:37.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:01:46.762+0000] {processor.py:161} INFO - Started process (PID=15213) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:46.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:46.766+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:46.766+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:46.813+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:46.807+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:46.815+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:46.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:01:53.681+0000] {processor.py:161} INFO - Started process (PID=15292) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:53.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:01:53.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:53.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:53.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:01:53.722+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:01:53.731+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:01:53.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:02:02.195+0000] {processor.py:161} INFO - Started process (PID=15371) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:02.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:02.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:02.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:02.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:02.238+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:02.248+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:02.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:02:14.142+0000] {processor.py:161} INFO - Started process (PID=15456) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:14.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:14.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:14.146+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:14.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:14.178+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:14.187+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:14.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T14:02:22.016+0000] {processor.py:161} INFO - Started process (PID=15536) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:22.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:22.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:22.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:22.061+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:22.054+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:22.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:22.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:02:30.292+0000] {processor.py:161} INFO - Started process (PID=15615) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:30.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:30.298+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:30.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:30.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:30.342+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:30.353+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:30.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:02:39.901+0000] {processor.py:161} INFO - Started process (PID=15694) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:39.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:39.906+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:39.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:39.952+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:39.946+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:39.955+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:39.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:02:48.443+0000] {processor.py:161} INFO - Started process (PID=15779) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:48.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:48.448+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:48.448+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:48.499+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:48.492+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:48.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:48.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:02:59.587+0000] {processor.py:161} INFO - Started process (PID=15858) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:59.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:02:59.592+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:59.592+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:59.646+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:02:59.639+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:02:59.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:02:59.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T14:03:10.740+0000] {processor.py:161} INFO - Started process (PID=15943) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:10.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:10.744+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:10.744+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:10.783+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:10.777+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:10.785+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:10.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:03:19.882+0000] {processor.py:161} INFO - Started process (PID=16022) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:19.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:19.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:19.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:19.919+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:19.913+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:19.922+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:19.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T14:03:29.050+0000] {processor.py:161} INFO - Started process (PID=16102) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:29.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:29.057+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:29.057+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:29.136+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:29.126+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:29.142+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:29.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T14:03:36.946+0000] {processor.py:161} INFO - Started process (PID=16181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:36.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:36.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:36.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:37.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:37.005+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:37.017+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:37.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T14:03:44.849+0000] {processor.py:161} INFO - Started process (PID=16266) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:44.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:44.854+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:44.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:44.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:44.896+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:44.907+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:44.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T14:03:53.043+0000] {processor.py:161} INFO - Started process (PID=16345) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:53.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:03:53.048+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:53.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:53.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:03:53.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:03:53.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:03:53.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T14:04:00.128+0000] {processor.py:161} INFO - Started process (PID=16424) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:00.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:00.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:00.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:00.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:00.170+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:00.179+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:00.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:04:10.789+0000] {processor.py:161} INFO - Started process (PID=16503) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:10.794+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:10.799+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:10.798+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:10.856+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:10.846+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:10.859+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:10.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T14:04:23.096+0000] {processor.py:161} INFO - Started process (PID=16589) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:23.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:23.100+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:23.100+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:23.139+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:23.134+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:23.142+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:23.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:04:33.239+0000] {processor.py:161} INFO - Started process (PID=16668) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:33.241+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:33.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:33.245+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:33.296+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:33.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:33.300+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:33.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:04:42.364+0000] {processor.py:161} INFO - Started process (PID=16754) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:42.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:42.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:42.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:42.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:42.424+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:42.436+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:42.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:04:53.330+0000] {processor.py:161} INFO - Started process (PID=16833) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:53.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:04:53.336+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:53.335+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:53.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:04:53.391+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:04:53.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:04:53.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.120 seconds
[2024-05-10T14:05:03.256+0000] {processor.py:161} INFO - Started process (PID=16912) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:03.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:05:03.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:03.262+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:03.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:03.309+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:05:03.320+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:03.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T14:05:16.563+0000] {processor.py:161} INFO - Started process (PID=16999) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:16.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:05:16.566+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:16.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:16.629+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:16.622+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:05:16.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:16.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T14:05:29.121+0000] {processor.py:161} INFO - Started process (PID=17080) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:29.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:05:29.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:29.124+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:29.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:29.160+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:05:29.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:29.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T14:05:41.865+0000] {processor.py:161} INFO - Started process (PID=17166) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:41.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:05:41.870+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:41.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:41.935+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:41.927+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:05:41.940+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:41.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.121 seconds
[2024-05-10T14:05:53.147+0000] {processor.py:161} INFO - Started process (PID=17246) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:53.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:05:53.153+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:53.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:53.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:05:53.219+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:05:53.234+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:05:53.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T14:06:28.354+0000] {processor.py:161} INFO - Started process (PID=17332) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:28.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:06:28.360+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:06:28.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:28.438+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:06:28.426+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:06:28.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:28.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T14:06:46.127+0000] {processor.py:161} INFO - Started process (PID=17419) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:46.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:06:46.130+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:06:46.130+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:46.193+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:06:46.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:06:46.196+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:06:46.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:07:02.385+0000] {processor.py:161} INFO - Started process (PID=17499) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:02.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:07:02.391+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:02.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:02.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:02.457+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:07:02.473+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:02.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T14:07:18.811+0000] {processor.py:161} INFO - Started process (PID=17585) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:18.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:07:18.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:18.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:18.876+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:18.866+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:07:18.880+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:18.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.112 seconds
[2024-05-10T14:07:34.027+0000] {processor.py:161} INFO - Started process (PID=17665) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:34.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:07:34.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:34.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:34.078+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:34.071+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:07:34.082+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:34.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:07:46.905+0000] {processor.py:161} INFO - Started process (PID=17751) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:46.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:07:46.909+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:46.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:46.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:46.960+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:07:46.972+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:46.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T14:07:56.402+0000] {processor.py:161} INFO - Started process (PID=17831) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:56.403+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:07:56.405+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:56.404+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:56.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:07:56.444+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:07:56.454+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:07:56.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:08:05.539+0000] {processor.py:161} INFO - Started process (PID=17911) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:05.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:05.543+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:05.543+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:05.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:05.582+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:05.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:05.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:08:13.673+0000] {processor.py:161} INFO - Started process (PID=17997) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:13.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:13.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:13.675+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:13.717+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:13.711+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:13.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:13.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:08:22.040+0000] {processor.py:161} INFO - Started process (PID=18077) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:22.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:22.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:22.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:22.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:22.200+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:22.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:22.246+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.227 seconds
[2024-05-10T14:08:33.699+0000] {processor.py:161} INFO - Started process (PID=18157) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:33.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:33.703+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:33.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:33.752+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:33.744+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:33.756+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:33.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:08:42.351+0000] {processor.py:161} INFO - Started process (PID=18243) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:42.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:42.355+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:42.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:42.407+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:42.400+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:42.409+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:42.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T14:08:50.437+0000] {processor.py:161} INFO - Started process (PID=18323) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:50.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:50.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:50.440+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:50.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:50.476+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:50.486+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:50.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:08:59.498+0000] {processor.py:161} INFO - Started process (PID=18403) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:59.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:08:59.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:59.501+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:59.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:08:59.541+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:08:59.551+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:08:59.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T14:09:09.958+0000] {processor.py:161} INFO - Started process (PID=18483) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:09.960+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:09.963+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:09.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:10.009+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:10.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:10.012+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:10.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:09:19.364+0000] {processor.py:161} INFO - Started process (PID=18569) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:19.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:19.368+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:19.367+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:19.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:19.418+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:19.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:19.453+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T14:09:26.509+0000] {processor.py:161} INFO - Started process (PID=18649) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:26.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:26.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:26.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:26.559+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:26.552+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:26.562+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:26.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:09:35.381+0000] {processor.py:161} INFO - Started process (PID=18729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:35.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:35.385+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:35.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:35.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:35.442+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:35.456+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:35.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.117 seconds
[2024-05-10T14:09:44.785+0000] {processor.py:161} INFO - Started process (PID=18815) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:44.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:44.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:44.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:44.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:44.840+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:44.856+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:44.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:09:53.032+0000] {processor.py:161} INFO - Started process (PID=18895) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:53.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:09:53.037+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:53.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:53.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:09:53.080+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:09:53.090+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:09:53.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T14:10:02.533+0000] {processor.py:161} INFO - Started process (PID=18975) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:02.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:02.536+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:02.536+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:02.592+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:02.584+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:02.596+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:02.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:10:11.570+0000] {processor.py:161} INFO - Started process (PID=19055) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:11.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:11.573+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:11.573+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:11.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:11.610+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:11.620+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:11.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:10:19.626+0000] {processor.py:161} INFO - Started process (PID=19141) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:19.628+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:19.630+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:19.629+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:19.678+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:19.671+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:19.680+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:19.705+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:10:27.355+0000] {processor.py:161} INFO - Started process (PID=19221) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:27.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:27.358+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:27.358+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:27.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:27.397+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:27.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:27.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T14:10:37.517+0000] {processor.py:161} INFO - Started process (PID=19301) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:37.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:37.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:37.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:37.580+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:37.572+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:37.591+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:37.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:10:44.754+0000] {processor.py:161} INFO - Started process (PID=19387) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:44.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:44.757+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:44.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:44.792+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:44.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:44.794+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:44.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T14:10:52.227+0000] {processor.py:161} INFO - Started process (PID=19467) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:52.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:10:52.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:52.230+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:52.275+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:10:52.268+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:10:52.277+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:10:52.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:11:01.902+0000] {processor.py:161} INFO - Started process (PID=19547) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:01.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:01.906+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:01.906+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:01.978+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:01.964+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:01.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:02.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T14:11:13.689+0000] {processor.py:161} INFO - Started process (PID=19633) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:13.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:13.697+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:13.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:13.753+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:13.746+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:13.757+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:13.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T14:11:21.926+0000] {processor.py:161} INFO - Started process (PID=19713) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:21.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:21.929+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:21.928+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:21.967+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:21.961+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:21.970+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:21.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T14:11:30.850+0000] {processor.py:161} INFO - Started process (PID=19794) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:30.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:30.856+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:30.855+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:30.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:30.932+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:30.951+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:30.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T14:11:42.099+0000] {processor.py:161} INFO - Started process (PID=19880) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:42.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:42.106+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:42.105+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:42.195+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:42.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:42.199+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:42.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T14:11:51.238+0000] {processor.py:161} INFO - Started process (PID=19960) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:51.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:51.243+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:51.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:51.303+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:51.293+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:51.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:51.338+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.115 seconds
[2024-05-10T14:11:59.926+0000] {processor.py:161} INFO - Started process (PID=20040) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:59.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:11:59.929+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:59.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:59.973+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:11:59.965+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:11:59.976+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:11:59.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:12:08.847+0000] {processor.py:161} INFO - Started process (PID=20120) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:08.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:08.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:08.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:08.888+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:08.883+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:08.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:08.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:12:19.910+0000] {processor.py:161} INFO - Started process (PID=20207) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:19.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:19.914+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:19.913+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:19.965+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:19.958+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:19.968+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:19.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T14:12:28.521+0000] {processor.py:161} INFO - Started process (PID=20287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:28.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:28.525+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:28.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:28.575+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:28.568+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:28.577+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:28.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:12:36.924+0000] {processor.py:161} INFO - Started process (PID=20367) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:36.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:36.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:36.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:36.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:36.969+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:36.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:36.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:12:45.312+0000] {processor.py:161} INFO - Started process (PID=20454) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:45.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:45.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:45.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:45.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:45.352+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:45.362+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:45.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:12:54.335+0000] {processor.py:161} INFO - Started process (PID=20533) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:54.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:12:54.340+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:54.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:54.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:12:54.405+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:12:54.477+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:12:54.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.212 seconds
[2024-05-10T14:13:08.166+0000] {processor.py:161} INFO - Started process (PID=20614) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:08.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:08.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:08.169+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:08.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:08.211+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:08.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:08.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:13:16.277+0000] {processor.py:161} INFO - Started process (PID=20701) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:16.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:16.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:16.281+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:16.349+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:16.342+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:16.351+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:16.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T14:13:24.381+0000] {processor.py:161} INFO - Started process (PID=20781) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:24.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:24.385+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:24.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:24.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:24.424+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:24.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:24.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:13:35.583+0000] {processor.py:161} INFO - Started process (PID=20861) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:35.585+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:35.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:35.586+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:35.633+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:35.627+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:35.635+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:35.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T14:13:45.394+0000] {processor.py:161} INFO - Started process (PID=20947) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:45.396+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:45.399+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:45.398+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:45.476+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:45.466+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:45.481+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:45.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T14:13:53.666+0000] {processor.py:161} INFO - Started process (PID=21027) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:53.668+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:13:53.670+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:53.669+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:53.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:13:53.723+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:13:53.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:13:53.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:14:02.196+0000] {processor.py:161} INFO - Started process (PID=21107) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:02.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:02.202+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:02.201+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:02.261+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:02.253+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:02.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:02.287+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T14:14:12.754+0000] {processor.py:161} INFO - Started process (PID=21194) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:12.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:12.759+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:12.758+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:12.813+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:12.805+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:12.816+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:12.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T14:14:23.521+0000] {processor.py:161} INFO - Started process (PID=21274) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:23.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:23.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:23.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:23.559+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:23.553+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:23.561+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:23.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T14:14:32.621+0000] {processor.py:161} INFO - Started process (PID=21354) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:32.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:32.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:32.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:32.702+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:32.689+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:32.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:32.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.129 seconds
[2024-05-10T14:14:43.588+0000] {processor.py:161} INFO - Started process (PID=21440) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:43.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:43.591+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:43.591+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:43.634+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:43.627+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:43.637+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:43.651+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:14:52.186+0000] {processor.py:161} INFO - Started process (PID=21521) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:52.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:14:52.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:52.189+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:52.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:14:52.228+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:14:52.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:14:52.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:15:00.761+0000] {processor.py:161} INFO - Started process (PID=21601) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:00.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:00.763+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:00.763+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:00.811+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:00.804+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:00.814+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:00.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:15:09.597+0000] {processor.py:161} INFO - Started process (PID=21681) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:09.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:09.601+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:09.601+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:09.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:09.645+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:09.655+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:09.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T14:15:19.710+0000] {processor.py:161} INFO - Started process (PID=21767) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:19.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:19.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:19.713+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:19.766+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:19.759+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:19.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:19.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:15:28.184+0000] {processor.py:161} INFO - Started process (PID=21847) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:28.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:28.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:28.187+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:28.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:28.238+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:28.250+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:28.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T14:15:37.075+0000] {processor.py:161} INFO - Started process (PID=21927) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:37.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:37.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:37.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:37.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:37.117+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:37.127+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:37.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:15:44.274+0000] {processor.py:161} INFO - Started process (PID=22013) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:44.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:44.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:44.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:44.318+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:44.312+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:44.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:44.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:15:52.526+0000] {processor.py:161} INFO - Started process (PID=22093) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:52.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:15:52.529+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:52.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:52.574+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:15:52.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:15:52.577+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:15:52.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:16:00.865+0000] {processor.py:161} INFO - Started process (PID=22173) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:00.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:00.868+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:00.867+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:00.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:00.908+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:00.917+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:00.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:16:09.801+0000] {processor.py:161} INFO - Started process (PID=22254) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:09.803+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:09.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:09.804+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:09.864+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:09.856+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:09.867+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:09.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T14:16:19.944+0000] {processor.py:161} INFO - Started process (PID=22341) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:19.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:19.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:19.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:19.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:19.980+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:19.988+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:20.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T14:16:28.861+0000] {processor.py:161} INFO - Started process (PID=22421) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:28.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:28.865+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:28.864+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:28.917+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:28.910+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:28.919+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:28.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:16:42.506+0000] {processor.py:161} INFO - Started process (PID=22501) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:42.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:42.513+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:42.513+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:42.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:42.576+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:42.588+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:42.612+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:16:52.733+0000] {processor.py:161} INFO - Started process (PID=22587) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:52.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:16:52.736+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:52.735+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:52.809+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:16:52.785+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:16:52.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:16:52.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T14:17:03.710+0000] {processor.py:161} INFO - Started process (PID=22667) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:03.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:03.714+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:03.713+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:03.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:03.776+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:03.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:03.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T14:17:14.566+0000] {processor.py:161} INFO - Started process (PID=22753) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:14.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:14.569+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:14.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:14.620+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:14.611+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:14.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:14.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T14:17:22.388+0000] {processor.py:161} INFO - Started process (PID=22833) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:22.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:22.391+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:22.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:22.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:22.431+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:22.444+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:22.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:17:30.613+0000] {processor.py:161} INFO - Started process (PID=22913) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:30.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:30.618+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:30.617+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:30.678+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:30.671+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:30.682+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:30.720+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T14:17:39.533+0000] {processor.py:161} INFO - Started process (PID=22994) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:39.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:39.541+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:39.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:39.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:39.588+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:39.599+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:39.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T14:17:53.464+0000] {processor.py:161} INFO - Started process (PID=23080) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:53.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:17:53.467+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:53.467+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:53.515+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:17:53.508+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:17:53.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:17:53.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T14:18:00.804+0000] {processor.py:161} INFO - Started process (PID=23160) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:00.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:00.807+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:00.807+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:00.865+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:00.855+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:00.871+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:00.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T14:18:12.458+0000] {processor.py:161} INFO - Started process (PID=23242) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:12.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:12.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:12.462+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:12.542+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:12.529+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:12.546+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:12.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T14:18:24.576+0000] {processor.py:161} INFO - Started process (PID=23329) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:24.578+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:24.580+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:24.580+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:24.651+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:24.639+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:24.655+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:24.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.119 seconds
[2024-05-10T14:18:36.302+0000] {processor.py:161} INFO - Started process (PID=23410) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:36.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:36.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:36.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:36.348+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:36.341+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:36.351+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:36.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:18:45.364+0000] {processor.py:161} INFO - Started process (PID=23497) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:45.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:45.368+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:45.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:45.423+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:45.414+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:45.425+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:45.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T14:18:54.243+0000] {processor.py:161} INFO - Started process (PID=23578) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:54.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:18:54.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:54.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:54.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:18:54.281+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:18:54.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:18:54.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:19:04.430+0000] {processor.py:161} INFO - Started process (PID=23660) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:04.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:04.435+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:04.434+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:04.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:04.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:04.513+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:04.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T14:19:14.239+0000] {processor.py:161} INFO - Started process (PID=23747) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:14.241+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:14.242+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:14.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:14.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:14.283+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:14.293+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:14.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:19:24.034+0000] {processor.py:161} INFO - Started process (PID=23828) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:24.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:24.037+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:24.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:24.077+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:24.071+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:24.079+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:24.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:19:31.982+0000] {processor.py:161} INFO - Started process (PID=23909) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:31.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:31.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:31.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:32.037+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:32.030+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:32.040+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:32.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:19:41.159+0000] {processor.py:161} INFO - Started process (PID=23990) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:41.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:41.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:41.162+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:41.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:41.221+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:41.231+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:41.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:19:49.356+0000] {processor.py:161} INFO - Started process (PID=24078) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:49.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:49.358+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:49.358+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:49.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:49.388+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:49.396+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:49.409+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T14:19:57.639+0000] {processor.py:161} INFO - Started process (PID=24159) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:57.640+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:19:57.641+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:57.641+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:57.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:19:57.675+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:19:57.685+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:19:57.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T14:20:08.289+0000] {processor.py:161} INFO - Started process (PID=24241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:08.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:08.292+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:08.292+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:08.346+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:08.339+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:08.349+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:08.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T14:20:18.195+0000] {processor.py:161} INFO - Started process (PID=24328) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:18.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:18.200+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:18.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:18.269+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:18.257+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:18.275+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:18.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T14:20:28.185+0000] {processor.py:161} INFO - Started process (PID=24409) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:28.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:28.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:28.187+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:28.237+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:28.230+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:28.240+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:28.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:20:37.449+0000] {processor.py:161} INFO - Started process (PID=24490) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:37.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:37.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:37.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:37.488+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:37.483+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:37.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:37.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T14:20:45.257+0000] {processor.py:161} INFO - Started process (PID=24577) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:45.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:45.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:45.260+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:45.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:45.299+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:45.309+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:45.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:20:52.546+0000] {processor.py:161} INFO - Started process (PID=24658) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:52.548+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:20:52.551+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:52.550+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:52.603+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:20:52.594+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:20:52.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:20:52.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:21:02.945+0000] {processor.py:161} INFO - Started process (PID=24739) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:02.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:02.949+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:02.948+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:03.000+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:02.992+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:03.004+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:03.023+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T14:21:13.382+0000] {processor.py:161} INFO - Started process (PID=24820) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:13.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:13.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:13.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:13.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:13.425+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:13.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:13.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:21:22.446+0000] {processor.py:161} INFO - Started process (PID=24908) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:22.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:22.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:22.449+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:22.482+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:22.477+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:22.485+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:22.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T14:21:31.164+0000] {processor.py:161} INFO - Started process (PID=24989) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:31.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:31.169+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:31.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:31.231+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:31.222+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:31.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:31.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T14:21:40.138+0000] {processor.py:161} INFO - Started process (PID=25070) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:40.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:40.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:40.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:40.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:40.180+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:40.191+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:40.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:21:47.258+0000] {processor.py:161} INFO - Started process (PID=25157) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:47.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:47.261+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:47.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:47.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:47.298+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:47.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:47.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:21:54.919+0000] {processor.py:161} INFO - Started process (PID=25238) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:54.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:21:54.921+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:54.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:54.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:21:54.958+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:21:54.967+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:21:54.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:22:05.254+0000] {processor.py:161} INFO - Started process (PID=25319) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:05.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:05.264+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:05.263+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:05.340+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:05.325+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:05.344+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:05.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T14:22:17.054+0000] {processor.py:161} INFO - Started process (PID=25407) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:17.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:17.059+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:17.058+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:17.114+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:17.106+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:17.116+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:17.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T14:22:26.246+0000] {processor.py:161} INFO - Started process (PID=25488) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:26.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:26.249+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:26.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:26.298+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:26.291+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:26.301+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:26.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:22:34.909+0000] {processor.py:161} INFO - Started process (PID=25570) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:34.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:34.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:34.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:34.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:34.949+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:34.958+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:34.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:22:42.241+0000] {processor.py:161} INFO - Started process (PID=25651) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:42.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:42.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:42.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:42.288+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:42.281+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:42.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:42.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:22:51.413+0000] {processor.py:161} INFO - Started process (PID=25737) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:51.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:51.416+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:51.416+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:51.466+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:51.458+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:51.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:51.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:22:57.968+0000] {processor.py:161} INFO - Started process (PID=25818) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:57.969+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:22:57.971+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:57.970+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:58.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:22:58.008+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:22:58.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:22:58.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:23:06.231+0000] {processor.py:161} INFO - Started process (PID=25899) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:06.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:06.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:06.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:06.283+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:06.275+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:06.286+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:06.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:23:13.784+0000] {processor.py:161} INFO - Started process (PID=25980) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:13.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:13.787+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:13.787+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:13.828+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:13.821+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:13.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:13.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:23:20.984+0000] {processor.py:161} INFO - Started process (PID=26067) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:20.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:20.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:20.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:21.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:21.038+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:21.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:21.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T14:23:29.352+0000] {processor.py:161} INFO - Started process (PID=26148) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:29.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:29.355+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:29.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:29.396+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:29.389+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:29.399+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:29.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:23:38.103+0000] {processor.py:161} INFO - Started process (PID=26229) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:38.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:38.107+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:38.106+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:38.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:38.167+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:38.180+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:38.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.112 seconds
[2024-05-10T14:23:48.617+0000] {processor.py:161} INFO - Started process (PID=26316) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:48.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:48.621+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:48.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:48.666+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:48.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:48.669+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:48.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:23:59.723+0000] {processor.py:161} INFO - Started process (PID=26397) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:59.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:23:59.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:59.725+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:59.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:23:59.772+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:23:59.784+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:23:59.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:24:11.779+0000] {processor.py:161} INFO - Started process (PID=26478) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:11.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:24:11.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:11.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:11.819+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:11.812+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:24:11.821+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:11.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T14:24:22.044+0000] {processor.py:161} INFO - Started process (PID=26565) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:22.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:24:22.048+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:22.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:22.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:22.097+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:24:22.107+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:22.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:24:32.461+0000] {processor.py:161} INFO - Started process (PID=26646) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:32.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:24:32.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:32.464+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:32.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:32.501+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:24:32.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:32.527+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:24:43.297+0000] {processor.py:161} INFO - Started process (PID=26727) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:43.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:24:43.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:43.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:43.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:43.337+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:24:43.345+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:43.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:24:53.580+0000] {processor.py:161} INFO - Started process (PID=26815) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:53.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:24:53.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:53.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:53.627+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:24:53.621+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:24:53.630+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:24:53.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:25:02.772+0000] {processor.py:161} INFO - Started process (PID=26896) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:02.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:02.776+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:02.775+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:02.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:02.818+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:02.827+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:02.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:25:15.874+0000] {processor.py:161} INFO - Started process (PID=26983) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:15.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:15.877+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:15.877+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:15.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:15.920+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:15.930+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:15.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:25:23.455+0000] {processor.py:161} INFO - Started process (PID=27064) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:23.457+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:23.459+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:23.458+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:23.507+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:23.500+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:23.509+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:23.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:25:31.898+0000] {processor.py:161} INFO - Started process (PID=27146) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:31.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:31.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:31.901+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:31.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:31.941+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:31.950+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:31.964+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:25:38.561+0000] {processor.py:161} INFO - Started process (PID=27228) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:38.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:38.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:38.563+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:38.609+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:38.601+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:38.612+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:38.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:25:46.118+0000] {processor.py:161} INFO - Started process (PID=27315) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:46.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:46.125+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:46.124+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:46.183+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:46.177+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:46.186+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:46.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T14:25:53.993+0000] {processor.py:161} INFO - Started process (PID=27396) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:53.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:25:53.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:53.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:54.040+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:25:54.033+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:25:54.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:25:54.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:26:01.628+0000] {processor.py:161} INFO - Started process (PID=27477) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:01.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:01.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:01.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:01.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:01.670+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:01.680+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:01.700+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:26:12.470+0000] {processor.py:161} INFO - Started process (PID=27559) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:12.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:12.474+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:12.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:12.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:12.517+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:12.526+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:12.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:26:23.760+0000] {processor.py:161} INFO - Started process (PID=27648) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:23.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:23.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:23.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:23.812+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:23.805+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:23.815+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:23.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:26:32.225+0000] {processor.py:161} INFO - Started process (PID=27729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:32.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:32.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:32.227+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:32.272+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:32.266+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:32.274+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:32.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:26:39.838+0000] {processor.py:161} INFO - Started process (PID=27811) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:39.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:39.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:39.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:39.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:39.879+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:39.887+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:39.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:26:46.243+0000] {processor.py:161} INFO - Started process (PID=27898) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:46.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:46.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:46.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:46.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:46.280+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:46.290+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:46.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:26:54.272+0000] {processor.py:161} INFO - Started process (PID=27979) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:54.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:26:54.274+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:54.274+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:54.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:26:54.309+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:26:54.318+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:26:54.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:27:03.089+0000] {processor.py:161} INFO - Started process (PID=28060) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:03.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:03.093+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:03.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:03.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:03.135+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:03.144+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:03.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T14:27:13.790+0000] {processor.py:161} INFO - Started process (PID=28141) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:13.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:13.794+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:13.793+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:13.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:13.835+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:13.844+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:13.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:27:21.745+0000] {processor.py:161} INFO - Started process (PID=28229) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:21.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:21.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:21.747+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:21.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:21.782+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:21.791+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:21.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:27:30.211+0000] {processor.py:161} INFO - Started process (PID=28310) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:30.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:30.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:30.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:30.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:30.255+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:30.265+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:30.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:27:37.725+0000] {processor.py:161} INFO - Started process (PID=28391) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:37.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:37.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:37.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:37.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:37.770+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:37.779+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:37.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T14:27:45.450+0000] {processor.py:161} INFO - Started process (PID=28478) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:45.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:45.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:45.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:45.494+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:45.487+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:45.497+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:45.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:27:54.185+0000] {processor.py:161} INFO - Started process (PID=28559) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:54.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:27:54.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:54.187+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:54.235+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:27:54.228+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:27:54.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:27:54.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:28:02.612+0000] {processor.py:161} INFO - Started process (PID=28640) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:02.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:02.616+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:02.616+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:02.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:02.668+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:02.679+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:02.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T14:28:12.743+0000] {processor.py:161} INFO - Started process (PID=28721) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:12.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:12.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:12.746+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:12.796+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:12.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:12.799+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:12.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.107 seconds
[2024-05-10T14:28:24.386+0000] {processor.py:161} INFO - Started process (PID=28809) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:24.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:24.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:24.389+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:24.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:24.430+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:24.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:24.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T14:28:33.623+0000] {processor.py:161} INFO - Started process (PID=28890) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:33.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:33.627+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:33.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:33.683+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:33.675+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:33.686+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:33.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T14:28:43.050+0000] {processor.py:161} INFO - Started process (PID=28971) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:43.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:43.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:43.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:43.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:43.090+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:43.099+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:43.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:28:50.578+0000] {processor.py:161} INFO - Started process (PID=29057) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:50.580+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:50.581+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:50.581+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:50.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:50.618+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:50.627+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:50.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:28:57.433+0000] {processor.py:161} INFO - Started process (PID=29139) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:57.435+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:28:57.436+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:57.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:57.474+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:28:57.468+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:28:57.477+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:28:57.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T14:29:04.337+0000] {processor.py:161} INFO - Started process (PID=29220) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:04.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:04.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:04.342+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:04.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:04.405+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:04.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:04.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.119 seconds
[2024-05-10T14:29:12.427+0000] {processor.py:161} INFO - Started process (PID=29301) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:12.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:12.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:12.432+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:12.507+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:12.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:12.510+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:12.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T14:29:23.091+0000] {processor.py:161} INFO - Started process (PID=29388) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:23.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:23.095+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:23.094+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:23.148+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:23.140+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:23.151+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:23.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:29:32.376+0000] {processor.py:161} INFO - Started process (PID=29469) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:32.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:32.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:32.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:32.418+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:32.413+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:32.421+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:32.439+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:29:40.168+0000] {processor.py:161} INFO - Started process (PID=29550) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:40.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:40.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:40.170+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:40.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:40.203+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:40.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:40.225+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:29:47.394+0000] {processor.py:161} INFO - Started process (PID=29638) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:47.396+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:47.397+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:47.397+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:47.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:47.433+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:47.441+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:47.458+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:29:54.411+0000] {processor.py:161} INFO - Started process (PID=29719) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:54.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:29:54.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:54.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:54.456+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:29:54.449+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:29:54.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:29:54.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:30:02.553+0000] {processor.py:161} INFO - Started process (PID=29800) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:02.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:02.556+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:02.555+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:02.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:02.600+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:02.609+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:02.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:30:10.998+0000] {processor.py:161} INFO - Started process (PID=29881) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:11.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:11.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:11.001+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:11.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:11.038+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:11.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:11.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:30:18.474+0000] {processor.py:161} INFO - Started process (PID=29968) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:18.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:18.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:18.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:18.517+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:18.510+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:18.519+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:18.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:30:25.656+0000] {processor.py:161} INFO - Started process (PID=30049) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:25.657+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:25.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:25.658+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:25.703+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:25.696+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:25.705+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:25.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:30:33.954+0000] {processor.py:161} INFO - Started process (PID=30130) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:33.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:33.958+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:33.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:34.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:33.998+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:34.007+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:34.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:30:43.076+0000] {processor.py:161} INFO - Started process (PID=30212) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:43.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:43.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:43.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:43.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:43.118+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:43.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:43.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:30:50.058+0000] {processor.py:161} INFO - Started process (PID=30299) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:50.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:50.061+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:50.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:50.102+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:50.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:50.105+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:50.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:30:57.380+0000] {processor.py:161} INFO - Started process (PID=30381) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:57.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:30:57.383+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:57.383+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:57.435+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:30:57.426+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:30:57.439+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:30:57.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T14:31:05.892+0000] {processor.py:161} INFO - Started process (PID=30462) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:05.894+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:05.896+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:05.895+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:05.944+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:05.936+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:05.946+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:05.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T14:31:15.669+0000] {processor.py:161} INFO - Started process (PID=30549) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:15.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:15.673+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:15.673+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:15.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:15.718+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:15.728+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:15.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T14:31:24.202+0000] {processor.py:161} INFO - Started process (PID=30630) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:24.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:24.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:24.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:24.242+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:24.236+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:24.244+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:24.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T14:31:30.900+0000] {processor.py:161} INFO - Started process (PID=30711) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:30.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:30.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:30.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:30.953+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:30.947+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:30.956+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:30.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:31:38.575+0000] {processor.py:161} INFO - Started process (PID=30792) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:38.577+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:38.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:38.578+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:38.621+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:38.616+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:38.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:38.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:31:45.440+0000] {processor.py:161} INFO - Started process (PID=30879) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:45.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:45.443+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:45.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:45.496+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:45.489+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:45.498+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:45.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:31:53.582+0000] {processor.py:161} INFO - Started process (PID=30960) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:53.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:53.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:53.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:53.636+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:53.628+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:53.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:53.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:31:59.924+0000] {processor.py:161} INFO - Started process (PID=31041) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:59.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:31:59.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:59.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:59.975+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:31:59.970+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:31:59.978+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:31:59.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:32:08.252+0000] {processor.py:161} INFO - Started process (PID=31122) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:08.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:08.257+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:08.256+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:08.307+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:08.300+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:08.310+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:08.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:32:16.227+0000] {processor.py:161} INFO - Started process (PID=31209) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:16.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:16.229+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:16.229+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:16.266+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:16.261+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:16.268+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:16.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T14:32:24.907+0000] {processor.py:161} INFO - Started process (PID=31290) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:24.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:24.913+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:24.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:24.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:24.960+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:24.972+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:24.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T14:32:34.442+0000] {processor.py:161} INFO - Started process (PID=31371) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:34.444+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:34.446+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:34.446+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:34.513+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:34.505+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:34.516+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:34.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T14:32:43.226+0000] {processor.py:161} INFO - Started process (PID=31453) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:43.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:43.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:43.229+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:43.276+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:43.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:43.278+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:43.294+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:32:50.931+0000] {processor.py:161} INFO - Started process (PID=31540) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:50.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:50.934+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:50.934+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:50.974+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:50.967+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:50.977+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:50.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:32:57.680+0000] {processor.py:161} INFO - Started process (PID=31621) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:57.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:32:57.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:57.683+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:57.727+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:32:57.721+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:32:57.730+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:32:57.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:33:07.942+0000] {processor.py:161} INFO - Started process (PID=31702) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:07.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:07.948+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:07.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:08.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:07.996+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:08.005+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:08.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T14:33:16.170+0000] {processor.py:161} INFO - Started process (PID=31788) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:16.172+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:16.174+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:16.173+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:16.222+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:16.215+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:16.225+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:16.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T14:33:25.487+0000] {processor.py:161} INFO - Started process (PID=31869) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:25.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:25.490+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:25.489+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:25.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:25.539+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:25.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:25.572+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T14:33:34.446+0000] {processor.py:161} INFO - Started process (PID=31950) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:34.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:34.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:34.449+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:34.498+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:34.491+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:34.501+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:34.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:33:42.293+0000] {processor.py:161} INFO - Started process (PID=32031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:42.295+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:42.296+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:42.296+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:42.336+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:42.330+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:42.338+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:42.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:33:48.917+0000] {processor.py:161} INFO - Started process (PID=32118) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:48.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:48.920+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:48.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:48.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:48.957+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:48.966+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:48.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:33:57.866+0000] {processor.py:161} INFO - Started process (PID=32199) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:57.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:33:57.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:57.868+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:57.920+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:33:57.912+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:33:57.923+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:33:57.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:34:06.042+0000] {processor.py:161} INFO - Started process (PID=32280) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:06.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:06.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:06.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:06.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:06.082+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:06.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:06.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:34:14.929+0000] {processor.py:161} INFO - Started process (PID=32362) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:14.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:14.931+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:14.931+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:14.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:14.969+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:14.979+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:14.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:34:23.654+0000] {processor.py:161} INFO - Started process (PID=32449) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:23.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:23.657+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:23.657+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:23.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:23.701+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:23.710+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:23.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:34:33.379+0000] {processor.py:161} INFO - Started process (PID=32531) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:33.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:33.383+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:33.382+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:33.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:33.423+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:33.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:33.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T14:34:40.710+0000] {processor.py:161} INFO - Started process (PID=32612) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:40.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:40.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:40.712+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:40.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:40.742+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:40.749+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:40.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.059 seconds
[2024-05-10T14:34:48.562+0000] {processor.py:161} INFO - Started process (PID=32699) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:48.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:48.565+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:48.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:48.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:48.600+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:48.609+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:48.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:34:56.558+0000] {processor.py:161} INFO - Started process (PID=32780) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:56.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:34:56.561+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:56.561+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:56.602+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:34:56.596+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:34:56.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:34:56.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:35:06.439+0000] {processor.py:161} INFO - Started process (PID=32861) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:06.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:06.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:06.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:06.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:06.498+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:06.510+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:06.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:35:16.752+0000] {processor.py:161} INFO - Started process (PID=32948) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:16.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:16.755+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:16.755+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:16.796+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:16.789+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:16.798+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:16.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:35:26.261+0000] {processor.py:161} INFO - Started process (PID=33030) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:26.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:26.269+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:26.268+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:26.347+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:26.336+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:26.351+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:26.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T14:35:36.165+0000] {processor.py:161} INFO - Started process (PID=33112) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:36.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:36.168+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:36.167+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:36.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:36.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:36.213+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:36.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:35:44.359+0000] {processor.py:161} INFO - Started process (PID=33194) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:44.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:44.362+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:44.362+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:44.405+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:44.398+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:44.407+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:44.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:35:51.072+0000] {processor.py:161} INFO - Started process (PID=33281) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:51.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:51.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:51.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:51.114+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:51.109+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:51.116+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:51.133+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:35:59.723+0000] {processor.py:161} INFO - Started process (PID=33362) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:59.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:35:59.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:59.725+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:59.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:35:59.768+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:35:59.777+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:35:59.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T14:36:10.641+0000] {processor.py:161} INFO - Started process (PID=33443) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:10.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:10.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:10.644+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:10.691+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:10.684+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:10.693+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:10.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T14:36:20.807+0000] {processor.py:161} INFO - Started process (PID=33531) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:20.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:20.810+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:20.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:20.856+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:20.849+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:20.859+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:20.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:36:28.359+0000] {processor.py:161} INFO - Started process (PID=33612) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:28.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:28.362+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:28.361+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:28.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:28.394+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:28.403+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:28.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T14:36:38.081+0000] {processor.py:161} INFO - Started process (PID=33693) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:38.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:38.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:38.084+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:38.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:38.118+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:38.126+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:38.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:36:45.593+0000] {processor.py:161} INFO - Started process (PID=33774) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:45.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:45.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:45.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:45.639+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:45.632+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:45.641+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:45.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:36:53.919+0000] {processor.py:161} INFO - Started process (PID=33860) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:53.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:36:53.923+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:53.922+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:53.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:36:53.964+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:36:53.975+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:36:53.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T14:37:03.603+0000] {processor.py:161} INFO - Started process (PID=33941) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:03.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:03.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:03.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:03.657+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:03.651+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:03.659+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:03.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:37:13.690+0000] {processor.py:161} INFO - Started process (PID=34022) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:13.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:13.694+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:13.693+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:13.759+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:13.747+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:13.763+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:13.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.117 seconds
[2024-05-10T14:37:22.667+0000] {processor.py:161} INFO - Started process (PID=34109) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:22.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:22.670+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:22.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:22.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:22.702+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:22.710+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:22.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:37:30.625+0000] {processor.py:161} INFO - Started process (PID=34190) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:30.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:30.629+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:30.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:30.672+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:30.665+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:30.674+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:30.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:37:39.242+0000] {processor.py:161} INFO - Started process (PID=34271) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:39.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:39.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:39.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:39.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:39.281+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:39.289+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:39.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:37:48.175+0000] {processor.py:161} INFO - Started process (PID=34359) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:48.177+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:48.180+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:48.179+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:48.233+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:48.227+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:48.236+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:48.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:37:56.460+0000] {processor.py:161} INFO - Started process (PID=34440) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:56.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:37:56.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:56.463+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:56.507+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:37:56.501+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:37:56.509+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:37:56.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:38:05.385+0000] {processor.py:161} INFO - Started process (PID=34521) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:05.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:05.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:05.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:05.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:05.434+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:05.444+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:05.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:38:13.520+0000] {processor.py:161} INFO - Started process (PID=34602) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:13.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:13.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:13.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:13.566+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:13.560+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:13.568+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:13.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:38:21.494+0000] {processor.py:161} INFO - Started process (PID=34689) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:21.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:21.497+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:21.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:21.544+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:21.538+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:21.546+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:21.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:38:30.119+0000] {processor.py:161} INFO - Started process (PID=34770) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:30.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:30.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:30.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:30.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:30.158+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:30.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:30.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:38:39.509+0000] {processor.py:161} INFO - Started process (PID=34852) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:39.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:39.513+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:39.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:39.561+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:39.555+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:39.563+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:39.580+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T14:38:47.538+0000] {processor.py:161} INFO - Started process (PID=34939) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:47.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:47.542+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:47.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:47.589+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:47.582+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:47.592+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:47.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T14:38:54.919+0000] {processor.py:161} INFO - Started process (PID=35020) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:54.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:38:54.922+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:54.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:54.971+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:38:54.963+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:38:54.973+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:38:54.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:39:03.877+0000] {processor.py:161} INFO - Started process (PID=35101) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:03.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:03.881+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:03.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:03.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:03.918+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:03.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:03.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:39:11.690+0000] {processor.py:161} INFO - Started process (PID=35183) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:11.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:11.693+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:11.693+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:11.736+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:11.730+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:11.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:11.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:39:19.080+0000] {processor.py:161} INFO - Started process (PID=35270) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:19.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:19.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:19.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:19.131+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:19.125+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:19.134+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:19.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:39:25.898+0000] {processor.py:161} INFO - Started process (PID=35351) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:25.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:25.901+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:25.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:25.943+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:25.936+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:25.946+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:25.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:39:33.241+0000] {processor.py:161} INFO - Started process (PID=35432) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:33.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:33.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:33.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:33.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:33.283+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:33.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:33.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:39:41.478+0000] {processor.py:161} INFO - Started process (PID=35513) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:41.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:41.481+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:41.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:41.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:41.516+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:41.526+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:41.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:39:49.585+0000] {processor.py:161} INFO - Started process (PID=35601) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:49.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:49.588+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:49.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:49.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:49.632+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:49.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:49.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:39:58.207+0000] {processor.py:161} INFO - Started process (PID=35682) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:58.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:39:58.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:58.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:58.256+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:39:58.248+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:39:58.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:39:58.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:40:07.286+0000] {processor.py:161} INFO - Started process (PID=35763) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:07.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:07.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:07.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:07.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:07.334+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:07.346+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:07.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T14:40:18.071+0000] {processor.py:161} INFO - Started process (PID=35851) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:18.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:18.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:18.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:18.118+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:18.110+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:18.121+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:18.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:40:26.863+0000] {processor.py:161} INFO - Started process (PID=35932) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:26.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:26.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:26.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:26.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:26.898+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:26.906+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:26.923+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T14:40:34.086+0000] {processor.py:161} INFO - Started process (PID=36013) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:34.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:34.088+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:34.088+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:34.133+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:34.127+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:34.136+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:34.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:40:41.214+0000] {processor.py:161} INFO - Started process (PID=36094) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:41.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:41.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:41.216+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:41.261+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:41.254+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:41.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:41.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:40:50.008+0000] {processor.py:161} INFO - Started process (PID=36181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:50.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:50.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:50.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:50.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:50.046+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:50.054+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:50.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:40:58.233+0000] {processor.py:161} INFO - Started process (PID=36262) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:58.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:40:58.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:58.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:58.280+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:40:58.274+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:40:58.283+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:40:58.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:41:06.880+0000] {processor.py:161} INFO - Started process (PID=36343) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:06.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:41:06.882+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:06.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:06.920+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:06.913+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:41:06.922+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:06.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T14:41:17.107+0000] {processor.py:161} INFO - Started process (PID=36430) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:17.109+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:41:17.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:17.110+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:17.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:17.150+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:41:17.160+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:17.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T14:41:29.022+0000] {processor.py:161} INFO - Started process (PID=36512) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:29.024+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:41:29.025+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:29.025+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:29.068+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:29.061+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:41:29.071+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:29.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T14:41:38.308+0000] {processor.py:161} INFO - Started process (PID=36594) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:38.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:41:38.310+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:38.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:38.353+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:38.346+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:41:38.356+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:38.406+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T14:41:49.802+0000] {processor.py:161} INFO - Started process (PID=36681) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:49.803+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:41:49.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:49.804+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:49.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:41:49.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:41:49.855+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:41:49.871+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T14:42:03.567+0000] {processor.py:161} INFO - Started process (PID=36762) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:03.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:03.571+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:03.571+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:03.623+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:03.615+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:03.626+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:03.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T14:42:14.818+0000] {processor.py:161} INFO - Started process (PID=36843) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:14.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:14.822+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:14.821+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:14.871+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:14.864+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:14.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:14.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T14:42:22.298+0000] {processor.py:161} INFO - Started process (PID=36926) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:22.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:22.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:22.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:22.361+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:22.355+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:22.363+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:22.376+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:42:29.449+0000] {processor.py:161} INFO - Started process (PID=37007) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:29.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:29.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:29.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:29.494+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:29.486+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:29.497+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:29.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:42:38.690+0000] {processor.py:161} INFO - Started process (PID=37088) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:38.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:38.693+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:38.692+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:38.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:38.725+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:38.734+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:38.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T14:42:46.139+0000] {processor.py:161} INFO - Started process (PID=37169) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:46.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:46.143+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:46.142+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:46.195+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:46.186+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:46.197+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:46.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:42:54.109+0000] {processor.py:161} INFO - Started process (PID=37256) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:54.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:42:54.111+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:54.111+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:54.155+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:42:54.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:42:54.157+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:42:54.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T14:43:01.942+0000] {processor.py:161} INFO - Started process (PID=37337) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:01.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:01.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:01.945+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:01.995+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:01.987+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:01.998+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:02.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:43:10.245+0000] {processor.py:161} INFO - Started process (PID=37418) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:10.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:10.249+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:10.248+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:10.286+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:10.281+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:10.288+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:10.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T14:43:18.658+0000] {processor.py:161} INFO - Started process (PID=37505) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:18.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:18.662+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:18.661+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:18.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:18.702+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:18.710+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:18.728+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T14:43:27.047+0000] {processor.py:161} INFO - Started process (PID=37586) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:27.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:27.051+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:27.051+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:27.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:27.100+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:27.111+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:27.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T14:43:35.000+0000] {processor.py:161} INFO - Started process (PID=37667) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:35.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:35.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:35.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:35.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:35.043+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:35.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:35.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T14:43:41.950+0000] {processor.py:161} INFO - Started process (PID=37748) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:41.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:41.953+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:41.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:41.997+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:41.991+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:42.000+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:42.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:43:49.783+0000] {processor.py:161} INFO - Started process (PID=37835) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:49.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:49.786+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:49.786+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:49.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:49.827+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:49.837+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:49.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T14:43:58.153+0000] {processor.py:161} INFO - Started process (PID=37916) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:58.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:43:58.156+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:58.156+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:58.200+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:43:58.194+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:43:58.203+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:43:58.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:44:05.859+0000] {processor.py:161} INFO - Started process (PID=37997) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:05.860+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:05.861+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:05.861+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:05.907+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:05.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:05.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:05.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:44:13.945+0000] {processor.py:161} INFO - Started process (PID=38078) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:13.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:13.948+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:13.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:13.994+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:13.987+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:13.997+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:14.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:44:22.061+0000] {processor.py:161} INFO - Started process (PID=38165) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:22.063+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:22.066+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:22.065+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:22.143+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:22.132+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:22.147+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:22.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:44:31.738+0000] {processor.py:161} INFO - Started process (PID=38246) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:31.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:31.741+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:31.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:31.796+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:31.788+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:31.800+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:31.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T14:44:40.202+0000] {processor.py:161} INFO - Started process (PID=38328) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:40.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:40.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:40.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:40.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:40.251+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:40.263+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:40.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T14:44:46.823+0000] {processor.py:161} INFO - Started process (PID=38415) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:46.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:46.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:46.825+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:46.860+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:46.854+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:46.863+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:46.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T14:44:53.831+0000] {processor.py:161} INFO - Started process (PID=38496) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:53.832+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:44:53.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:53.833+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:53.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:44:53.874+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:44:53.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:44:53.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T14:45:03.473+0000] {processor.py:161} INFO - Started process (PID=38577) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:03.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:03.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:03.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:03.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:03.528+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:03.541+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:03.565+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.108 seconds
[2024-05-10T14:45:12.408+0000] {processor.py:161} INFO - Started process (PID=38658) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:12.410+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:12.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:12.411+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:12.466+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:12.458+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:12.469+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:12.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T14:45:19.477+0000] {processor.py:161} INFO - Started process (PID=38745) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:19.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:19.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:19.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:19.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:19.511+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:19.518+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:19.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T14:45:28.010+0000] {processor.py:161} INFO - Started process (PID=38827) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:28.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:28.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:28.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:28.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:28.075+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:28.088+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:28.113+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.121 seconds
[2024-05-10T14:45:37.486+0000] {processor.py:161} INFO - Started process (PID=38908) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:37.487+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:37.489+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:37.488+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:37.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:37.529+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:37.540+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:37.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T14:45:45.951+0000] {processor.py:161} INFO - Started process (PID=38989) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:45.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:45.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:45.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:46.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:46.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:46.019+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:46.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T14:45:53.463+0000] {processor.py:161} INFO - Started process (PID=39076) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:53.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:45:53.466+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:53.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:53.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:45:53.503+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:45:53.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:45:53.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T14:46:04.900+0000] {processor.py:161} INFO - Started process (PID=39157) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:04.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:04.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:04.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:04.985+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:04.975+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:04.989+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:05.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T14:46:15.154+0000] {processor.py:161} INFO - Started process (PID=39240) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:15.156+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:15.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:15.157+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:15.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:15.199+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:15.208+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:15.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T14:46:26.963+0000] {processor.py:161} INFO - Started process (PID=39327) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:26.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:26.967+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:26.966+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:27.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:27.005+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:27.013+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:27.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T14:46:36.429+0000] {processor.py:161} INFO - Started process (PID=39409) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:36.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:36.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:36.432+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:36.493+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:36.483+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:36.497+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:36.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T14:46:44.868+0000] {processor.py:161} INFO - Started process (PID=39491) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:44.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:44.871+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:44.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:44.913+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:44.907+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:44.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:44.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:46:57.114+0000] {processor.py:161} INFO - Started process (PID=39579) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:57.116+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:46:57.119+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:57.118+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:57.203+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:46:57.193+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:46:57.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:46:57.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T14:47:08.707+0000] {processor.py:161} INFO - Started process (PID=39660) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:08.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:08.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:08.709+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:08.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:08.752+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:08.760+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:08.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T14:47:16.735+0000] {processor.py:161} INFO - Started process (PID=39743) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:16.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:16.737+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:16.737+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:16.786+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:16.779+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:16.789+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:16.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T14:47:25.674+0000] {processor.py:161} INFO - Started process (PID=39831) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:25.675+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:25.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:25.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:25.720+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:25.713+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:25.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:25.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T14:47:38.255+0000] {processor.py:161} INFO - Started process (PID=39914) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:38.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:38.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:38.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:38.313+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:38.306+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:38.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:38.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T14:47:45.895+0000] {processor.py:161} INFO - Started process (PID=39996) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:45.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:45.898+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:45.898+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:46.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:46.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:46.013+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:46.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T14:47:53.883+0000] {processor.py:161} INFO - Started process (PID=40084) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:53.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:47:53.887+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:53.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:53.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:47:53.921+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:47:53.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:47:53.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T14:48:01.256+0000] {processor.py:161} INFO - Started process (PID=40166) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:01.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:01.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:01.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:01.297+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:01.290+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:48:01.298+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:01.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T14:48:13.040+0000] {processor.py:161} INFO - Started process (PID=40248) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:13.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:13.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:13.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:13.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:13.097+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 143, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable mysql_to_hive does not exist'
[2024-05-10T14:48:13.107+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:13.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T14:48:23.029+0000] {processor.py:161} INFO - Started process (PID=40337) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:23.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:23.032+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:23.085+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:23.239+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.238+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:etl_mysql_to_hive_all
[2024-05-10T14:48:23.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.257+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:etl_mysql_to_hive_all
[2024-05-10T14:48:23.269+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.269+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:etl_mysql_to_hive_all
[2024-05-10T14:48:23.271+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.270+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:48:23.291+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.290+0000] {dag.py:3118} INFO - Creating ORM DAG for etl_mysql_to_hive_all
[2024-05-10T14:48:23.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:23.304+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:48:23.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.308 seconds
[2024-05-10T14:48:34.755+0000] {processor.py:161} INFO - Started process (PID=40419) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:34.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:34.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:34.757+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:34.820+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:34.827+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:34.826+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:48:34.877+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:34.877+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:48:34.920+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.184 seconds
[2024-05-10T14:48:44.499+0000] {processor.py:161} INFO - Started process (PID=40501) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:44.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:44.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:44.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:44.565+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:44.572+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:44.572+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:48:44.616+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:44.616+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:48:44.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.169 seconds
[2024-05-10T14:48:54.054+0000] {processor.py:161} INFO - Started process (PID=40590) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:54.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:48:54.057+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:54.056+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:54.095+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:48:54.113+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:54.113+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:48:54.137+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:48:54.137+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:48:54.159+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.112 seconds
[2024-05-10T14:49:02.551+0000] {processor.py:161} INFO - Started process (PID=40673) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:02.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:02.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:02.554+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:02.602+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:02.621+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:02.620+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:02.657+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:02.657+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:02.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.148 seconds
[2024-05-10T14:49:11.466+0000] {processor.py:161} INFO - Started process (PID=40755) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:11.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:11.470+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:11.469+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:11.568+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:11.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:11.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:11.649+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:11.649+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:11.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.244 seconds
[2024-05-10T14:49:22.532+0000] {processor.py:161} INFO - Started process (PID=40843) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:22.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:22.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:22.535+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:22.589+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:22.614+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:22.613+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:22.666+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:22.665+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:22.705+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.186 seconds
[2024-05-10T14:49:32.793+0000] {processor.py:161} INFO - Started process (PID=40925) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:32.794+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:32.795+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:32.795+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:32.845+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:32.873+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:32.873+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:32.924+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:32.924+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:32.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.174 seconds
[2024-05-10T14:49:44.094+0000] {processor.py:161} INFO - Started process (PID=41007) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:44.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:44.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:44.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:44.159+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:44.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:44.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:44.225+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:44.224+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:44.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.178 seconds
[2024-05-10T14:49:52.505+0000] {processor.py:161} INFO - Started process (PID=41096) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:52.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:49:52.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:52.507+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:52.560+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:49:52.580+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:52.580+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:49:52.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:49:52.616+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:49:52.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T14:50:00.297+0000] {processor.py:161} INFO - Started process (PID=41178) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:00.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:00.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:00.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:00.386+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:00.421+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:00.420+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:00.482+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:00.482+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:00.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.245 seconds
[2024-05-10T14:50:14.622+0000] {processor.py:161} INFO - Started process (PID=41260) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:14.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:14.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:14.625+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:14.711+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:14.739+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:14.738+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:14.787+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:14.786+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:14.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.209 seconds
[2024-05-10T14:50:23.081+0000] {processor.py:161} INFO - Started process (PID=41348) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:23.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:23.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:23.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:23.128+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:23.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:23.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:23.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:23.176+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:23.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T14:50:30.857+0000] {processor.py:161} INFO - Started process (PID=41430) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:30.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:30.860+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:30.860+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:30.905+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:30.925+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:30.925+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:30.962+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:30.962+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:30.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T14:50:39.236+0000] {processor.py:161} INFO - Started process (PID=41512) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:39.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:39.239+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:39.239+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:39.291+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:39.337+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:39.336+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:39.385+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:39.384+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:39.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.187 seconds
[2024-05-10T14:50:48.549+0000] {processor.py:161} INFO - Started process (PID=41600) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:48.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:48.552+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:48.551+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:48.592+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:48.609+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:48.609+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:48.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:48.640+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:48.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:50:55.319+0000] {processor.py:161} INFO - Started process (PID=41683) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:55.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:50:55.322+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:55.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:55.364+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:50:55.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:55.379+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:50:55.407+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:50:55.407+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:50:55.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T14:51:02.612+0000] {processor.py:161} INFO - Started process (PID=41765) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:02.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:02.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:02.614+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:02.669+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:02.693+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:02.693+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:02.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:02.734+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:02.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.172 seconds
[2024-05-10T14:51:10.358+0000] {processor.py:161} INFO - Started process (PID=41847) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:10.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:10.361+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:10.361+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:10.408+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:10.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:10.434+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:10.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:10.467+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:10.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T14:51:22.122+0000] {processor.py:161} INFO - Started process (PID=41936) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:22.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:22.126+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:22.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:22.182+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:22.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:22.204+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:22.238+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:22.238+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:22.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T14:51:30.482+0000] {processor.py:161} INFO - Started process (PID=42018) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:30.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:30.485+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:30.484+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:30.532+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:30.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:30.549+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:30.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:30.583+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:30.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T14:51:38.339+0000] {processor.py:161} INFO - Started process (PID=42100) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:38.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:38.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:38.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:38.380+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:38.395+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:38.395+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:38.423+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:38.422+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:38.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T14:51:45.812+0000] {processor.py:161} INFO - Started process (PID=42182) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:45.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:45.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:45.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:45.861+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:45.879+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:45.879+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:45.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:45.912+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:45.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.143 seconds
[2024-05-10T14:51:57.433+0000] {processor.py:161} INFO - Started process (PID=42270) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:57.435+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:51:57.437+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:57.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:57.506+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:51:57.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:57.536+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:51:57.586+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:51:57.586+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:51:57.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.206 seconds
[2024-05-10T14:52:07.419+0000] {processor.py:161} INFO - Started process (PID=42353) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:07.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:52:07.422+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:07.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:07.459+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:07.478+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:07.477+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:52:07.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:07.508+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:52:07.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.120 seconds
[2024-05-10T14:52:15.166+0000] {processor.py:161} INFO - Started process (PID=42436) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:15.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:52:15.169+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:15.169+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:15.213+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:15.233+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:15.233+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:52:15.266+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:15.266+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:52:15.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T14:52:24.707+0000] {processor.py:161} INFO - Started process (PID=42524) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:24.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:52:24.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:24.710+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:24.763+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:24.786+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:24.786+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:52:24.835+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:24.834+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:52:24.871+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.176 seconds
[2024-05-10T14:52:40.817+0000] {processor.py:161} INFO - Started process (PID=42606) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:40.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:52:40.821+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:40.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:40.907+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:40.936+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:40.935+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:52:40.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:40.987+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:52:41.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.242 seconds
[2024-05-10T14:52:53.715+0000] {processor.py:161} INFO - Started process (PID=42694) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:53.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:52:53.718+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:53.718+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:53.762+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:52:53.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:53.780+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:52:53.812+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:52:53.812+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:52:53.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T14:53:02.119+0000] {processor.py:161} INFO - Started process (PID=42777) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:02.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:02.122+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:02.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:02.169+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:02.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:02.190+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:02.227+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:02.226+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:02.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T14:53:10.112+0000] {processor.py:161} INFO - Started process (PID=42860) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:10.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:10.116+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:10.115+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:10.168+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:10.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:10.188+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:10.224+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:10.224+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:10.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T14:53:19.096+0000] {processor.py:161} INFO - Started process (PID=42949) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:19.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:19.100+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:19.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:19.152+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:19.172+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:19.172+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:19.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:19.206+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:19.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T14:53:27.608+0000] {processor.py:161} INFO - Started process (PID=43031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:27.609+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:27.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:27.610+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:27.652+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:27.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:27.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:27.698+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:27.697+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:27.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T14:53:35.153+0000] {processor.py:161} INFO - Started process (PID=43113) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:35.154+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:35.156+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:35.155+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:35.199+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:35.218+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:35.218+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:35.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:35.245+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:35.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T14:53:43.002+0000] {processor.py:161} INFO - Started process (PID=43196) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:43.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:43.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:43.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:43.051+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:43.069+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:43.069+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:43.099+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:43.099+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:43.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T14:53:52.026+0000] {processor.py:161} INFO - Started process (PID=43284) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:52.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:52.029+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:52.029+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:52.071+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:52.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:52.086+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:52.118+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:52.118+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:52.144+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T14:53:59.025+0000] {processor.py:161} INFO - Started process (PID=43366) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:59.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:53:59.028+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:59.028+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:59.073+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:53:59.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:59.091+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:53:59.130+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:53:59.130+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:53:59.164+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T14:54:06.482+0000] {processor.py:161} INFO - Started process (PID=43448) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:06.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:06.484+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:06.484+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:06.532+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:06.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:06.548+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:06.580+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:06.580+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:06.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T14:54:14.177+0000] {processor.py:161} INFO - Started process (PID=43530) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:14.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:14.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:14.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:14.242+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:14.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:14.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:14.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:14.301+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:14.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.170 seconds
[2024-05-10T14:54:21.214+0000] {processor.py:161} INFO - Started process (PID=43618) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:21.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:21.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:21.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:21.272+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:21.291+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:21.290+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:21.323+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:21.323+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:21.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.150 seconds
[2024-05-10T14:54:29.830+0000] {processor.py:161} INFO - Started process (PID=43700) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:29.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:29.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:29.832+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:29.874+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:29.892+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:29.892+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:29.919+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:29.918+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:29.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T14:54:38.459+0000] {processor.py:161} INFO - Started process (PID=43783) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:38.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:38.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:38.462+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:38.520+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:38.540+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:38.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:38.581+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:38.581+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:38.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T14:54:45.371+0000] {processor.py:161} INFO - Started process (PID=43865) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:45.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:45.374+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:45.374+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:45.424+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:45.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:45.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:45.471+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:45.471+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:45.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T14:54:52.405+0000] {processor.py:161} INFO - Started process (PID=43953) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:52.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:54:52.408+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:52.408+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:52.453+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:54:52.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:52.469+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:54:52.496+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:54:52.496+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:54:52.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T14:55:01.102+0000] {processor.py:161} INFO - Started process (PID=44035) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:01.104+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:01.105+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:01.105+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:01.149+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:01.167+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:01.167+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:01.197+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:01.197+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:01.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.128 seconds
[2024-05-10T14:55:09.678+0000] {processor.py:161} INFO - Started process (PID=44118) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:09.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:09.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:09.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:09.726+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:09.745+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:09.745+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:09.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:09.784+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:09.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T14:55:18.597+0000] {processor.py:161} INFO - Started process (PID=44206) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:18.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:18.601+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:18.601+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:18.648+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:18.666+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:18.666+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:18.696+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:18.695+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:18.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T14:55:26.277+0000] {processor.py:161} INFO - Started process (PID=44288) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:26.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:26.280+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:26.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:26.321+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:26.337+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:26.336+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:26.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:26.364+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:26.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T14:55:34.679+0000] {processor.py:161} INFO - Started process (PID=44370) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:34.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:34.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:34.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:34.735+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:34.754+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:34.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:34.808+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:34.808+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:34.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.175 seconds
[2024-05-10T14:55:45.411+0000] {processor.py:161} INFO - Started process (PID=44452) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:45.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:45.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:45.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:45.463+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:45.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:45.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:45.519+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:45.518+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:45.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T14:55:53.015+0000] {processor.py:161} INFO - Started process (PID=44540) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:53.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:55:53.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:53.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:53.068+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:55:53.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:53.086+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:55:53.119+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:55:53.119+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:55:53.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.148 seconds
[2024-05-10T14:56:01.549+0000] {processor.py:161} INFO - Started process (PID=44622) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:01.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:01.552+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:01.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:01.605+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:01.630+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:01.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:01.665+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:01.665+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:01.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T14:56:11.011+0000] {processor.py:161} INFO - Started process (PID=44704) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:11.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:11.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:11.013+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:11.060+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:11.080+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:11.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:11.115+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:11.114+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:11.146+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T14:56:20.017+0000] {processor.py:161} INFO - Started process (PID=44793) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:20.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:20.021+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:20.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:20.081+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:20.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:20.104+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:20.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:20.144+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:20.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.174 seconds
[2024-05-10T14:56:27.997+0000] {processor.py:161} INFO - Started process (PID=44875) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:27.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:28.000+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:28.000+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:28.048+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:28.065+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:28.065+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:28.098+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:28.098+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:28.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T14:56:36.360+0000] {processor.py:161} INFO - Started process (PID=44957) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:36.362+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:36.363+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:36.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:36.419+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:36.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:36.440+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:36.475+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:36.474+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:36.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T14:56:43.997+0000] {processor.py:161} INFO - Started process (PID=45040) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:43.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:44.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:44.000+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:44.054+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:44.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:44.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:44.116+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:44.116+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:44.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.163 seconds
[2024-05-10T14:56:51.857+0000] {processor.py:161} INFO - Started process (PID=45128) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:51.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:51.861+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:51.861+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:51.910+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:51.931+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:51.931+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:51.968+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:51.967+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:51.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T14:56:59.277+0000] {processor.py:161} INFO - Started process (PID=45210) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:59.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:56:59.281+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:59.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:59.324+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:56:59.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:59.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:56:59.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:56:59.371+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:56:59.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T14:57:06.617+0000] {processor.py:161} INFO - Started process (PID=45292) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:06.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:06.620+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:06.619+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:06.670+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:06.687+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:06.687+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:06.716+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:06.716+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:06.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T14:57:14.954+0000] {processor.py:161} INFO - Started process (PID=45374) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:14.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:14.959+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:14.959+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:15.020+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:15.043+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:15.042+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:15.085+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:15.085+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:15.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.187 seconds
[2024-05-10T14:57:23.309+0000] {processor.py:161} INFO - Started process (PID=45462) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:23.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:23.312+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:23.311+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:23.366+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:23.393+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:23.393+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:23.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:23.430+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:23.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.166 seconds
[2024-05-10T14:57:34.651+0000] {processor.py:161} INFO - Started process (PID=45544) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:34.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:34.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:34.658+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:34.720+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:34.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:34.747+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:34.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:34.805+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:34.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.219 seconds
[2024-05-10T14:57:42.360+0000] {processor.py:161} INFO - Started process (PID=45626) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:42.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:42.363+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:42.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:42.412+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:42.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:42.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:42.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:42.453+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:42.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T14:57:50.063+0000] {processor.py:161} INFO - Started process (PID=45714) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:50.064+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:50.066+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:50.065+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:50.109+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:50.128+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:50.128+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:50.167+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:50.167+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:50.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T14:57:58.147+0000] {processor.py:161} INFO - Started process (PID=45796) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:58.148+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:57:58.150+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:58.149+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:58.192+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:57:58.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:58.209+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:57:58.237+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:57:58.236+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:57:58.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T14:58:08.111+0000] {processor.py:161} INFO - Started process (PID=45878) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:08.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:58:08.114+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:08.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:08.164+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:08.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:08.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:58:08.212+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:08.212+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:58:08.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.297 seconds
[2024-05-10T14:58:17.397+0000] {processor.py:161} INFO - Started process (PID=45960) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:17.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:58:17.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:17.400+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:17.465+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:17.490+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:17.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:58:17.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:17.527+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:58:17.565+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.181 seconds
[2024-05-10T14:58:28.052+0000] {processor.py:161} INFO - Started process (PID=46048) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:28.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:58:28.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:28.055+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:28.112+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:28.137+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:28.137+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:58:28.184+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:28.183+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:58:28.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T14:58:41.637+0000] {processor.py:161} INFO - Started process (PID=46131) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:41.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:58:41.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:41.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:41.679+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:41.852+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:41.852+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:58:41.887+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:41.886+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:58:41.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.283 seconds
[2024-05-10T14:58:52.776+0000] {processor.py:161} INFO - Started process (PID=46219) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:52.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:58:52.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:52.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:52.823+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:58:52.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:52.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:58:53.024+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:58:53.024+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:58:53.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.280 seconds
[2024-05-10T14:59:04.119+0000] {processor.py:161} INFO - Started process (PID=46301) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:04.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:04.123+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:04.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:04.182+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:04.203+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:04.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:04.240+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:04.239+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:04.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T14:59:12.357+0000] {processor.py:161} INFO - Started process (PID=46384) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:12.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:12.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:12.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:12.405+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:12.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:12.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:12.465+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:12.464+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:12.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T14:59:19.946+0000] {processor.py:161} INFO - Started process (PID=46473) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:19.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:19.949+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:19.949+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:19.999+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:20.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:20.015+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:20.046+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:20.045+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:20.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T14:59:28.532+0000] {processor.py:161} INFO - Started process (PID=46555) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:28.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:28.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:28.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:28.586+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:28.602+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:28.602+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:28.636+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:28.635+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:28.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.138 seconds
[2024-05-10T14:59:40.908+0000] {processor.py:161} INFO - Started process (PID=46637) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:40.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:40.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:40.911+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:40.967+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:40.990+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:40.989+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:41.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:41.029+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:41.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.170 seconds
[2024-05-10T14:59:51.272+0000] {processor.py:161} INFO - Started process (PID=46724) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:51.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:51.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:51.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:51.342+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:51.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:51.369+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:51.422+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:51.421+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:51.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.203 seconds
[2024-05-10T14:59:59.428+0000] {processor.py:161} INFO - Started process (PID=46806) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:59.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T14:59:59.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:59.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:59.506+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T14:59:59.530+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:59.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T14:59:59.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T14:59:59.562+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T14:59:59.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:00:06.671+0000] {processor.py:161} INFO - Started process (PID=46888) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:06.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:06.674+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:06.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:06.719+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:06.737+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:06.737+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:06.769+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:06.768+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:06.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:00:15.603+0000] {processor.py:161} INFO - Started process (PID=46970) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:15.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:15.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:15.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:15.665+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:15.688+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:15.687+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:15.733+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:15.733+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:15.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.177 seconds
[2024-05-10T15:00:22.923+0000] {processor.py:161} INFO - Started process (PID=47058) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:22.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:22.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:22.926+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:22.982+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:23.000+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:23.000+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:23.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:23.030+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:23.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T15:00:33.795+0000] {processor.py:161} INFO - Started process (PID=47140) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:33.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:33.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:33.798+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:33.857+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:33.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:33.890+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:33.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:33.942+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:33.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.209 seconds
[2024-05-10T15:00:42.731+0000] {processor.py:161} INFO - Started process (PID=47222) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:42.732+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:42.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:42.733+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:42.776+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:42.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:42.793+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:42.821+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:42.821+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:42.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T15:00:50.081+0000] {processor.py:161} INFO - Started process (PID=47311) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:50.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:50.085+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:50.084+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:50.137+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:50.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:50.158+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:50.196+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:50.195+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:50.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.162 seconds
[2024-05-10T15:00:59.318+0000] {processor.py:161} INFO - Started process (PID=47393) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:59.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:00:59.323+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:59.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:59.397+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:00:59.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:59.433+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:00:59.490+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:00:59.489+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:00:59.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.226 seconds
[2024-05-10T15:01:15.468+0000] {processor.py:161} INFO - Started process (PID=47475) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:15.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:15.472+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:15.472+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:15.524+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:15.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:15.546+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:15.586+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:15.585+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:15.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.164 seconds
[2024-05-10T15:01:23.662+0000] {processor.py:161} INFO - Started process (PID=47563) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:23.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:23.665+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:23.664+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:23.721+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:23.743+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:23.743+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:23.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:23.781+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:23.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.163 seconds
[2024-05-10T15:01:32.331+0000] {processor.py:161} INFO - Started process (PID=47645) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:32.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:32.334+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:32.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:32.388+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:32.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:32.413+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:32.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:32.451+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:32.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.162 seconds
[2024-05-10T15:01:41.879+0000] {processor.py:161} INFO - Started process (PID=47727) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:41.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:41.882+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:41.881+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:41.941+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:41.963+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:41.962+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:42.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:42.005+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:42.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T15:01:49.442+0000] {processor.py:161} INFO - Started process (PID=47815) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:49.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:49.445+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:49.444+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:49.490+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:49.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:49.505+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:49.532+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:49.532+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:49.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T15:01:56.815+0000] {processor.py:161} INFO - Started process (PID=47897) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:56.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:01:56.818+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:56.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:56.877+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:01:56.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:56.901+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:01:56.946+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:01:56.945+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:01:56.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.175 seconds
[2024-05-10T15:02:05.516+0000] {processor.py:161} INFO - Started process (PID=47979) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:05.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:05.519+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:05.518+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:05.565+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:05.584+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:05.584+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:05.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:05.616+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:05.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T15:02:14.118+0000] {processor.py:161} INFO - Started process (PID=48061) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:14.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:14.120+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:14.119+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:14.156+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:14.171+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:14.171+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:14.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:14.199+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:14.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T15:02:26.103+0000] {processor.py:161} INFO - Started process (PID=48150) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:26.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:26.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:26.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:26.191+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:26.222+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:26.222+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:26.284+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:26.283+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:26.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.236 seconds
[2024-05-10T15:02:37.988+0000] {processor.py:161} INFO - Started process (PID=48232) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:37.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:37.991+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:37.990+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:38.049+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:38.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:38.074+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:38.113+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:38.112+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:38.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T15:02:48.125+0000] {processor.py:161} INFO - Started process (PID=48314) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:48.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:48.128+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:48.128+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:48.200+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:48.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:48.233+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:48.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:48.290+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:48.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.210 seconds
[2024-05-10T15:02:58.159+0000] {processor.py:161} INFO - Started process (PID=48402) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:58.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:02:58.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:58.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:58.296+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:02:58.348+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:58.347+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:02:58.443+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:02:58.442+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:02:58.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.394 seconds
[2024-05-10T15:03:25.241+0000] {processor.py:161} INFO - Started process (PID=48490) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:25.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:03:25.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:25.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:25.399+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:25.466+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:25.465+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:03:25.572+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:25.571+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:03:25.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.430 seconds
[2024-05-10T15:03:45.720+0000] {processor.py:161} INFO - Started process (PID=48573) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:45.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:03:45.730+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:45.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:45.829+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:45.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:45.902+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:03:46.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:46.054+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:03:46.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.448 seconds
[2024-05-10T15:03:56.987+0000] {processor.py:161} INFO - Started process (PID=48662) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:56.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:03:56.991+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:56.990+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:57.034+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:03:57.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:57.051+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:03:57.083+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:03:57.082+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:03:57.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:04:11.060+0000] {processor.py:161} INFO - Started process (PID=48744) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:11.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:04:11.064+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:11.063+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:11.115+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:11.135+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:11.135+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:04:11.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:11.176+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:04:11.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.163 seconds
[2024-05-10T15:04:24.886+0000] {processor.py:161} INFO - Started process (PID=48832) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:24.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:04:24.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:24.890+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:24.981+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:25.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:25.041+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:04:25.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:25.176+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:04:25.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.386 seconds
[2024-05-10T15:04:40.651+0000] {processor.py:161} INFO - Started process (PID=48915) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:40.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:04:40.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:40.657+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:40.739+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:40.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:40.788+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:04:40.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:40.853+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:04:40.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.262 seconds
[2024-05-10T15:04:54.525+0000] {processor.py:161} INFO - Started process (PID=49004) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:54.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:04:54.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:54.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:54.603+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:04:54.635+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:54.635+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:04:54.686+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:04:54.685+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:04:54.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.209 seconds
[2024-05-10T15:05:04.715+0000] {processor.py:161} INFO - Started process (PID=49086) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:04.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:04.718+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:04.718+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:04.773+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:04.799+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:04.798+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:04.852+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:04.851+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:04.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.206 seconds
[2024-05-10T15:05:14.494+0000] {processor.py:161} INFO - Started process (PID=49168) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:14.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:14.497+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:14.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:14.541+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:14.558+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:14.558+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:14.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:14.587+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:14.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:05:22.812+0000] {processor.py:161} INFO - Started process (PID=49255) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:22.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:22.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:22.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:22.858+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:22.876+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:22.875+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:22.909+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:22.909+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:22.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:05:32.191+0000] {processor.py:161} INFO - Started process (PID=49337) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:32.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:32.194+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:32.194+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:32.241+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:32.264+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:32.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:32.303+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:32.303+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:32.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.150 seconds
[2024-05-10T15:05:40.613+0000] {processor.py:161} INFO - Started process (PID=49419) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:40.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:40.616+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:40.615+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:40.663+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:40.680+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:40.680+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:40.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:40.709+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:40.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:05:49.905+0000] {processor.py:161} INFO - Started process (PID=49501) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:49.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:49.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:49.907+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:49.973+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:49.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:49.998+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:50.036+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:50.036+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:50.063+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.172 seconds
[2024-05-10T15:05:57.925+0000] {processor.py:161} INFO - Started process (PID=49589) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:57.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:05:57.929+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:57.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:57.987+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:05:58.013+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:58.012+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:05:58.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:05:58.047+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:05:58.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.164 seconds
[2024-05-10T15:06:07.390+0000] {processor.py:161} INFO - Started process (PID=49671) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:07.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:06:07.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:07.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:07.448+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:07.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:07.468+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:06:07.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:07.503+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:06:07.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T15:06:21.837+0000] {processor.py:161} INFO - Started process (PID=49759) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:21.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:06:21.843+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:21.842+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:21.916+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:21.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:21.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:06:22.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:22.013+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:06:22.091+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.274 seconds
[2024-05-10T15:06:36.798+0000] {processor.py:161} INFO - Started process (PID=49841) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:36.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:06:36.811+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:36.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:36.957+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:37.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:37.020+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:06:37.157+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:37.157+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:06:37.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.486 seconds
[2024-05-10T15:06:48.197+0000] {processor.py:161} INFO - Started process (PID=49923) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:48.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:06:48.200+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:48.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:48.254+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:48.275+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:48.274+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:06:48.317+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:48.317+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:06:48.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T15:06:56.519+0000] {processor.py:161} INFO - Started process (PID=50011) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:56.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:06:56.522+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:56.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:56.566+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:06:56.582+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:56.582+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:06:56.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:06:56.611+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:06:56.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:07:05.882+0000] {processor.py:161} INFO - Started process (PID=50093) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:05.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:05.884+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:05.884+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:05.928+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:05.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:05.946+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:05.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:05.985+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:06.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:07:13.473+0000] {processor.py:161} INFO - Started process (PID=50175) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:13.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:13.476+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:13.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:13.523+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:13.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:13.545+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:13.606+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:13.606+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:13.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.189 seconds
[2024-05-10T15:07:23.419+0000] {processor.py:161} INFO - Started process (PID=50263) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:23.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:23.424+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:23.423+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:23.484+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:23.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:23.512+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:23.552+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:23.552+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:23.585+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.179 seconds
[2024-05-10T15:07:34.235+0000] {processor.py:161} INFO - Started process (PID=50346) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:34.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:34.241+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:34.240+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:34.319+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:34.352+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:34.352+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:34.409+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:34.408+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:34.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.242 seconds
[2024-05-10T15:07:43.185+0000] {processor.py:161} INFO - Started process (PID=50429) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:43.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:43.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:43.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:43.233+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:43.251+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:43.251+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:43.291+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:43.291+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:43.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:07:51.466+0000] {processor.py:161} INFO - Started process (PID=50517) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:51.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:07:51.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:51.468+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:51.511+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:07:51.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:51.526+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:07:51.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:07:51.554+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:07:51.580+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T15:08:00.022+0000] {processor.py:161} INFO - Started process (PID=50599) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:00.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:00.027+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:00.027+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:00.086+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:00.112+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:00.111+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:00.155+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:00.155+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:00.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.185 seconds
[2024-05-10T15:08:11.863+0000] {processor.py:161} INFO - Started process (PID=50682) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:11.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:11.867+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:11.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:11.926+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:11.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:11.953+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:11.997+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:11.996+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:12.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.185 seconds
[2024-05-10T15:08:21.669+0000] {processor.py:161} INFO - Started process (PID=50771) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:21.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:21.672+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:21.671+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:21.715+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:21.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:21.731+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:21.762+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:21.762+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:21.792+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:08:29.416+0000] {processor.py:161} INFO - Started process (PID=50853) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:29.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:29.420+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:29.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:29.468+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:29.484+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:29.484+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:29.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:29.512+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:29.541+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:08:37.935+0000] {processor.py:161} INFO - Started process (PID=50935) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:37.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:37.937+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:37.937+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:37.982+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:38.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:38.001+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:38.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:38.044+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:38.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:08:46.216+0000] {processor.py:161} INFO - Started process (PID=51017) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:46.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:46.219+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:46.219+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:46.273+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:46.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:46.295+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:46.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:46.329+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:46.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.161 seconds
[2024-05-10T15:08:53.497+0000] {processor.py:161} INFO - Started process (PID=51105) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:53.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:08:53.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:53.500+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:53.554+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:08:53.576+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:53.575+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:08:53.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:08:53.617+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:08:53.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.162 seconds
[2024-05-10T15:09:01.134+0000] {processor.py:161} INFO - Started process (PID=51187) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:01.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:01.137+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:01.137+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:01.181+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:01.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:01.199+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:01.233+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:01.233+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:01.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.138 seconds
[2024-05-10T15:09:08.794+0000] {processor.py:161} INFO - Started process (PID=51269) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:08.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:08.796+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:08.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:08.844+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:08.861+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:08.861+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:08.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:08.891+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:08.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:09:18.331+0000] {processor.py:161} INFO - Started process (PID=51351) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:18.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:18.334+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:18.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:18.387+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:18.407+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:18.407+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:18.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:18.448+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:18.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.162 seconds
[2024-05-10T15:09:26.812+0000] {processor.py:161} INFO - Started process (PID=51439) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:26.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:26.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:26.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:26.861+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:26.879+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:26.879+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:26.909+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:26.909+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:26.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:09:34.373+0000] {processor.py:161} INFO - Started process (PID=51521) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:34.374+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:34.376+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:34.376+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:34.428+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:34.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:34.450+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:34.494+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:34.494+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:34.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.170 seconds
[2024-05-10T15:09:42.376+0000] {processor.py:161} INFO - Started process (PID=51603) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:42.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:42.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:42.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:42.424+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:42.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:42.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:42.471+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:42.471+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:42.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:09:50.532+0000] {processor.py:161} INFO - Started process (PID=51685) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:50.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:09:50.538+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:50.537+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:50.595+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:09:50.620+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:50.619+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:09:50.656+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:09:50.655+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:09:50.685+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T15:10:00.432+0000] {processor.py:161} INFO - Started process (PID=51774) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:00.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:00.435+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:00.434+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:00.487+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:00.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:00.505+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:00.539+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:00.539+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:00.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T15:10:08.670+0000] {processor.py:161} INFO - Started process (PID=51856) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:08.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:08.673+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:08.672+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:08.724+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:08.743+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:08.743+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:08.778+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:08.778+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:08.805+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T15:10:17.800+0000] {processor.py:161} INFO - Started process (PID=51938) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:17.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:17.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:17.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:17.843+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:17.857+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:17.857+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:17.888+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:17.888+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:17.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T15:10:26.470+0000] {processor.py:161} INFO - Started process (PID=52026) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:26.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:26.474+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:26.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:26.527+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:26.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:26.547+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:26.584+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:26.584+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:26.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T15:10:35.128+0000] {processor.py:161} INFO - Started process (PID=52108) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:35.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:35.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:35.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:35.204+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:35.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:35.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:35.286+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:35.285+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:35.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.224 seconds
[2024-05-10T15:10:43.690+0000] {processor.py:161} INFO - Started process (PID=52190) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:43.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:43.692+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:43.692+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:43.753+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:43.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:43.776+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:43.812+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:43.812+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:43.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.161 seconds
[2024-05-10T15:10:52.607+0000] {processor.py:161} INFO - Started process (PID=52279) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:52.609+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:10:52.612+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:52.611+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:52.692+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:10:52.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:52.728+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:10:52.780+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:10:52.780+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:10:52.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.232 seconds
[2024-05-10T15:11:05.775+0000] {processor.py:161} INFO - Started process (PID=52361) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:05.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:11:05.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:05.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:05.873+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:05.918+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:05.917+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:11:06.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:06.010+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:11:06.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.310 seconds
[2024-05-10T15:11:23.393+0000] {processor.py:161} INFO - Started process (PID=52449) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:23.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:11:23.396+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:23.395+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:23.453+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:23.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:23.477+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:11:23.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:23.527+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:11:23.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T15:11:31.789+0000] {processor.py:161} INFO - Started process (PID=52531) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:31.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:11:31.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:31.792+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:31.844+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:31.864+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:31.863+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:11:31.899+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:31.899+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:11:31.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:11:41.162+0000] {processor.py:161} INFO - Started process (PID=52613) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:41.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:11:41.168+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:41.167+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:41.234+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:41.261+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:41.260+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:11:41.308+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:41.308+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:11:41.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.207 seconds
[2024-05-10T15:11:52.724+0000] {processor.py:161} INFO - Started process (PID=52701) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:52.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:11:52.727+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:52.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:52.781+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:11:52.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:52.803+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:11:52.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:11:52.848+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:11:52.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.175 seconds
[2024-05-10T15:12:04.042+0000] {processor.py:161} INFO - Started process (PID=52783) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:04.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:04.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:04.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:04.099+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:04.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:04.117+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:04.147+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:04.146+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:04.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:12:13.438+0000] {processor.py:161} INFO - Started process (PID=52865) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:13.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:13.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:13.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:13.592+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:13.636+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:13.635+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:13.736+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:13.735+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:13.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.371 seconds
[2024-05-10T15:12:27.611+0000] {processor.py:161} INFO - Started process (PID=52953) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:27.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:27.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:27.615+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:27.676+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:27.699+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:27.699+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:27.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:27.742+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:27.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.184 seconds
[2024-05-10T15:12:37.083+0000] {processor.py:161} INFO - Started process (PID=53035) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:37.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:37.086+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:37.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:37.143+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:37.169+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:37.168+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:37.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:37.217+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:37.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.183 seconds
[2024-05-10T15:12:45.054+0000] {processor.py:161} INFO - Started process (PID=53117) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:45.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:45.058+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:45.058+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:45.141+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:45.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:45.189+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:45.279+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:45.278+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:45.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.289 seconds
[2024-05-10T15:12:53.869+0000] {processor.py:161} INFO - Started process (PID=53205) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:53.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:12:53.872+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:53.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:53.913+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:12:53.931+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:53.930+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:12:53.962+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:12:53.961+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:12:53.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:13:04.298+0000] {processor.py:161} INFO - Started process (PID=53288) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:04.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:04.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:04.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:04.383+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:04.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:04.412+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:04.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:04.469+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:04.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.215 seconds
[2024-05-10T15:13:13.352+0000] {processor.py:161} INFO - Started process (PID=53371) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:13.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:13.355+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:13.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:13.421+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:13.448+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:13.448+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:13.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:13.500+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:13.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.207 seconds
[2024-05-10T15:13:20.784+0000] {processor.py:161} INFO - Started process (PID=53453) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:20.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:20.788+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:20.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:20.844+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:20.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:20.868+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:20.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:20.912+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:20.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.184 seconds
[2024-05-10T15:13:28.877+0000] {processor.py:161} INFO - Started process (PID=53541) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:28.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:28.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:28.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:28.927+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:28.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:28.945+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:28.975+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:28.974+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:29.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:13:38.269+0000] {processor.py:161} INFO - Started process (PID=53623) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:38.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:38.272+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:38.272+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:38.321+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:38.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:38.340+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:38.382+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:38.381+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:38.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:13:45.987+0000] {processor.py:161} INFO - Started process (PID=53705) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:45.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:45.990+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:45.990+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:46.041+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:46.061+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:46.060+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:46.109+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:46.108+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:46.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.166 seconds
[2024-05-10T15:13:53.119+0000] {processor.py:161} INFO - Started process (PID=53793) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:53.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:13:53.122+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:53.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:53.171+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:13:53.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:53.189+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:13:53.220+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:13:53.220+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:13:53.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T15:14:00.684+0000] {processor.py:161} INFO - Started process (PID=53875) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:00.685+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:00.687+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:00.686+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:00.738+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:00.757+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:00.757+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:00.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:00.789+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:00.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.141 seconds
[2024-05-10T15:14:08.898+0000] {processor.py:161} INFO - Started process (PID=53957) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:08.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:08.901+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:08.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:08.950+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:08.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:08.968+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:09.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:09.011+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:09.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:14:19.197+0000] {processor.py:161} INFO - Started process (PID=54039) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:19.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:19.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:19.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:19.239+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:19.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:19.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:19.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:19.286+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:19.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T15:14:28.446+0000] {processor.py:161} INFO - Started process (PID=54128) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:28.449+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:28.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:28.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:28.540+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:28.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:28.569+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:28.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:28.636+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:28.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.311 seconds
[2024-05-10T15:14:38.571+0000] {processor.py:161} INFO - Started process (PID=54210) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:38.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:38.575+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:38.574+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:38.637+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:38.662+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:38.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:38.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:38.700+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:38.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.177 seconds
[2024-05-10T15:14:48.046+0000] {processor.py:161} INFO - Started process (PID=54292) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:48.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:48.050+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:48.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:48.108+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:48.138+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:48.137+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:48.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:48.205+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:48.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.216 seconds
[2024-05-10T15:14:56.781+0000] {processor.py:161} INFO - Started process (PID=54381) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:56.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:14:56.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:56.783+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:56.833+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:14:56.857+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:56.857+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:14:56.898+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:14:56.898+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:14:56.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:15:05.982+0000] {processor.py:161} INFO - Started process (PID=54463) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:05.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:05.985+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:05.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:06.035+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:06.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:06.055+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:06.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:06.091+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:06.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:15:18.263+0000] {processor.py:161} INFO - Started process (PID=54546) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:18.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:18.267+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:18.266+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:18.318+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:18.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:18.342+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:18.388+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:18.387+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:18.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.176 seconds
[2024-05-10T15:15:28.030+0000] {processor.py:161} INFO - Started process (PID=54636) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:28.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:28.033+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:28.033+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:28.078+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:28.096+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:28.096+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:28.125+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:28.125+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:28.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.129 seconds
[2024-05-10T15:15:36.186+0000] {processor.py:161} INFO - Started process (PID=54718) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:36.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:36.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:36.189+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:36.244+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:36.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:36.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:36.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:36.295+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:36.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:15:44.804+0000] {processor.py:161} INFO - Started process (PID=54800) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:44.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:44.808+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:44.807+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:44.870+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:44.892+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:44.891+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:44.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:44.925+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:44.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.164 seconds
[2024-05-10T15:15:54.236+0000] {processor.py:161} INFO - Started process (PID=54888) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:54.237+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:15:54.238+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:54.238+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:54.284+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:15:54.302+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:54.301+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:15:54.335+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:15:54.335+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:15:54.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:16:04.247+0000] {processor.py:161} INFO - Started process (PID=54970) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:04.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:04.249+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:04.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:04.301+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:04.321+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:04.320+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:04.364+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:04.364+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:04.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.166 seconds
[2024-05-10T15:16:13.812+0000] {processor.py:161} INFO - Started process (PID=55052) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:13.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:13.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:13.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:13.880+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:13.950+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:13.950+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:14.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:14.029+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:14.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.266 seconds
[2024-05-10T15:16:22.580+0000] {processor.py:161} INFO - Started process (PID=55141) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:22.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:22.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:22.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:22.643+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:22.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:22.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:22.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:22.711+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:22.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T15:16:32.370+0000] {processor.py:161} INFO - Started process (PID=55223) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:32.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:32.373+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:32.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:32.411+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:32.428+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:32.428+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:32.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:32.456+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:32.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T15:16:47.926+0000] {processor.py:161} INFO - Started process (PID=55306) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:47.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:47.930+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:47.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:47.987+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:48.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:48.011+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:48.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:48.054+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:48.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.179 seconds
[2024-05-10T15:16:59.160+0000] {processor.py:161} INFO - Started process (PID=55394) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:59.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:16:59.165+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:59.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:59.237+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:16:59.276+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:59.276+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:16:59.335+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:16:59.334+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:16:59.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.470 seconds
[2024-05-10T15:17:10.262+0000] {processor.py:161} INFO - Started process (PID=55476) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:10.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:17:10.265+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:10.264+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:10.308+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:10.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:10.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:17:10.363+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:10.362+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:17:10.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T15:17:23.882+0000] {processor.py:161} INFO - Started process (PID=55564) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:23.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:17:23.887+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:23.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:23.944+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:23.965+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:23.965+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:17:24.017+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:24.017+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:17:24.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.179 seconds
[2024-05-10T15:17:36.285+0000] {processor.py:161} INFO - Started process (PID=55646) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:36.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:17:36.288+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:36.288+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:36.338+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:36.360+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:36.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:17:36.399+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:36.398+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:17:36.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:17:45.232+0000] {processor.py:161} INFO - Started process (PID=55729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:45.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:17:45.235+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:45.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:45.276+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:45.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:45.295+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:17:45.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:45.327+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:17:45.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.280 seconds
[2024-05-10T15:17:57.229+0000] {processor.py:161} INFO - Started process (PID=55818) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:57.232+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:17:57.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:57.233+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:57.303+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:17:57.335+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:57.334+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:17:57.376+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:17:57.375+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:17:57.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.204 seconds
[2024-05-10T15:18:11.679+0000] {processor.py:161} INFO - Started process (PID=55900) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:11.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:18:11.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:11.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:11.730+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:11.751+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:11.750+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:18:11.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:11.784+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:18:11.815+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:18:25.118+0000] {processor.py:161} INFO - Started process (PID=55989) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:25.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:18:25.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:25.120+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:25.172+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:25.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:25.389+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:18:25.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:25.432+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:18:25.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.376 seconds
[2024-05-10T15:18:40.769+0000] {processor.py:161} INFO - Started process (PID=56071) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:40.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:18:40.776+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:40.775+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:40.871+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:40.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:40.902+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:18:41.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:41.358+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:18:41.396+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.663 seconds
[2024-05-10T15:18:53.683+0000] {processor.py:161} INFO - Started process (PID=56159) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:53.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:18:53.686+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:53.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:53.729+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:18:53.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:53.747+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:18:53.778+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:18:53.778+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:18:53.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T15:19:02.302+0000] {processor.py:161} INFO - Started process (PID=56241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:02.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:02.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:02.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:02.354+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:02.374+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:02.373+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:02.415+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:02.415+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:02.445+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T15:19:09.482+0000] {processor.py:161} INFO - Started process (PID=56323) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:09.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:09.486+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:09.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:09.547+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:09.571+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:09.571+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:09.619+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:09.619+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:09.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.193 seconds
[2024-05-10T15:19:18.685+0000] {processor.py:161} INFO - Started process (PID=56405) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:18.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:18.688+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:18.688+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:18.736+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:18.753+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:18.753+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:18.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:18.782+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:18.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:19:25.957+0000] {processor.py:161} INFO - Started process (PID=56493) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:25.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:25.960+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:25.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:26.010+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:26.027+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:26.027+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:26.060+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:26.060+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:26.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T15:19:32.710+0000] {processor.py:161} INFO - Started process (PID=56575) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:32.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:32.714+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:32.713+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:32.758+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:32.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:32.774+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:32.804+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:32.803+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:32.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T15:19:40.641+0000] {processor.py:161} INFO - Started process (PID=56657) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:40.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:40.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:40.644+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:40.693+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:40.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:40.713+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:40.744+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:40.744+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:40.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:19:48.789+0000] {processor.py:161} INFO - Started process (PID=56739) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:48.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:48.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:48.792+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:48.851+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:48.875+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:48.874+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:48.910+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:48.910+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:48.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.163 seconds
[2024-05-10T15:19:55.881+0000] {processor.py:161} INFO - Started process (PID=56827) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:55.882+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:19:55.884+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:55.883+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:55.927+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:19:55.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:55.944+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:19:55.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:19:55.971+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:19:56.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T15:20:02.995+0000] {processor.py:161} INFO - Started process (PID=56909) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:02.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:02.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:02.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:03.048+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:03.071+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:03.071+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:03.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:03.107+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:03.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:20:09.728+0000] {processor.py:161} INFO - Started process (PID=56991) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:09.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:09.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:09.730+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:09.781+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:09.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:09.798+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:09.828+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:09.827+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:09.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T15:20:17.320+0000] {processor.py:161} INFO - Started process (PID=57073) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:17.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:17.323+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:17.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:17.377+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:17.398+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:17.398+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:17.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:17.438+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:17.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.169 seconds
[2024-05-10T15:20:26.182+0000] {processor.py:161} INFO - Started process (PID=57161) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:26.183+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:26.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:26.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:26.229+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:26.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:26.244+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:26.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:26.285+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:26.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:20:32.981+0000] {processor.py:161} INFO - Started process (PID=57243) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:32.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:32.984+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:32.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:33.032+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:33.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:33.053+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:33.093+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:33.093+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:33.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:20:40.021+0000] {processor.py:161} INFO - Started process (PID=57325) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:40.023+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:40.025+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:40.024+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:40.083+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:40.103+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:40.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:40.136+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:40.135+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:40.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T15:20:46.621+0000] {processor.py:161} INFO - Started process (PID=57407) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:46.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:46.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:46.623+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:46.666+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:46.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:46.683+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:46.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:46.712+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:46.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:20:54.473+0000] {processor.py:161} INFO - Started process (PID=57495) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:54.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:20:54.476+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:54.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:54.526+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:20:54.543+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:54.542+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:20:54.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:20:54.578+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:20:54.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T15:21:03.763+0000] {processor.py:161} INFO - Started process (PID=57577) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:03.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:03.766+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:03.765+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:03.821+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:03.844+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:03.844+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:03.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:03.885+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:03.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.169 seconds
[2024-05-10T15:21:14.179+0000] {processor.py:161} INFO - Started process (PID=57659) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:14.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:14.182+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:14.182+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:14.232+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:14.250+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:14.249+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:14.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:14.289+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:14.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:21:21.521+0000] {processor.py:161} INFO - Started process (PID=57741) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:21.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:21.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:21.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:21.575+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:21.594+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:21.594+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:21.627+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:21.627+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:21.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T15:21:29.761+0000] {processor.py:161} INFO - Started process (PID=57829) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:29.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:29.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:29.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:29.813+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:29.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:29.832+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:29.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:29.869+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:29.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:21:38.953+0000] {processor.py:161} INFO - Started process (PID=57911) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:38.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:38.955+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:38.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:38.989+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:39.006+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:39.006+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:39.032+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:39.032+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:39.055+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T15:21:47.442+0000] {processor.py:161} INFO - Started process (PID=57994) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:47.444+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:47.445+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:47.445+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:47.491+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:47.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:47.506+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:47.534+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:47.533+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:47.558+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T15:21:54.592+0000] {processor.py:161} INFO - Started process (PID=58083) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:54.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:21:54.595+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:54.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:54.648+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:21:54.668+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:54.668+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:21:54.705+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:21:54.705+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:21:54.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T15:22:02.241+0000] {processor.py:161} INFO - Started process (PID=58165) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:02.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:02.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:02.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:02.296+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:02.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:02.327+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:02.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:02.365+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:02.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.167 seconds
[2024-05-10T15:22:10.477+0000] {processor.py:161} INFO - Started process (PID=58247) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:10.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:10.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:10.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:10.532+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:10.552+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:10.552+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:10.590+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:10.590+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:10.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:22:20.786+0000] {processor.py:161} INFO - Started process (PID=58329) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:20.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:20.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:20.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:20.837+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:20.855+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:20.855+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:20.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:20.890+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:20.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T15:22:28.636+0000] {processor.py:161} INFO - Started process (PID=58417) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:28.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:28.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:28.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:28.721+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:28.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:28.783+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:28.857+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:28.857+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:28.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.272 seconds
[2024-05-10T15:22:37.614+0000] {processor.py:161} INFO - Started process (PID=58499) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:37.615+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:37.618+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:37.617+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:37.677+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:37.699+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:37.699+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:37.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:37.742+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:37.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:22:46.051+0000] {processor.py:161} INFO - Started process (PID=58581) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:46.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:46.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:46.053+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:46.109+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:46.133+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:46.133+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:46.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:46.170+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:46.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.168 seconds
[2024-05-10T15:22:53.812+0000] {processor.py:161} INFO - Started process (PID=58669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:53.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:22:53.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:53.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:53.865+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:22:53.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:53.883+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:22:53.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:22:53.916+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:22:53.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.143 seconds
[2024-05-10T15:23:00.907+0000] {processor.py:161} INFO - Started process (PID=58751) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:00.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:00.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:00.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:00.965+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:00.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:00.985+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:01.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:01.018+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:01.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:23:08.233+0000] {processor.py:161} INFO - Started process (PID=58833) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:08.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:08.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:08.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:08.283+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:08.299+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:08.299+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:08.331+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:08.331+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:08.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T15:23:17.480+0000] {processor.py:161} INFO - Started process (PID=58915) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:17.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:17.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:17.482+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:17.531+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:17.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:17.553+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:17.594+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:17.593+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:17.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:23:25.476+0000] {processor.py:161} INFO - Started process (PID=59003) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:25.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:25.482+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:25.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:25.541+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:25.569+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:25.569+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:25.636+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:25.634+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:25.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.216 seconds
[2024-05-10T15:23:33.131+0000] {processor.py:161} INFO - Started process (PID=59085) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:33.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:33.136+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:33.135+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:33.198+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:33.223+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:33.222+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:33.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:33.259+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:33.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.174 seconds
[2024-05-10T15:23:40.775+0000] {processor.py:161} INFO - Started process (PID=59167) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:40.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:40.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:40.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:40.864+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:40.905+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:40.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:40.974+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:40.973+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:41.032+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.275 seconds
[2024-05-10T15:23:50.601+0000] {processor.py:161} INFO - Started process (PID=59249) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:50.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:50.603+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:50.603+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:50.651+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:50.670+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:50.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:50.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:50.706+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:50.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:23:58.370+0000] {processor.py:161} INFO - Started process (PID=59337) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:58.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:23:58.373+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:58.373+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:58.423+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:23:58.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:58.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:23:58.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:23:58.473+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:23:58.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.142 seconds
[2024-05-10T15:24:06.804+0000] {processor.py:161} INFO - Started process (PID=59419) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:06.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:06.806+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:06.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:06.842+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:06.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:06.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:06.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:06.885+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:06.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T15:24:14.225+0000] {processor.py:161} INFO - Started process (PID=59502) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:14.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:14.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:14.228+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:14.277+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:14.329+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:14.321+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:14.395+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:14.394+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:14.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.218 seconds
[2024-05-10T15:24:22.259+0000] {processor.py:161} INFO - Started process (PID=59584) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:22.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:22.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:22.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:22.310+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:22.331+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:22.331+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:22.395+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:22.395+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:22.445+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.197 seconds
[2024-05-10T15:24:30.549+0000] {processor.py:161} INFO - Started process (PID=59672) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:30.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:30.553+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:30.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:30.599+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:30.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:30.616+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:30.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:30.646+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:30.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:24:39.304+0000] {processor.py:161} INFO - Started process (PID=59754) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:39.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:39.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:39.306+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:39.343+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:39.358+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:39.358+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:39.382+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:39.382+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:39.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T15:24:46.450+0000] {processor.py:161} INFO - Started process (PID=59836) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:46.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:46.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:46.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:46.503+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:46.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:46.523+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:46.556+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:46.556+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:46.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T15:24:53.847+0000] {processor.py:161} INFO - Started process (PID=59925) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:53.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:24:53.850+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:53.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:53.899+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:24:53.921+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:53.921+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:24:53.951+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:24:53.951+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:24:53.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.141 seconds
[2024-05-10T15:25:03.331+0000] {processor.py:161} INFO - Started process (PID=60007) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:03.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:03.334+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:03.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:03.386+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:03.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:03.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:03.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:03.451+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:03.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T15:25:13.264+0000] {processor.py:161} INFO - Started process (PID=60089) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:13.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:13.267+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:13.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:13.328+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:13.352+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:13.352+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:13.393+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:13.392+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:13.442+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.189 seconds
[2024-05-10T15:25:21.634+0000] {processor.py:161} INFO - Started process (PID=60171) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:21.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:21.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:21.636+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:21.688+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:21.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:21.708+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:21.745+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:21.745+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:21.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.160 seconds
[2024-05-10T15:25:30.106+0000] {processor.py:161} INFO - Started process (PID=60259) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:30.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:30.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:30.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:30.160+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:30.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:30.176+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:30.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:30.206+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:30.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:25:39.607+0000] {processor.py:161} INFO - Started process (PID=60341) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:39.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:39.610+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:39.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:39.658+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:39.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:39.676+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:39.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:39.710+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:39.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:25:48.481+0000] {processor.py:161} INFO - Started process (PID=60423) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:48.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:48.485+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:48.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:48.586+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:48.638+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:48.637+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:48.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:48.677+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:48.707+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.242 seconds
[2024-05-10T15:25:55.778+0000] {processor.py:161} INFO - Started process (PID=60511) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:55.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:25:55.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:55.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:55.827+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:25:55.846+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:55.845+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:25:55.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:25:55.886+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:25:55.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:26:03.986+0000] {processor.py:161} INFO - Started process (PID=60593) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:03.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:03.990+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:03.989+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:04.041+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:04.059+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:04.058+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:04.090+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:04.090+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:04.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:26:12.479+0000] {processor.py:161} INFO - Started process (PID=60675) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:12.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:12.481+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:12.481+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:12.525+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:12.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:12.549+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:12.594+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:12.594+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:12.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.168 seconds
[2024-05-10T15:26:21.129+0000] {processor.py:161} INFO - Started process (PID=60757) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:21.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:21.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:21.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:21.183+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:21.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:21.204+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:21.239+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:21.239+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:21.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T15:26:28.673+0000] {processor.py:161} INFO - Started process (PID=60845) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:28.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:28.675+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:28.675+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:28.723+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:28.740+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:28.740+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:28.773+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:28.772+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:28.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.150 seconds
[2024-05-10T15:26:37.438+0000] {processor.py:161} INFO - Started process (PID=60927) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:37.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:37.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:37.441+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:37.491+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:37.511+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:37.510+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:37.544+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:37.543+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:37.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.148 seconds
[2024-05-10T15:26:45.301+0000] {processor.py:161} INFO - Started process (PID=61009) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:45.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:45.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:45.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:45.369+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:45.402+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:45.401+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:45.447+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:45.447+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:45.490+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.201 seconds
[2024-05-10T15:26:52.738+0000] {processor.py:161} INFO - Started process (PID=61091) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:52.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:26:52.741+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:52.741+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:52.813+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:26:52.844+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:52.844+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:26:52.884+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:26:52.884+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:26:52.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.191 seconds
[2024-05-10T15:27:00.213+0000] {processor.py:161} INFO - Started process (PID=61179) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:00.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:00.216+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:00.215+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:00.264+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:00.283+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:00.283+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:00.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:00.314+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:00.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.139 seconds
[2024-05-10T15:27:07.765+0000] {processor.py:161} INFO - Started process (PID=61261) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:07.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:07.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:07.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:07.813+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:07.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:07.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:07.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:07.857+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:07.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T15:27:18.296+0000] {processor.py:161} INFO - Started process (PID=61343) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:18.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:18.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:18.299+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:18.373+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:18.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:18.400+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:18.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:18.439+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:18.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.199 seconds
[2024-05-10T15:27:25.713+0000] {processor.py:161} INFO - Started process (PID=61431) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:25.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:25.716+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:25.715+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:25.767+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:25.788+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:25.787+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:25.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:25.829+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:25.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.178 seconds
[2024-05-10T15:27:35.036+0000] {processor.py:161} INFO - Started process (PID=61513) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:35.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:35.038+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:35.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:35.094+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:35.115+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:35.115+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:35.157+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:35.156+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:35.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.161 seconds
[2024-05-10T15:27:42.403+0000] {processor.py:161} INFO - Started process (PID=61595) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:42.404+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:42.405+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:42.405+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:42.447+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:42.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:42.464+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:42.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:42.501+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:42.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:27:50.411+0000] {processor.py:161} INFO - Started process (PID=61677) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:50.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:50.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:50.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:50.463+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:50.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:50.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:50.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:50.523+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:50.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:27:58.876+0000] {processor.py:161} INFO - Started process (PID=61765) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:58.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:27:58.879+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:58.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:58.921+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:27:58.939+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:58.939+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:27:58.979+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:27:58.979+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:27:59.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.143 seconds
[2024-05-10T15:28:07.392+0000] {processor.py:161} INFO - Started process (PID=61848) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:07.393+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:07.395+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:07.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:07.449+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:07.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:07.471+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:07.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:07.516+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:07.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.168 seconds
[2024-05-10T15:28:16.368+0000] {processor.py:161} INFO - Started process (PID=61930) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:16.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:16.370+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:16.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:16.412+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:16.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:16.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:16.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:16.453+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:16.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T15:28:24.112+0000] {processor.py:161} INFO - Started process (PID=62018) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:24.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:24.114+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:24.114+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:24.162+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:24.180+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:24.179+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:24.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:24.208+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:24.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:28:32.708+0000] {processor.py:161} INFO - Started process (PID=62100) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:32.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:32.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:32.710+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:32.757+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:32.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:32.774+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:32.808+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:32.808+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:32.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:28:41.315+0000] {processor.py:161} INFO - Started process (PID=62182) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:41.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:41.318+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:41.317+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:41.374+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:41.393+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:41.393+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:41.436+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:41.436+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:41.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.167 seconds
[2024-05-10T15:28:48.455+0000] {processor.py:161} INFO - Started process (PID=62264) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:48.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:48.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:48.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:48.506+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:48.525+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:48.524+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:48.561+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:48.561+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:48.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T15:28:55.551+0000] {processor.py:161} INFO - Started process (PID=62352) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:55.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:28:55.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:55.553+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:55.601+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:28:55.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:55.617+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:28:55.649+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:28:55.649+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:28:55.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:29:03.182+0000] {processor.py:161} INFO - Started process (PID=62434) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:03.184+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:03.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:03.185+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:03.237+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:03.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:03.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:03.302+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:03.301+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:03.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.169 seconds
[2024-05-10T15:29:12.301+0000] {processor.py:161} INFO - Started process (PID=62516) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:12.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:12.303+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:12.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:12.348+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:12.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:12.364+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:12.392+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:12.392+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:12.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T15:29:22.071+0000] {processor.py:161} INFO - Started process (PID=62599) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:22.072+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:22.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:22.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:22.124+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:22.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:22.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:22.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:22.185+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:22.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:29:29.888+0000] {processor.py:161} INFO - Started process (PID=62688) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:29.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:29.892+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:29.891+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:29.963+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:29.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:29.988+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:30.027+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:30.027+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:30.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T15:29:37.881+0000] {processor.py:161} INFO - Started process (PID=62770) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:37.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:37.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:37.884+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:37.937+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:37.958+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:37.958+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:37.992+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:37.992+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:38.023+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:29:46.918+0000] {processor.py:161} INFO - Started process (PID=62852) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:46.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:46.921+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:46.920+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:46.968+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:46.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:46.985+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:47.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:47.015+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:47.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:29:55.342+0000] {processor.py:161} INFO - Started process (PID=62940) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:55.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:29:55.345+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:55.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:55.390+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:29:55.406+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:55.406+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:29:55.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:29:55.434+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:29:55.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T15:30:03.923+0000] {processor.py:161} INFO - Started process (PID=63022) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:03.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:03.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:03.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:03.980+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:04.006+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:04.006+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:04.049+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:04.049+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:04.084+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T15:30:11.738+0000] {processor.py:161} INFO - Started process (PID=63104) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:11.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:11.741+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:11.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:11.792+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:11.809+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:11.808+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:11.838+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:11.838+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:11.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:30:19.418+0000] {processor.py:161} INFO - Started process (PID=63186) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:19.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:19.421+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:19.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:19.468+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:19.485+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:19.485+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:19.514+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:19.514+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:19.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:30:28.132+0000] {processor.py:161} INFO - Started process (PID=63274) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:28.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:28.135+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:28.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:28.184+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:28.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:28.204+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:28.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:28.243+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:28.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:30:37.451+0000] {processor.py:161} INFO - Started process (PID=63356) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:37.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:37.454+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:37.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:37.500+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:37.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:37.516+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:37.551+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:37.550+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:37.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:30:44.774+0000] {processor.py:161} INFO - Started process (PID=63438) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:44.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:44.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:44.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:44.825+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:44.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:44.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:44.874+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:44.873+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:44.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.138 seconds
[2024-05-10T15:30:53.261+0000] {processor.py:161} INFO - Started process (PID=63520) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:53.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:30:53.268+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:53.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:53.339+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:30:53.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:53.368+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:30:53.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:30:53.413+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:30:53.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.207 seconds
[2024-05-10T15:31:00.770+0000] {processor.py:161} INFO - Started process (PID=63608) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:00.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:00.773+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:00.773+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:00.821+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:00.840+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:00.839+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:00.878+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:00.878+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:00.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.152 seconds
[2024-05-10T15:31:09.931+0000] {processor.py:161} INFO - Started process (PID=63690) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:09.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:09.934+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:09.934+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:09.989+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:10.009+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:10.009+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:10.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:10.043+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:10.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:31:20.732+0000] {processor.py:161} INFO - Started process (PID=63772) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:20.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:20.735+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:20.735+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:20.785+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:20.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:20.805+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:20.845+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:20.845+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:20.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:31:30.622+0000] {processor.py:161} INFO - Started process (PID=63860) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:30.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:30.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:30.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:30.666+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:30.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:30.682+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:30.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:30.711+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:30.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T15:31:39.929+0000] {processor.py:161} INFO - Started process (PID=63942) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:39.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:39.935+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:39.934+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:39.993+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:40.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:40.018+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:40.057+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:40.057+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:40.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.174 seconds
[2024-05-10T15:31:48.364+0000] {processor.py:161} INFO - Started process (PID=64024) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:48.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:48.368+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:48.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:48.419+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:48.437+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:48.437+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:48.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:48.469+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:48.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:31:56.621+0000] {processor.py:161} INFO - Started process (PID=64112) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:56.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:31:56.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:56.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:56.673+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:31:56.689+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:56.689+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:31:56.723+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:31:56.723+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:31:56.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:32:05.848+0000] {processor.py:161} INFO - Started process (PID=64194) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:05.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:05.850+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:05.850+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:05.897+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:05.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:05.916+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:05.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:05.954+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:05.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T15:32:14.202+0000] {processor.py:161} INFO - Started process (PID=64276) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:14.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:14.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:14.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:14.262+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:14.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:14.285+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:14.318+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:14.318+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:14.348+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:32:24.283+0000] {processor.py:161} INFO - Started process (PID=64364) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:24.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:24.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:24.286+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:24.341+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:24.364+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:24.364+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:24.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:24.403+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:24.439+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.167 seconds
[2024-05-10T15:32:32.913+0000] {processor.py:161} INFO - Started process (PID=64446) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:32.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:32.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:32.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:32.962+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:32.979+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:32.979+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:33.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:33.010+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:33.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:32:41.929+0000] {processor.py:161} INFO - Started process (PID=64528) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:41.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:41.932+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:41.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:41.982+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:42.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:42.004+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:42.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:42.041+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:42.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.150 seconds
[2024-05-10T15:32:50.910+0000] {processor.py:161} INFO - Started process (PID=64610) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:50.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:32:50.913+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:50.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:50.959+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:32:50.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:50.975+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:32:51.007+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:32:51.006+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:32:51.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:33:00.671+0000] {processor.py:161} INFO - Started process (PID=64699) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:00.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:00.679+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:00.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:00.800+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:00.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:00.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:00.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:00.910+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:00.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.308 seconds
[2024-05-10T15:33:09.726+0000] {processor.py:161} INFO - Started process (PID=64782) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:09.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:09.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:09.728+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:09.775+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:09.801+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:09.801+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:09.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:09.833+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:09.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:33:19.527+0000] {processor.py:161} INFO - Started process (PID=64864) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:19.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:19.530+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:19.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:19.577+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:19.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:19.596+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:19.632+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:19.632+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:19.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T15:33:30.655+0000] {processor.py:161} INFO - Started process (PID=64952) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:30.657+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:30.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:30.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:30.711+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:30.731+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:30.730+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:30.769+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:30.768+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:30.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.154 seconds
[2024-05-10T15:33:41.070+0000] {processor.py:161} INFO - Started process (PID=65035) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:41.072+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:41.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:41.073+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:41.127+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:41.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:41.151+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:41.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:41.187+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:41.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T15:33:53.529+0000] {processor.py:161} INFO - Started process (PID=65117) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:53.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:33:53.534+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:53.533+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:53.590+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:33:53.612+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:53.612+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:33:53.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:33:53.676+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:33:53.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.410 seconds
[2024-05-10T15:34:03.277+0000] {processor.py:161} INFO - Started process (PID=65205) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:03.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:34:03.280+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:03.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:03.336+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:03.360+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:03.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:34:03.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:03.401+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:34:03.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.176 seconds
[2024-05-10T15:34:16.009+0000] {processor.py:161} INFO - Started process (PID=65287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:16.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:34:16.015+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:16.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:16.073+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:16.100+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:16.100+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:34:16.143+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:16.143+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:34:16.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.192 seconds
[2024-05-10T15:34:31.462+0000] {processor.py:161} INFO - Started process (PID=65375) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:31.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:34:31.468+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:31.467+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:31.540+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:31.573+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:31.573+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:34:31.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:31.636+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:34:31.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.239 seconds
[2024-05-10T15:34:45.560+0000] {processor.py:161} INFO - Started process (PID=65457) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:45.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:34:45.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:45.563+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:45.607+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:45.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:45.625+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:34:45.656+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:45.655+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:34:45.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.300 seconds
[2024-05-10T15:34:58.586+0000] {processor.py:161} INFO - Started process (PID=65546) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:58.587+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:34:58.589+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:58.588+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:58.633+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:34:58.649+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:58.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:34:58.674+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:34:58.674+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:34:58.700+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T15:35:10.287+0000] {processor.py:161} INFO - Started process (PID=65629) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:10.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:35:10.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:10.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:10.335+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:10.354+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:10.354+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:35:10.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:10.386+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:35:10.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:35:26.085+0000] {processor.py:161} INFO - Started process (PID=65717) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:26.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:35:26.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:26.089+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:26.145+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:26.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:26.449+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:35:26.491+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:26.490+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:35:26.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.444 seconds
[2024-05-10T15:35:37.036+0000] {processor.py:161} INFO - Started process (PID=65799) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:37.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:35:37.040+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:37.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:37.094+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:37.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:37.117+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:35:37.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:37.158+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:35:37.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.165 seconds
[2024-05-10T15:35:46.897+0000] {processor.py:161} INFO - Started process (PID=65881) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:46.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:35:46.905+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:46.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:46.968+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:46.991+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:46.991+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:35:47.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:47.030+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:35:47.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.188 seconds
[2024-05-10T15:35:54.817+0000] {processor.py:161} INFO - Started process (PID=65970) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:54.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:35:54.821+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:54.821+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:54.873+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:35:54.894+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:54.894+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:35:54.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:35:54.927+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:35:54.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:36:05.280+0000] {processor.py:161} INFO - Started process (PID=66052) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:05.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:05.283+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:05.282+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:05.339+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:05.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:05.357+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:05.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:05.411+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:05.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.194 seconds
[2024-05-10T15:36:15.774+0000] {processor.py:161} INFO - Started process (PID=66134) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:15.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:15.780+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:15.779+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:15.876+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:15.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:15.914+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:15.973+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:15.972+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:16.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.261 seconds
[2024-05-10T15:36:25.900+0000] {processor.py:161} INFO - Started process (PID=66223) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:25.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:25.903+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:25.902+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:25.950+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:25.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:25.969+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:26.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:26.003+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:26.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T15:36:36.223+0000] {processor.py:161} INFO - Started process (PID=66305) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:36.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:36.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:36.229+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:36.304+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:36.335+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:36.335+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:36.384+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:36.383+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:36.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.222 seconds
[2024-05-10T15:36:44.895+0000] {processor.py:161} INFO - Started process (PID=66388) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:44.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:44.898+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:44.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:44.967+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:44.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:44.996+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:45.037+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:45.037+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:45.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.192 seconds
[2024-05-10T15:36:54.227+0000] {processor.py:161} INFO - Started process (PID=66476) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:54.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:36:54.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:54.230+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:54.281+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:36:54.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:54.305+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:36:54.348+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:36:54.347+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:36:54.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T15:37:05.628+0000] {processor.py:161} INFO - Started process (PID=66558) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:05.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:05.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:05.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:05.687+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:05.705+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:05.705+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:05.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:05.734+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:05.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:37:16.068+0000] {processor.py:161} INFO - Started process (PID=66641) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:16.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:16.071+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:16.071+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:16.121+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:16.140+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:16.139+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:16.169+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:16.169+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:16.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.138 seconds
[2024-05-10T15:37:25.327+0000] {processor.py:161} INFO - Started process (PID=66729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:25.329+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:25.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:25.330+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:25.384+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:25.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:25.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:25.438+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:25.438+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:25.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.151 seconds
[2024-05-10T15:37:34.697+0000] {processor.py:161} INFO - Started process (PID=66812) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:34.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:34.705+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:34.704+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:34.887+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:34.977+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:34.965+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:35.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:35.078+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:35.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.454 seconds
[2024-05-10T15:37:44.430+0000] {processor.py:161} INFO - Started process (PID=66894) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:44.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:44.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:44.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:44.491+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:44.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:44.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:44.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:44.570+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:44.610+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.194 seconds
[2024-05-10T15:37:56.391+0000] {processor.py:161} INFO - Started process (PID=66982) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:56.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:37:56.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:56.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:56.440+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:37:56.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:56.463+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:37:56.504+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:37:56.503+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:37:56.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.152 seconds
[2024-05-10T15:38:05.303+0000] {processor.py:161} INFO - Started process (PID=67064) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:05.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:05.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:05.305+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:05.351+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:05.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:05.370+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:05.405+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:05.404+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:05.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:38:13.314+0000] {processor.py:161} INFO - Started process (PID=67146) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:13.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:13.319+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:13.318+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:13.381+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:13.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:13.403+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:13.445+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:13.445+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:13.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:38:23.391+0000] {processor.py:161} INFO - Started process (PID=67229) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:23.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:23.415+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:23.415+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:23.521+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:23.569+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:23.568+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:23.641+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:23.640+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:23.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.326 seconds
[2024-05-10T15:38:35.098+0000] {processor.py:161} INFO - Started process (PID=67318) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:35.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:35.103+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:35.102+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:35.165+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:35.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:35.190+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:35.232+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:35.231+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:35.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.185 seconds
[2024-05-10T15:38:45.036+0000] {processor.py:161} INFO - Started process (PID=67401) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:45.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:45.040+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:45.039+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:45.100+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:45.131+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:45.131+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:45.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:45.190+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:45.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.212 seconds
[2024-05-10T15:38:54.246+0000] {processor.py:161} INFO - Started process (PID=67483) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:54.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:38:54.252+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:54.251+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:54.309+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:38:54.334+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:54.334+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:38:54.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:38:54.378+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:38:54.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.183 seconds
[2024-05-10T15:39:05.558+0000] {processor.py:161} INFO - Started process (PID=67571) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:05.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:05.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:05.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:05.623+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:05.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:05.652+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:05.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:05.700+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:05.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T15:39:15.016+0000] {processor.py:161} INFO - Started process (PID=67653) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:15.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:15.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:15.019+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:15.080+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:15.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:15.104+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:15.138+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:15.138+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:15.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.163 seconds
[2024-05-10T15:39:22.970+0000] {processor.py:161} INFO - Started process (PID=67736) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:22.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:22.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:22.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:23.013+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:23.029+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:23.029+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:23.056+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:23.056+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:23.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T15:39:30.420+0000] {processor.py:161} INFO - Started process (PID=67824) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:30.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:30.423+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:30.423+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:30.467+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:30.485+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:30.485+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:30.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:30.515+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:30.541+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T15:39:39.505+0000] {processor.py:161} INFO - Started process (PID=67906) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:39.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:39.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:39.508+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:39.571+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:39.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:39.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:39.649+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:39.648+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:39.693+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.204 seconds
[2024-05-10T15:39:50.626+0000] {processor.py:161} INFO - Started process (PID=67988) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:50.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:50.629+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:50.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:50.667+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:50.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:50.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:39:50.723+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:50.722+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:39:50.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T15:39:59.862+0000] {processor.py:161} INFO - Started process (PID=68077) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:59.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:39:59.865+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:59.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:59.927+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:39:59.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:39:59.964+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:00.028+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:00.028+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:00.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.207 seconds
[2024-05-10T15:40:08.161+0000] {processor.py:161} INFO - Started process (PID=68159) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:08.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:08.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:08.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:08.206+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:08.226+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:08.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:08.256+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:08.256+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:08.286+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:40:17.038+0000] {processor.py:161} INFO - Started process (PID=68241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:17.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:17.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:17.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:17.109+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:17.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:17.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:17.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:17.186+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:17.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T15:40:26.118+0000] {processor.py:161} INFO - Started process (PID=68330) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:26.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:26.120+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:26.120+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:26.160+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:26.178+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:26.177+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:26.216+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:26.216+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:26.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:40:37.483+0000] {processor.py:161} INFO - Started process (PID=68412) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:37.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:37.495+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:37.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:37.679+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:37.748+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:37.747+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:37.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:37.858+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:37.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.470 seconds
[2024-05-10T15:40:48.890+0000] {processor.py:161} INFO - Started process (PID=68494) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:48.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:48.894+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:48.894+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:48.971+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:49.019+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:49.018+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:49.083+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:49.083+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:49.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.251 seconds
[2024-05-10T15:40:58.849+0000] {processor.py:161} INFO - Started process (PID=68583) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:58.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:40:58.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:58.852+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:58.892+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:40:58.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:58.908+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:40:58.940+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:40:58.940+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:40:58.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T15:41:07.731+0000] {processor.py:161} INFO - Started process (PID=68665) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:07.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:07.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:07.734+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:07.779+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:07.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:07.798+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:07.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:07.830+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:07.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.140 seconds
[2024-05-10T15:41:16.065+0000] {processor.py:161} INFO - Started process (PID=68747) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:16.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:16.068+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:16.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:16.108+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:16.125+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:16.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:16.155+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:16.155+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:16.182+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:41:24.387+0000] {processor.py:161} INFO - Started process (PID=68829) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:24.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:24.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:24.389+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:24.433+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:24.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:24.451+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:24.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:24.479+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:24.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T15:41:33.763+0000] {processor.py:161} INFO - Started process (PID=68917) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:33.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:33.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:33.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:33.870+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:33.906+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:33.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:34.048+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:34.048+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:34.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.397 seconds
[2024-05-10T15:41:44.146+0000] {processor.py:161} INFO - Started process (PID=68999) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:44.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:44.148+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:44.148+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:44.195+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:44.212+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:44.212+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:44.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:44.245+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:44.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.142 seconds
[2024-05-10T15:41:52.050+0000] {processor.py:161} INFO - Started process (PID=69081) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:52.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:52.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:52.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:52.098+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:52.115+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:52.115+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:52.144+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:52.144+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:52.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T15:41:58.769+0000] {processor.py:161} INFO - Started process (PID=69169) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:58.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:41:58.772+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:58.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:58.817+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:41:58.836+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:58.835+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:41:58.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:41:58.866+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:41:58.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:42:07.542+0000] {processor.py:161} INFO - Started process (PID=69251) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:07.544+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:07.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:07.546+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:07.598+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:07.619+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:07.619+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:07.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:07.652+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:07.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:42:16.788+0000] {processor.py:161} INFO - Started process (PID=69333) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:16.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:16.791+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:16.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:16.836+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:16.857+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:16.857+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:16.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:16.893+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:16.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.144 seconds
[2024-05-10T15:42:25.867+0000] {processor.py:161} INFO - Started process (PID=69421) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:25.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:25.871+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:25.870+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:25.919+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:25.938+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:25.938+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:25.970+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:25.970+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:26.005+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T15:42:35.437+0000] {processor.py:161} INFO - Started process (PID=69504) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:35.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:35.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:35.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:35.524+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:35.559+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:35.558+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:35.641+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:35.641+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:35.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.264 seconds
[2024-05-10T15:42:44.125+0000] {processor.py:161} INFO - Started process (PID=69586) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:44.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:44.128+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:44.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:44.177+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:44.195+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:44.195+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:44.227+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:44.226+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:44.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.142 seconds
[2024-05-10T15:42:52.566+0000] {processor.py:161} INFO - Started process (PID=69669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:52.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:42:52.569+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:52.568+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:52.606+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:42:52.624+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:52.624+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:42:52.653+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:42:52.653+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:42:52.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.118 seconds
[2024-05-10T15:43:01.895+0000] {processor.py:161} INFO - Started process (PID=69757) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:01.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:01.901+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:01.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:01.976+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:02.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:02.003+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:02.062+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:02.062+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:02.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.230 seconds
[2024-05-10T15:43:10.096+0000] {processor.py:161} INFO - Started process (PID=69840) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:10.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:10.099+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:10.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:10.153+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:10.174+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:10.174+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:10.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:10.209+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:10.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T15:43:20.615+0000] {processor.py:161} INFO - Started process (PID=69922) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:20.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:20.619+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:20.618+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:20.687+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:20.709+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:20.709+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:20.754+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:20.753+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:20.785+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.183 seconds
[2024-05-10T15:43:29.824+0000] {processor.py:161} INFO - Started process (PID=70010) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:29.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:29.827+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:29.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:29.870+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:29.887+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:29.887+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:29.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:29.915+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:29.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T15:43:37.582+0000] {processor.py:161} INFO - Started process (PID=70093) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:37.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:37.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:37.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:37.629+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:37.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:37.647+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:37.678+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:37.678+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:37.705+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:43:44.910+0000] {processor.py:161} INFO - Started process (PID=70175) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:44.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:44.913+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:44.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:44.962+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:44.977+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:44.977+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:45.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:45.003+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:45.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.129 seconds
[2024-05-10T15:43:52.424+0000] {processor.py:161} INFO - Started process (PID=70257) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:52.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:43:52.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:52.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:52.484+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:43:52.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:52.508+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:43:52.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:43:52.547+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:43:52.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.169 seconds
[2024-05-10T15:44:05.137+0000] {processor.py:161} INFO - Started process (PID=70345) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:05.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:05.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:05.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:05.193+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:05.220+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:05.219+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:05.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:05.258+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:05.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.173 seconds
[2024-05-10T15:44:13.748+0000] {processor.py:161} INFO - Started process (PID=70429) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:13.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:13.752+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:13.751+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:13.811+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:13.837+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:13.837+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:13.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:13.880+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:13.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:44:21.918+0000] {processor.py:161} INFO - Started process (PID=70512) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:21.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:21.921+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.920+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:21.974+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:21.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.995+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:22.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:22.030+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:22.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:44:30.918+0000] {processor.py:161} INFO - Started process (PID=70602) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:30.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:30.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:30.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:31.027+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:31.065+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:31.064+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:31.150+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:31.150+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:31.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.317 seconds
[2024-05-10T15:44:48.676+0000] {processor.py:161} INFO - Started process (PID=70686) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:48.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:48.678+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.678+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:48.735+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:48.760+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.760+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:48.810+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.809+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:48.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:44:57.579+0000] {processor.py:161} INFO - Started process (PID=70776) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:57.580+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:44:57.582+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.581+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:57.624+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:44:57.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.644+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:57.676+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.676+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:44:57.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.128 seconds
[2024-05-10T15:45:11.763+0000] {processor.py:161} INFO - Started process (PID=70864) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:11.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:45:11.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:11.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:11.921+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:11.993+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:11.992+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:12.135+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:12.126+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:45:12.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.576 seconds
[2024-05-10T15:45:29.005+0000] {processor.py:161} INFO - Started process (PID=70953) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:29.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:45:29.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:29.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:29.068+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:29.101+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:29.100+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:29.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:29.163+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:45:29.218+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.227 seconds
[2024-05-10T15:45:39.309+0000] {processor.py:161} INFO - Started process (PID=71036) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:39.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:45:39.312+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.311+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:39.383+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:39.407+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.407+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:39.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.441+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:45:39.472+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.172 seconds
[2024-05-10T15:45:52.215+0000] {processor.py:161} INFO - Started process (PID=71119) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:52.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:45:52.219+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:52.218+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:52.292+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:45:52.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:52.319+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:52.381+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:52.381+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:45:52.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.224 seconds
[2024-05-10T15:46:02.333+0000] {processor.py:161} INFO - Started process (PID=71209) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:02.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:02.336+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:02.335+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:02.381+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:02.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:02.400+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:02.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:02.439+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:02.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.146 seconds
[2024-05-10T15:46:10.537+0000] {processor.py:161} INFO - Started process (PID=71293) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:10.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:10.539+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:10.583+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:10.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:10.627+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.626+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:10.650+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T15:46:20.708+0000] {processor.py:161} INFO - Started process (PID=71376) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:20.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:20.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:20.768+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:20.790+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.789+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:20.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.824+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:20.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.161 seconds
[2024-05-10T15:46:28.147+0000] {processor.py:161} INFO - Started process (PID=71465) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:28.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:28.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:28.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:28.201+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:28.223+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:28.223+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:28.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:28.263+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:28.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T15:46:39.893+0000] {processor.py:161} INFO - Started process (PID=71548) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:39.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:39.898+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:39.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:39.963+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:40.006+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:40.005+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:40.059+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:40.058+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:40.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.258 seconds
[2024-05-10T15:46:48.749+0000] {processor.py:161} INFO - Started process (PID=71631) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:48.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:48.752+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.752+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:48.798+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:48.818+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.818+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:48.848+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.847+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:48.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:46:57.668+0000] {processor.py:161} INFO - Started process (PID=71720) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:57.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:46:57.670+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:57.712+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:46:57.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.728+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:57.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.756+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:46:57.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T15:47:08.495+0000] {processor.py:161} INFO - Started process (PID=71803) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:08.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:08.498+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.498+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:08.559+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:08.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.585+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:08.634+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.634+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:08.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.193 seconds
[2024-05-10T15:47:19.057+0000] {processor.py:161} INFO - Started process (PID=71887) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:19.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:19.059+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:19.059+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:19.113+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:19.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:19.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:19.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:19.170+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:19.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.166 seconds
[2024-05-10T15:47:27.376+0000] {processor.py:161} INFO - Started process (PID=71977) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:27.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:27.381+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:27.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:27.452+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:27.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:27.476+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:27.521+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:27.521+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:27.558+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T15:47:36.681+0000] {processor.py:161} INFO - Started process (PID=72060) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:36.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:36.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:36.734+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:36.754+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:36.786+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.786+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:36.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:47:45.037+0000] {processor.py:161} INFO - Started process (PID=72143) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:45.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:45.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:45.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:45.101+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:45.130+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:45.130+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:45.194+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:45.193+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:45.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.214 seconds
[2024-05-10T15:47:53.499+0000] {processor.py:161} INFO - Started process (PID=72226) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:53.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:47:53.502+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:53.547+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:47:53.564+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.563+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:53.592+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.592+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:47:53.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T15:48:00.980+0000] {processor.py:161} INFO - Started process (PID=72315) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:00.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:00.984+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:00.983+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:01.036+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:01.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:01.054+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:01.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:01.087+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:01.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:48:09.078+0000] {processor.py:161} INFO - Started process (PID=72398) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:09.080+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:09.081+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:09.081+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:09.134+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:09.152+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:09.152+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:09.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:09.190+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:09.225+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T15:48:16.779+0000] {processor.py:161} INFO - Started process (PID=72481) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:16.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:16.783+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:16.834+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:16.855+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.855+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:16.889+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.888+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:16.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.160 seconds
[2024-05-10T15:48:25.345+0000] {processor.py:161} INFO - Started process (PID=72564) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:25.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:25.349+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:25.393+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:25.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.412+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:25.446+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.446+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:25.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:48:33.755+0000] {processor.py:161} INFO - Started process (PID=72654) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:33.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:33.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.758+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:33.812+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:33.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:33.870+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.869+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:33.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T15:48:41.782+0000] {processor.py:161} INFO - Started process (PID=72738) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:41.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:41.786+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.785+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:41.838+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:41.857+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.857+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:41.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.889+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:41.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.158 seconds
[2024-05-10T15:48:51.477+0000] {processor.py:161} INFO - Started process (PID=72821) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:51.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:48:51.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:51.539+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:48:51.560+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.560+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:51.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.604+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:48:51.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.171 seconds
[2024-05-10T15:49:01.048+0000] {processor.py:161} INFO - Started process (PID=72910) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:01.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:01.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:01.053+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:01.101+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:01.120+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:01.119+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:01.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:01.151+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:01.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.142 seconds
[2024-05-10T15:49:09.948+0000] {processor.py:161} INFO - Started process (PID=72993) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:09.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:09.950+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:09.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:09.992+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:10.008+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:10.008+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:10.039+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:10.039+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:10.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T15:49:19.760+0000] {processor.py:161} INFO - Started process (PID=73077) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:19.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:19.763+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.763+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:19.811+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:19.828+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.828+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:19.859+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.858+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:19.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T15:49:30.340+0000] {processor.py:161} INFO - Started process (PID=73166) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:30.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:30.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:30.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:30.458+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:30.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:30.501+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:30.569+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:30.569+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:30.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.305 seconds
[2024-05-10T15:49:40.415+0000] {processor.py:161} INFO - Started process (PID=73250) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:40.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:40.420+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.419+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:40.467+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:40.484+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.484+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:40.515+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.515+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:40.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:49:50.351+0000] {processor.py:161} INFO - Started process (PID=73333) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:50.352+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:50.354+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:50.354+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:50.409+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:50.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:50.431+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:50.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:50.469+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:50.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.159 seconds
[2024-05-10T15:49:57.929+0000] {processor.py:161} INFO - Started process (PID=73423) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:57.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:49:57.933+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:57.933+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:57.980+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:49:57.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:57.998+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:58.036+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:58.035+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:49:58.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:50:06.194+0000] {processor.py:161} INFO - Started process (PID=73506) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:06.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:06.197+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:06.197+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:06.242+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:06.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:06.258+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:06.293+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:06.293+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:06.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.137 seconds
[2024-05-10T15:50:15.794+0000] {processor.py:161} INFO - Started process (PID=73590) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:15.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:15.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:15.840+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:15.855+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.855+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:15.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.885+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:15.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:50:24.361+0000] {processor.py:161} INFO - Started process (PID=73674) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:24.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:24.364+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.364+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:24.405+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:24.422+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.422+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:24.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.452+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:24.479+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:50:32.472+0000] {processor.py:161} INFO - Started process (PID=73765) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:32.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:32.475+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:32.521+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:32.538+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.538+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:32.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.568+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:32.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.133 seconds
[2024-05-10T15:50:41.016+0000] {processor.py:161} INFO - Started process (PID=73848) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:41.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:41.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:41.019+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:41.075+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:41.096+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:41.096+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:41.131+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:41.131+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:41.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T15:50:51.073+0000] {processor.py:161} INFO - Started process (PID=73931) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:51.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:51.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:51.081+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:51.158+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:51.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:51.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:51.223+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:51.222+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:51.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.203 seconds
[2024-05-10T15:50:59.600+0000] {processor.py:161} INFO - Started process (PID=74021) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:59.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:50:59.603+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.602+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:59.655+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:50:59.673+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.673+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:59.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.712+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:50:59.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T15:51:08.896+0000] {processor.py:161} INFO - Started process (PID=74104) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:08.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:08.899+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.899+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:08.943+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:08.958+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.958+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:08.984+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.984+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:09.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.120 seconds
[2024-05-10T15:51:17.294+0000] {processor.py:161} INFO - Started process (PID=74187) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:17.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:17.298+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:17.360+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:17.387+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.387+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:17.425+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.425+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:17.458+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.178 seconds
[2024-05-10T15:51:26.660+0000] {processor.py:161} INFO - Started process (PID=74276) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:26.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:26.664+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.662+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:26.714+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:26.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.731+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:26.765+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.765+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:26.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.143 seconds
[2024-05-10T15:51:35.925+0000] {processor.py:161} INFO - Started process (PID=74360) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:35.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:35.930+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:35.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:36.050+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:36.078+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:36.077+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:36.171+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:36.170+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:36.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.309 seconds
[2024-05-10T15:51:43.415+0000] {processor.py:161} INFO - Started process (PID=74443) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:43.416+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:43.417+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.417+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:43.462+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:43.478+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.478+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:43.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.507+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:43.531+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T15:51:51.434+0000] {processor.py:161} INFO - Started process (PID=74526) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:51.435+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:51.437+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:51.480+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:51.495+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.495+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:51.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.522+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:51:51.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T15:51:59.940+0000] {processor.py:161} INFO - Started process (PID=74616) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:59.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:51:59.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:59.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:51:59.986+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:00.002+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:00.002+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:00.037+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:00.036+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:00.063+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:52:11.177+0000] {processor.py:161} INFO - Started process (PID=74699) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:11.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:11.179+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:11.179+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:11.224+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:11.241+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:11.241+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:11.271+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:11.270+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:11.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:52:21.296+0000] {processor.py:161} INFO - Started process (PID=74783) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:21.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:21.298+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:21.341+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:21.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.357+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:21.388+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.387+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:21.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T15:52:28.419+0000] {processor.py:161} INFO - Started process (PID=74873) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:28.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:28.422+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:28.470+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:28.489+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:28.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.520+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:28.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:52:38.316+0000] {processor.py:161} INFO - Started process (PID=74956) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:38.318+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:38.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.320+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:38.376+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:38.395+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.395+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:38.425+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.425+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:38.453+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.148 seconds
[2024-05-10T15:52:47.174+0000] {processor.py:161} INFO - Started process (PID=75039) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:47.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:47.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:47.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:47.219+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:47.235+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:47.234+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:47.266+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:47.266+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:47.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.284 seconds
[2024-05-10T15:52:54.287+0000] {processor.py:161} INFO - Started process (PID=75122) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:54.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:52:54.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:54.346+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:52:54.367+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.367+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:54.405+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.404+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:52:54.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.153 seconds
[2024-05-10T15:53:01.965+0000] {processor.py:161} INFO - Started process (PID=75210) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:01.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:53:01.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:01.969+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:02.020+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:02.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:02.041+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:02.077+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:02.077+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:53:02.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T15:53:11.580+0000] {processor.py:161} INFO - Started process (PID=75293) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:11.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:53:11.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:11.624+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:11.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.646+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:11.680+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.680+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:53:11.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.143 seconds
[2024-05-10T15:53:24.172+0000] {processor.py:161} INFO - Started process (PID=75376) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:24.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:53:24.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:24.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:24.237+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:24.263+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:24.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:24.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:24.545+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:53:24.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.430 seconds
[2024-05-10T15:53:34.914+0000] {processor.py:161} INFO - Started process (PID=75465) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:34.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:53:34.917+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:34.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:34.978+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:34.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:34.998+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:35.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:35.047+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:53:35.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.186 seconds
[2024-05-10T15:53:49.235+0000] {processor.py:161} INFO - Started process (PID=75549) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:49.237+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:53:49.238+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:49.238+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:49.306+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:53:49.333+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:49.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:49.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:49.393+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:53:49.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.215 seconds
[2024-05-10T15:54:05.538+0000] {processor.py:161} INFO - Started process (PID=75639) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:05.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:54:05.543+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:05.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:05.620+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:05.941+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:05.941+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:06.017+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:06.016+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:54:06.055+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.535 seconds
[2024-05-10T15:54:19.480+0000] {processor.py:161} INFO - Started process (PID=75722) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:19.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:54:19.482+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:19.482+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:19.533+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:19.560+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:19.559+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:19.864+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:19.864+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:54:19.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.435 seconds
[2024-05-10T15:54:31.255+0000] {processor.py:161} INFO - Started process (PID=75811) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:31.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:54:31.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.257+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:31.307+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:31.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.324+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:31.356+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.356+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:54:31.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.141 seconds
[2024-05-10T15:54:41.606+0000] {processor.py:161} INFO - Started process (PID=75894) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:41.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:54:41.612+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.611+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:41.692+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:41.720+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.719+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:41.763+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.763+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:54:41.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.210 seconds
[2024-05-10T15:54:50.656+0000] {processor.py:161} INFO - Started process (PID=75977) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:50.658+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:54:50.660+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:50.717+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:54:50.737+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.737+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:50.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.773+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:54:50.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.164 seconds
[2024-05-10T15:55:00.298+0000] {processor.py:161} INFO - Started process (PID=76066) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:00.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:00.302+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:00.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:00.365+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:00.391+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:00.390+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:00.443+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:00.443+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:00.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.201 seconds
[2024-05-10T15:55:09.046+0000] {processor.py:161} INFO - Started process (PID=76149) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:09.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:09.049+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:09.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:09.096+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:09.116+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:09.116+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:09.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:09.151+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:09.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T15:55:18.695+0000] {processor.py:161} INFO - Started process (PID=76232) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:18.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:18.698+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.697+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:18.747+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:18.765+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.765+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:18.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.798+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:18.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.141 seconds
[2024-05-10T15:55:28.348+0000] {processor.py:161} INFO - Started process (PID=76321) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:28.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:28.352+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:28.413+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:28.436+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.436+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:28.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.482+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:28.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T15:55:37.587+0000] {processor.py:161} INFO - Started process (PID=76404) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:37.588+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:37.590+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.589+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:37.643+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:37.663+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:37.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.695+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:37.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T15:55:45.582+0000] {processor.py:161} INFO - Started process (PID=76487) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:45.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:45.585+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:45.633+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:45.651+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.650+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:45.686+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.686+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:45.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.160 seconds
[2024-05-10T15:55:54.545+0000] {processor.py:161} INFO - Started process (PID=76570) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:54.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:55:54.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.547+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:54.592+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:55:54.612+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.612+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:54.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.644+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:55:54.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.134 seconds
[2024-05-10T15:56:01.703+0000] {processor.py:161} INFO - Started process (PID=76660) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:01.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:01.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:01.752+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:01.773+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.772+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:56:01.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.803+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all to None, run_after=None
[2024-05-10T15:56:01.829+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.136 seconds
[2024-05-10T15:56:11.663+0000] {processor.py:161} INFO - Started process (PID=76743) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:11.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:11.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:11.668+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:11.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:11.730+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:11.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:11.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T15:56:23.474+0000] {processor.py:161} INFO - Started process (PID=76826) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:23.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:23.490+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:23.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:23.591+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:23.573+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:23.594+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:23.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.217 seconds
[2024-05-10T15:56:31.789+0000] {processor.py:161} INFO - Started process (PID=76915) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:31.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:31.792+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:31.792+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:31.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:31.833+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:31.843+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:31.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T15:56:41.339+0000] {processor.py:161} INFO - Started process (PID=76998) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:41.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:41.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:41.342+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:41.388+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:41.381+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:41.390+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:41.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T15:56:49.953+0000] {processor.py:161} INFO - Started process (PID=77082) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:49.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:49.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:49.956+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:50.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:49.994+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:50.004+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:50.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T15:56:56.838+0000] {processor.py:161} INFO - Started process (PID=77165) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:56.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:56:56.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:56.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:56.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:56.904+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:56.917+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:56:56.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.119 seconds
[2024-05-10T15:57:06.944+0000] {processor.py:161} INFO - Started process (PID=77254) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:06.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:06.948+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:06.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:07.000+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:06.993+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:07.001+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:07.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T15:57:16.616+0000] {processor.py:161} INFO - Started process (PID=77338) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:16.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:16.622+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:16.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:16.712+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:16.700+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:16.715+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:16.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.160 seconds
[2024-05-10T15:57:27.384+0000] {processor.py:161} INFO - Started process (PID=77427) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:27.386+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:27.388+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:27.387+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:27.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:27.431+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:27.441+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:27.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T15:57:37.551+0000] {processor.py:161} INFO - Started process (PID=77511) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:37.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:37.555+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:37.554+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:37.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:37.602+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:37.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:37.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T15:57:46.452+0000] {processor.py:161} INFO - Started process (PID=77594) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:46.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:46.458+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:46.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:46.514+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:46.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:46.515+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:46.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T15:57:58.321+0000] {processor.py:161} INFO - Started process (PID=77683) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:58.323+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:57:58.326+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:58.325+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:58.392+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:58.383+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:58.394+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:57:58.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.115 seconds
[2024-05-10T15:58:10.704+0000] {processor.py:161} INFO - Started process (PID=77766) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:10.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:10.709+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:10.708+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:10.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:10.768+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:10.779+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:10.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.117 seconds
[2024-05-10T15:58:20.277+0000] {processor.py:161} INFO - Started process (PID=77849) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:20.278+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:20.281+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:20.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:20.339+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:20.332+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:20.341+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:20.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T15:58:29.448+0000] {processor.py:161} INFO - Started process (PID=77938) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:29.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:29.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:29.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:29.558+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:29.543+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:29.561+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:29.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.167 seconds
[2024-05-10T15:58:40.884+0000] {processor.py:161} INFO - Started process (PID=78022) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:40.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:40.888+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:40.888+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:40.969+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:40.958+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:40.971+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:41.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T15:58:48.633+0000] {processor.py:161} INFO - Started process (PID=78105) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:48.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:48.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:48.637+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:48.683+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:48.675+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:48.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:48.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T15:58:56.388+0000] {processor.py:161} INFO - Started process (PID=78188) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:56.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:58:56.391+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:56.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:56.435+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:56.429+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:56.437+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:58:56.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T15:59:05.246+0000] {processor.py:161} INFO - Started process (PID=78277) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:05.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:05.249+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:05.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:05.299+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:05.293+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:05.301+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:05.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T15:59:15.104+0000] {processor.py:161} INFO - Started process (PID=78360) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:15.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:15.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:15.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:15.166+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:15.159+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:15.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:15.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T15:59:23.376+0000] {processor.py:161} INFO - Started process (PID=78443) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:23.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:23.379+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:23.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:23.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:23.436+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:23.445+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:23.466+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T15:59:31.796+0000] {processor.py:161} INFO - Started process (PID=78532) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:31.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:31.799+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:31.799+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:31.852+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:31.842+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:31.853+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:31.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T15:59:42.794+0000] {processor.py:161} INFO - Started process (PID=78616) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:42.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:42.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:42.797+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:42.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:42.834+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:42.842+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:42.858+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T15:59:53.119+0000] {processor.py:161} INFO - Started process (PID=78699) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:53.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T15:59:53.124+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:53.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:53.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:53.176+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:53.187+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T15:59:53.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T16:00:04.480+0000] {processor.py:161} INFO - Started process (PID=78788) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:04.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:04.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:04.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:04.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:04.527+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:04.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:04.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:00:13.758+0000] {processor.py:161} INFO - Started process (PID=78871) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:13.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:13.761+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:13.761+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:13.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:13.808+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:13.816+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:13.834+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:00:23.777+0000] {processor.py:161} INFO - Started process (PID=78954) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:23.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:23.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:23.779+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:23.823+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:23.817+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:23.825+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:23.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:00:33.392+0000] {processor.py:161} INFO - Started process (PID=79044) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:33.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:33.396+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:33.395+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:33.455+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:33.446+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:33.457+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:33.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T16:00:41.997+0000] {processor.py:161} INFO - Started process (PID=79128) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:41.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:42.000+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:42.000+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:42.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:42.036+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:42.046+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:42.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:00:51.142+0000] {processor.py:161} INFO - Started process (PID=79211) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:51.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:00:51.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:51.145+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:51.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:51.183+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:51.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:00:51.205+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:01:01.440+0000] {processor.py:161} INFO - Started process (PID=79300) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:01.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:01.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:01.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:01.502+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:01.494+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:01.504+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:01.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T16:01:10.390+0000] {processor.py:161} INFO - Started process (PID=79383) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:10.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:10.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:10.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:10.446+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:10.439+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:10.448+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:10.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:01:20.320+0000] {processor.py:161} INFO - Started process (PID=79466) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:20.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:20.323+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:20.323+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:20.375+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:20.368+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:20.377+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:20.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:01:28.226+0000] {processor.py:161} INFO - Started process (PID=79555) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:28.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:28.229+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:28.228+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:28.273+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:28.265+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:28.275+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:28.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:01:37.078+0000] {processor.py:161} INFO - Started process (PID=79638) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:37.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:37.083+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:37.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:37.140+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:37.133+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:37.141+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:37.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:01:47.272+0000] {processor.py:161} INFO - Started process (PID=79722) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:47.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:47.275+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:47.275+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:47.326+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:47.319+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:47.327+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:47.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:01:55.756+0000] {processor.py:161} INFO - Started process (PID=79805) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:55.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:01:55.761+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:55.760+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:55.814+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:55.807+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:55.816+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:01:55.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:02:04.206+0000] {processor.py:161} INFO - Started process (PID=79894) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:04.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:04.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:04.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:04.265+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:04.257+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:04.267+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:04.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T16:02:12.837+0000] {processor.py:161} INFO - Started process (PID=79977) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:12.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:12.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:12.840+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:12.881+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:12.875+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:12.882+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:12.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:02:23.932+0000] {processor.py:161} INFO - Started process (PID=80060) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:23.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:23.935+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:23.935+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:23.983+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:23.975+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:23.984+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:23.999+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:02:31.998+0000] {processor.py:161} INFO - Started process (PID=80149) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:32.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:32.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:32.001+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:32.048+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:32.040+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:32.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:32.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:02:43.009+0000] {processor.py:161} INFO - Started process (PID=80233) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:43.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:43.012+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:43.011+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:43.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:43.046+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:43.053+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:43.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:02:50.201+0000] {processor.py:161} INFO - Started process (PID=80316) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:50.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:50.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:50.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:50.249+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:50.241+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:50.251+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:50.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:02:58.732+0000] {processor.py:161} INFO - Started process (PID=80405) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:58.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:02:58.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:58.734+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:58.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:58.771+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:58.780+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:02:58.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:03:07.829+0000] {processor.py:161} INFO - Started process (PID=80488) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:07.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:07.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:07.833+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:07.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:07.895+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:07.906+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:07.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T16:03:17.039+0000] {processor.py:161} INFO - Started process (PID=80571) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:17.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:17.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:17.042+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:17.105+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:17.095+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:17.107+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:17.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:03:25.582+0000] {processor.py:161} INFO - Started process (PID=80655) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:25.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:25.586+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:25.585+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:25.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:25.630+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:25.638+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:25.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:03:33.681+0000] {processor.py:161} INFO - Started process (PID=80744) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:33.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:33.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:33.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:33.737+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:33.729+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:33.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:33.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:03:41.964+0000] {processor.py:161} INFO - Started process (PID=80827) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:41.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:41.967+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:41.966+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:42.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:42.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:42.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:42.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:03:49.513+0000] {processor.py:161} INFO - Started process (PID=80910) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:49.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:49.515+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:49.515+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:49.573+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:49.555+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:49.575+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:49.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T16:03:58.513+0000] {processor.py:161} INFO - Started process (PID=80999) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:58.514+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:03:58.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:58.515+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:58.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:58.556+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:58.564+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:03:58.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:04:07.533+0000] {processor.py:161} INFO - Started process (PID=81082) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:07.535+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:07.536+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:07.536+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:07.591+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:07.582+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:07.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:07.610+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T16:04:16.602+0000] {processor.py:161} INFO - Started process (PID=81165) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:16.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:16.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:16.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:16.668+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:16.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:16.670+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:16.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T16:04:24.681+0000] {processor.py:161} INFO - Started process (PID=81248) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:24.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:24.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:24.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:24.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:24.734+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:24.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:24.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T16:04:35.620+0000] {processor.py:161} INFO - Started process (PID=81337) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:35.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:35.623+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:35.623+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:35.683+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:35.673+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:35.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:35.707+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T16:04:45.516+0000] {processor.py:161} INFO - Started process (PID=81420) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:45.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:45.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:45.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:45.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:45.576+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:45.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:45.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.116 seconds
[2024-05-10T16:04:56.200+0000] {processor.py:161} INFO - Started process (PID=81503) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:56.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:04:56.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:56.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:56.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:56.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:56.285+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:04:56.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T16:05:08.613+0000] {processor.py:161} INFO - Started process (PID=81592) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:08.615+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:08.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:08.616+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:08.674+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:08.665+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:08.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:08.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T16:05:16.815+0000] {processor.py:161} INFO - Started process (PID=81675) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:16.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:16.820+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:16.819+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:16.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:16.872+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:16.884+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:16.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:05:26.730+0000] {processor.py:161} INFO - Started process (PID=81758) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:26.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:26.733+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:26.732+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:26.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:26.773+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:26.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:26.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:05:35.503+0000] {processor.py:161} INFO - Started process (PID=81847) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:35.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:35.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:35.506+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:35.556+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:35.548+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:35.557+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:35.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:05:43.698+0000] {processor.py:161} INFO - Started process (PID=81930) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:43.699+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:43.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:43.700+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:43.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:43.737+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:43.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:43.757+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:05:51.302+0000] {processor.py:161} INFO - Started process (PID=82013) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:51.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:05:51.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:51.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:51.346+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:51.339+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:51.348+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:05:51.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:06:02.002+0000] {processor.py:161} INFO - Started process (PID=82101) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:02.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:02.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:02.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:02.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:02.045+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:02.054+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:02.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:06:10.024+0000] {processor.py:161} INFO - Started process (PID=82185) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:10.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:10.026+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:10.026+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:10.071+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:10.064+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:10.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:10.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:06:19.812+0000] {processor.py:161} INFO - Started process (PID=82268) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:19.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:19.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:19.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:19.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:19.858+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:19.871+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:19.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T16:06:27.945+0000] {processor.py:161} INFO - Started process (PID=82351) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:27.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:27.949+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:27.948+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:27.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:27.988+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:27.998+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:28.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:06:37.291+0000] {processor.py:161} INFO - Started process (PID=82440) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:37.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:37.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:37.294+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:37.356+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:37.346+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:37.358+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:37.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T16:06:45.658+0000] {processor.py:161} INFO - Started process (PID=82524) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:45.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:45.660+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:45.660+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:45.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:45.699+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:45.707+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:45.720+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:06:54.655+0000] {processor.py:161} INFO - Started process (PID=82607) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:54.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:06:54.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:54.657+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:54.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:54.707+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:54.715+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:06:54.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:07:03.418+0000] {processor.py:161} INFO - Started process (PID=82696) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:03.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:03.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:03.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:03.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:03.508+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:03.523+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:03.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.156 seconds
[2024-05-10T16:07:12.296+0000] {processor.py:161} INFO - Started process (PID=82779) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:12.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:12.299+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:12.299+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:12.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:12.336+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:12.344+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:12.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:07:20.257+0000] {processor.py:161} INFO - Started process (PID=82862) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:20.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:20.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:20.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:20.307+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:20.299+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:20.309+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:20.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:07:28.944+0000] {processor.py:161} INFO - Started process (PID=82951) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:28.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:28.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:28.947+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:28.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:28.993+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:29.001+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:29.016+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:07:37.829+0000] {processor.py:161} INFO - Started process (PID=83035) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:37.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:37.832+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:37.832+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:37.875+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:37.868+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:37.876+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:37.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:07:46.856+0000] {processor.py:161} INFO - Started process (PID=83119) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:46.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:46.860+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:46.860+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:46.907+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:46.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:46.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:46.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:07:55.297+0000] {processor.py:161} INFO - Started process (PID=83202) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:55.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:07:55.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:55.299+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:55.347+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:55.340+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:55.348+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:07:55.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:08:04.711+0000] {processor.py:161} INFO - Started process (PID=83291) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:04.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:04.716+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:04.715+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:04.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:04.761+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:04.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:04.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:08:12.103+0000] {processor.py:161} INFO - Started process (PID=83374) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:12.104+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:12.106+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:12.105+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:12.152+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:12.146+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:12.154+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:12.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:08:21.830+0000] {processor.py:161} INFO - Started process (PID=83457) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:21.832+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:21.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:21.833+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:21.882+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:21.874+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:21.884+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:21.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T16:08:31.215+0000] {processor.py:161} INFO - Started process (PID=83546) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:31.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:31.218+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:31.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:31.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:31.253+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:31.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:31.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:08:41.889+0000] {processor.py:161} INFO - Started process (PID=83630) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:41.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:41.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:41.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:41.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:41.935+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:41.945+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:41.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T16:08:49.584+0000] {processor.py:161} INFO - Started process (PID=83713) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:49.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:49.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:49.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:49.634+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:49.627+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:49.636+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:49.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:08:57.381+0000] {processor.py:161} INFO - Started process (PID=83796) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:57.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:08:57.385+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:57.383+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:57.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:57.445+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:57.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:08:57.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T16:09:05.499+0000] {processor.py:161} INFO - Started process (PID=83885) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:05.501+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:05.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:05.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:05.558+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:05.551+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:05.559+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:05.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:09:13.154+0000] {processor.py:161} INFO - Started process (PID=83969) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:13.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:13.156+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:13.156+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:13.202+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:13.195+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:13.204+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:13.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:09:22.980+0000] {processor.py:161} INFO - Started process (PID=84052) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:22.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:22.983+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:22.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:23.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:23.022+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:23.031+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:23.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:09:30.729+0000] {processor.py:161} INFO - Started process (PID=84142) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:30.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:30.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:30.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:30.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:30.775+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:30.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:30.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:09:39.247+0000] {processor.py:161} INFO - Started process (PID=84225) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:39.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:39.250+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:39.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:39.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:39.284+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:39.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:39.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:09:46.397+0000] {processor.py:161} INFO - Started process (PID=84308) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:46.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:46.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:46.400+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:46.455+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:46.447+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:46.456+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:46.472+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T16:09:55.318+0000] {processor.py:161} INFO - Started process (PID=84391) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:55.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:09:55.322+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:55.321+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:55.377+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:55.359+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:55.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:09:55.409+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:10:02.564+0000] {processor.py:161} INFO - Started process (PID=84480) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:02.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:02.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:02.567+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:02.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:02.606+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:02.616+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:02.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:10:11.136+0000] {processor.py:161} INFO - Started process (PID=84563) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:11.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:11.139+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:11.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:11.179+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:11.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:11.180+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:11.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:10:19.875+0000] {processor.py:161} INFO - Started process (PID=84646) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:19.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:19.878+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:19.877+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:19.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:19.920+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:19.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:19.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:10:29.998+0000] {processor.py:161} INFO - Started process (PID=84735) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:29.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:30.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:30.001+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:30.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:30.046+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:30.054+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:30.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:10:38.620+0000] {processor.py:161} INFO - Started process (PID=84819) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:38.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:38.623+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:38.623+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:38.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:38.661+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:38.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:38.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:10:47.097+0000] {processor.py:161} INFO - Started process (PID=84902) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:47.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:47.100+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:47.100+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:47.145+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:47.138+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:47.147+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:47.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:10:54.880+0000] {processor.py:161} INFO - Started process (PID=84985) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:54.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:10:54.882+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:54.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:54.920+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:54.914+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:54.921+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:10:54.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.062 seconds
[2024-05-10T16:11:03.657+0000] {processor.py:161} INFO - Started process (PID=85074) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:03.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:11:03.661+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:03.660+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:03.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:03.699+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:03.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:03.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:11:18.458+0000] {processor.py:161} INFO - Started process (PID=85157) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:18.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:11:18.462+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:18.461+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:18.514+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:18.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:18.516+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:18.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:11:30.609+0000] {processor.py:161} INFO - Started process (PID=85246) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:30.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:11:30.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:30.611+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:30.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:30.650+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:30.659+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:30.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:11:41.949+0000] {processor.py:161} INFO - Started process (PID=85330) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:41.951+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:11:41.953+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:41.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:42.024+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:42.015+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:42.026+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:42.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.114 seconds
[2024-05-10T16:11:52.928+0000] {processor.py:161} INFO - Started process (PID=85413) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:52.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:11:52.931+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:52.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:52.980+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:52.972+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:52.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:11:53.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:12:05.007+0000] {processor.py:161} INFO - Started process (PID=85502) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:05.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:05.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:05.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:05.058+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:05.050+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:05.060+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:05.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:12:16.579+0000] {processor.py:161} INFO - Started process (PID=85585) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:16.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:16.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:16.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:16.651+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:16.643+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:16.653+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:16.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T16:12:27.284+0000] {processor.py:161} INFO - Started process (PID=85669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:27.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:27.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:27.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:27.331+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:27.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:27.332+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:27.347+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:12:37.927+0000] {processor.py:161} INFO - Started process (PID=85758) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:37.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:37.932+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:37.931+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:37.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:37.980+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:37.991+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:38.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T16:12:46.313+0000] {processor.py:161} INFO - Started process (PID=85841) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:46.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:46.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:46.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:46.358+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:46.351+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:46.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:46.374+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:12:54.160+0000] {processor.py:161} INFO - Started process (PID=85924) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:54.162+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:12:54.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:54.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:54.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:54.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:12:54.218+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:12:54.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T16:13:03.267+0000] {processor.py:161} INFO - Started process (PID=86015) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:03.270+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:03.272+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:03.271+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:03.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:03.320+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:03.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:03.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:13:13.469+0000] {processor.py:161} INFO - Started process (PID=86098) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:13.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:13.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:13.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:13.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:13.537+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:13.548+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:13.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.132 seconds
[2024-05-10T16:13:24.396+0000] {processor.py:161} INFO - Started process (PID=86181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:24.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:24.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:24.399+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:24.460+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:24.450+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:24.462+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:24.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T16:13:32.135+0000] {processor.py:161} INFO - Started process (PID=86270) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:32.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:32.139+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:32.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:32.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:32.195+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:32.209+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:32.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.108 seconds
[2024-05-10T16:13:39.684+0000] {processor.py:161} INFO - Started process (PID=86353) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:39.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:39.689+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:39.688+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:39.736+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:39.728+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:39.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:39.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:13:49.013+0000] {processor.py:161} INFO - Started process (PID=86436) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:49.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:49.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:49.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:49.060+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:49.053+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:49.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:49.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:13:57.292+0000] {processor.py:161} INFO - Started process (PID=86519) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:57.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:13:57.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:57.295+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:57.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:57.334+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:13:57.343+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:13:57.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:14:06.015+0000] {processor.py:161} INFO - Started process (PID=86608) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:06.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:06.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:06.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:06.061+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:06.054+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:06.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:06.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:14:13.571+0000] {processor.py:161} INFO - Started process (PID=86691) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:13.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:13.574+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:13.574+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:13.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:13.611+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:13.618+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:13.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:14:24.595+0000] {processor.py:161} INFO - Started process (PID=86774) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:24.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:24.598+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:24.597+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:24.646+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:24.638+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:24.648+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:24.663+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:14:33.310+0000] {processor.py:161} INFO - Started process (PID=86864) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:33.311+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:33.313+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:33.312+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:33.360+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:33.351+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:33.361+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:33.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:14:42.520+0000] {processor.py:161} INFO - Started process (PID=86947) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:42.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:42.522+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:42.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:42.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:42.563+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:42.571+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:42.585+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:14:50.463+0000] {processor.py:161} INFO - Started process (PID=87031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:50.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:50.466+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:50.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:50.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:50.502+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:50.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:50.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:14:59.113+0000] {processor.py:161} INFO - Started process (PID=87114) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:59.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:14:59.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:59.117+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:59.174+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:59.166+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:14:59.176+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:14:59.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T16:15:08.491+0000] {processor.py:161} INFO - Started process (PID=87204) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:08.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:08.493+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:08.493+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:08.532+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:08.526+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:08.533+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:08.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T16:15:16.216+0000] {processor.py:161} INFO - Started process (PID=87288) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:16.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:16.219+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:16.218+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:16.259+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:16.253+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:16.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:16.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:15:24.164+0000] {processor.py:161} INFO - Started process (PID=87372) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:24.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:24.168+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:24.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:24.208+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:24.201+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:24.209+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:24.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:15:32.428+0000] {processor.py:161} INFO - Started process (PID=87461) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:32.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:32.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:32.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:32.488+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:32.480+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:32.489+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:32.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T16:15:42.983+0000] {processor.py:161} INFO - Started process (PID=87544) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:42.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:42.985+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:42.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:43.024+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:43.017+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:43.025+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:43.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T16:15:50.980+0000] {processor.py:161} INFO - Started process (PID=87627) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:50.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:50.982+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:50.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:51.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:51.010+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:51.017+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:51.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.056 seconds
[2024-05-10T16:15:58.282+0000] {processor.py:161} INFO - Started process (PID=87711) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:58.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:15:58.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:58.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:58.333+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:58.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:15:58.334+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:15:58.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:16:05.908+0000] {processor.py:161} INFO - Started process (PID=87800) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:05.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:05.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:05.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:05.959+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:05.953+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:05.961+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:05.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:16:13.753+0000] {processor.py:161} INFO - Started process (PID=87883) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:13.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:13.757+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:13.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:13.819+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:13.809+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:13.821+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:13.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:16:22.911+0000] {processor.py:161} INFO - Started process (PID=87966) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:22.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:22.914+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:22.914+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:22.960+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:22.953+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:22.961+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:22.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:16:30.532+0000] {processor.py:161} INFO - Started process (PID=88055) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:30.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:30.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:30.535+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:30.576+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:30.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:30.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:30.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:16:39.430+0000] {processor.py:161} INFO - Started process (PID=88138) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:39.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:39.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:39.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:39.486+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:39.478+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:39.488+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:39.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:16:49.763+0000] {processor.py:161} INFO - Started process (PID=88221) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:49.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:49.767+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:49.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:49.822+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:49.813+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:49.823+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:49.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:16:58.537+0000] {processor.py:161} INFO - Started process (PID=88304) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:58.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:16:58.540+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:58.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:58.588+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:58.581+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:16:58.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:16:58.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:17:06.696+0000] {processor.py:161} INFO - Started process (PID=88393) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:06.698+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:06.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:06.699+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:06.743+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:06.737+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:06.744+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:06.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:17:13.457+0000] {processor.py:161} INFO - Started process (PID=88476) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:13.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:13.461+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:13.460+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:13.516+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:13.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:13.517+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:13.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:17:20.481+0000] {processor.py:161} INFO - Started process (PID=88559) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:20.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:20.484+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:20.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:20.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:20.524+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:20.535+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:20.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:17:31.647+0000] {processor.py:161} INFO - Started process (PID=88648) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:31.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:31.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:31.657+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:31.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:31.762+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:31.777+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:31.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.182 seconds
[2024-05-10T16:17:41.793+0000] {processor.py:161} INFO - Started process (PID=88731) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:41.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:41.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:41.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:41.844+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:41.837+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:41.845+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:41.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:17:49.974+0000] {processor.py:161} INFO - Started process (PID=88815) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:49.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:49.982+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:49.981+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:50.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:50.036+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:50.046+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:50.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T16:17:59.282+0000] {processor.py:161} INFO - Started process (PID=88898) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:59.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:17:59.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:59.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:59.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:59.320+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:17:59.329+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:17:59.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:18:08.236+0000] {processor.py:161} INFO - Started process (PID=88988) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:08.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:08.239+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:08.239+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:08.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:08.277+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:08.286+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:08.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:18:18.019+0000] {processor.py:161} INFO - Started process (PID=89073) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:18.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:18.022+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:18.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:18.060+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:18.053+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:18.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:18.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T16:18:26.306+0000] {processor.py:161} INFO - Started process (PID=89158) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:26.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:26.309+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:26.309+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:26.353+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:26.345+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:26.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:26.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:18:37.365+0000] {processor.py:161} INFO - Started process (PID=89248) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:37.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:37.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:37.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:37.467+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:37.451+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:37.471+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:37.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.171 seconds
[2024-05-10T16:18:45.887+0000] {processor.py:161} INFO - Started process (PID=89333) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:45.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:45.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:45.889+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:45.933+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:45.926+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:45.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:45.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:18:53.510+0000] {processor.py:161} INFO - Started process (PID=89417) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:53.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:18:53.513+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:53.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:53.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:53.547+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:18:53.555+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:18:53.570+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:19:00.969+0000] {processor.py:161} INFO - Started process (PID=89507) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:00.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:00.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:00.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:01.019+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:01.011+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:01.020+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:01.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:19:11.199+0000] {processor.py:161} INFO - Started process (PID=89591) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:11.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:11.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:11.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:11.280+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:11.269+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:11.283+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:11.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T16:19:22.367+0000] {processor.py:161} INFO - Started process (PID=89675) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:22.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:22.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:22.369+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:22.410+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:22.403+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:22.411+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:22.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:19:30.885+0000] {processor.py:161} INFO - Started process (PID=89765) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:30.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:30.889+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:30.888+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:30.953+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:30.944+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:30.955+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:30.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T16:19:39.601+0000] {processor.py:161} INFO - Started process (PID=89849) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:39.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:39.604+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:39.603+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:39.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:39.641+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:39.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:39.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:19:46.788+0000] {processor.py:161} INFO - Started process (PID=89933) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:46.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:46.791+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:46.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:46.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:46.827+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:46.835+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:46.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:19:55.462+0000] {processor.py:161} INFO - Started process (PID=90017) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:55.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:19:55.465+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:55.465+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:55.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:55.505+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:19:55.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:19:55.527+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:20:02.583+0000] {processor.py:161} INFO - Started process (PID=90107) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:02.585+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:02.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:02.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:02.635+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:02.628+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:02.637+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:02.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:20:10.685+0000] {processor.py:161} INFO - Started process (PID=90191) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:10.687+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:10.688+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:10.688+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:10.734+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:10.727+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:10.736+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:10.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:20:19.692+0000] {processor.py:161} INFO - Started process (PID=90275) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:19.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:19.696+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:19.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:19.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:19.763+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:19.777+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:19.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T16:20:30.701+0000] {processor.py:161} INFO - Started process (PID=90365) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:30.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:30.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:30.707+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:30.810+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:30.792+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:30.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:30.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.180 seconds
[2024-05-10T16:20:39.702+0000] {processor.py:161} INFO - Started process (PID=90450) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:39.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:39.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:39.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:39.785+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:39.750+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:39.789+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:39.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.123 seconds
[2024-05-10T16:20:48.278+0000] {processor.py:161} INFO - Started process (PID=90534) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:48.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:48.281+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:48.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:48.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:48.319+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:48.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:48.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:20:55.590+0000] {processor.py:161} INFO - Started process (PID=90619) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:55.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:20:55.593+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:55.592+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:55.639+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:55.631+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:20:55.640+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:20:55.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:21:04.509+0000] {processor.py:161} INFO - Started process (PID=90709) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:04.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:04.514+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:04.514+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:04.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:04.575+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:04.585+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:04.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.121 seconds
[2024-05-10T16:21:14.038+0000] {processor.py:161} INFO - Started process (PID=90794) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:14.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:14.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:14.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:14.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:14.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:14.105+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:14.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T16:21:26.693+0000] {processor.py:161} INFO - Started process (PID=90878) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:26.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:26.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:26.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:26.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:26.738+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:26.749+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:26.764+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:21:34.602+0000] {processor.py:161} INFO - Started process (PID=90969) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:34.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:34.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:34.605+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:34.651+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:34.644+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:34.652+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:34.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:21:44.728+0000] {processor.py:161} INFO - Started process (PID=91053) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:44.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:44.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:44.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:44.783+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:44.776+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:44.785+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:44.804+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:21:52.767+0000] {processor.py:161} INFO - Started process (PID=91138) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:52.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:21:52.769+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:52.769+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:52.811+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:52.804+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:21:52.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:21:52.827+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:22:00.324+0000] {processor.py:161} INFO - Started process (PID=91222) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:00.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:00.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:00.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:00.374+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:00.368+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:00.376+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:00.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:22:08.939+0000] {processor.py:161} INFO - Started process (PID=91312) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:08.941+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:08.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:08.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:08.985+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:08.978+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:08.986+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:09.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:22:19.335+0000] {processor.py:161} INFO - Started process (PID=91396) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:19.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:19.338+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:19.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:19.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:19.379+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:19.388+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:19.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:22:27.767+0000] {processor.py:161} INFO - Started process (PID=91480) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:27.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:27.771+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:27.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:27.832+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:27.822+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:27.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:27.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T16:22:36.382+0000] {processor.py:161} INFO - Started process (PID=91571) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:36.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:36.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:36.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:36.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:36.427+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:36.435+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:36.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:22:45.887+0000] {processor.py:161} INFO - Started process (PID=91655) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:45.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:45.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:45.890+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:45.957+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:45.948+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:45.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:45.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.112 seconds
[2024-05-10T16:22:57.036+0000] {processor.py:161} INFO - Started process (PID=91740) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:57.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:22:57.039+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:57.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:57.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:57.080+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:22:57.090+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:22:57.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:23:05.357+0000] {processor.py:161} INFO - Started process (PID=91831) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:05.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:05.361+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:05.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:05.402+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:05.396+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:05.404+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:05.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:23:13.222+0000] {processor.py:161} INFO - Started process (PID=91915) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:13.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:13.225+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:13.225+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:13.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:13.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:13.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:13.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T16:23:21.554+0000] {processor.py:161} INFO - Started process (PID=91999) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:21.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:21.557+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:21.557+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:21.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:21.604+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:21.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:21.631+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T16:23:30.400+0000] {processor.py:161} INFO - Started process (PID=92083) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:30.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:30.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:30.402+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:30.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:30.441+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:30.452+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:30.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:23:38.828+0000] {processor.py:161} INFO - Started process (PID=92174) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:38.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:38.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:38.832+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:38.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:38.879+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:38.888+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:38.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:23:46.546+0000] {processor.py:161} INFO - Started process (PID=92258) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:46.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:46.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:46.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:46.590+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:46.584+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:46.591+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:46.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:23:53.756+0000] {processor.py:161} INFO - Started process (PID=92342) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:53.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:23:53.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:53.758+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:53.804+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:53.797+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:23:53.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:23:53.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:24:05.364+0000] {processor.py:161} INFO - Started process (PID=92432) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:05.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:05.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:05.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:05.437+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:05.427+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:05.439+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:05.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T16:24:14.439+0000] {processor.py:161} INFO - Started process (PID=92517) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:14.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:14.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:14.441+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:14.491+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:14.484+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:14.493+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:14.509+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:24:23.291+0000] {processor.py:161} INFO - Started process (PID=92601) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:23.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:23.299+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:23.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:23.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:23.342+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:23.351+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:23.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T16:24:31.242+0000] {processor.py:161} INFO - Started process (PID=92691) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:31.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:31.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:31.245+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:31.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:31.320+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:31.359+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:31.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.155 seconds
[2024-05-10T16:24:44.491+0000] {processor.py:161} INFO - Started process (PID=92775) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:44.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:44.495+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:44.494+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:44.556+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:44.545+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:44.557+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:44.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.099 seconds
[2024-05-10T16:24:53.519+0000] {processor.py:161} INFO - Started process (PID=92860) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:53.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:24:53.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:53.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:53.574+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:53.567+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:24:53.575+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:24:53.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:25:01.844+0000] {processor.py:161} INFO - Started process (PID=92951) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:01.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:25:01.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:01.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:01.910+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:01.900+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:25:01.913+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:01.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T16:25:14.029+0000] {processor.py:161} INFO - Started process (PID=93035) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:14.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:25:14.032+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:14.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:14.081+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:14.072+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:25:14.083+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:14.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:25:28.609+0000] {processor.py:161} INFO - Started process (PID=93119) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:28.611+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:25:28.613+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:28.613+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:28.689+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:28.677+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:25:28.690+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:28.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T16:25:44.066+0000] {processor.py:161} INFO - Started process (PID=93209) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:44.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:25:44.073+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:44.072+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:44.147+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:44.136+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:25:44.150+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:44.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T16:25:52.415+0000] {processor.py:161} INFO - Started process (PID=93293) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:52.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:25:52.418+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:52.418+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:52.472+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:52.463+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:25:52.473+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:25:52.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:26:03.784+0000] {processor.py:161} INFO - Started process (PID=93382) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:03.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:03.788+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:03.787+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:03.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:03.833+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:03.844+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:03.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T16:26:12.987+0000] {processor.py:161} INFO - Started process (PID=93465) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:12.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:12.990+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:12.990+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:13.039+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:13.032+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:13.040+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:13.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:26:20.521+0000] {processor.py:161} INFO - Started process (PID=93549) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:20.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:20.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:20.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:20.578+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:20.568+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:20.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:20.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:26:28.613+0000] {processor.py:161} INFO - Started process (PID=93632) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:28.615+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:28.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:28.616+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:28.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:28.661+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:28.671+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:28.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T16:26:39.086+0000] {processor.py:161} INFO - Started process (PID=93721) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:39.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:39.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:39.089+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:39.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:39.125+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:39.133+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:39.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:26:46.930+0000] {processor.py:161} INFO - Started process (PID=93805) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:46.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:46.932+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:46.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:46.971+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:46.964+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:46.972+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:46.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T16:26:55.741+0000] {processor.py:161} INFO - Started process (PID=93888) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:55.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:26:55.744+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:55.743+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:55.792+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:26:55.783+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:26:55.793+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:26:55.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:27:04.499+0000] {processor.py:161} INFO - Started process (PID=93978) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:04.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:04.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:04.501+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:04.551+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:04.544+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:04.552+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:04.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:27:17.271+0000] {processor.py:161} INFO - Started process (PID=94063) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:17.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:17.276+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:17.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:17.346+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:17.335+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:17.350+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:17.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.127 seconds
[2024-05-10T16:27:27.155+0000] {processor.py:161} INFO - Started process (PID=94147) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:27.160+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:27.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:27.162+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:27.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:27.248+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:27.263+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:27.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.157 seconds
[2024-05-10T16:27:36.985+0000] {processor.py:161} INFO - Started process (PID=94237) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:36.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:36.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:36.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:37.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:37.023+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:37.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:37.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:27:46.308+0000] {processor.py:161} INFO - Started process (PID=94322) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:46.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:46.312+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:46.311+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:46.373+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:46.361+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:46.375+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:46.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T16:27:58.758+0000] {processor.py:161} INFO - Started process (PID=94407) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:58.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:27:58.760+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:58.760+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:58.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:58.796+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:27:58.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:27:58.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:28:07.692+0000] {processor.py:161} INFO - Started process (PID=94497) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:07.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:07.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:07.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:07.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:07.726+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:07.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:07.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:28:18.159+0000] {processor.py:161} INFO - Started process (PID=94581) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:18.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:18.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:18.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:18.212+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:18.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:18.213+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:18.228+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:28:28.863+0000] {processor.py:161} INFO - Started process (PID=94664) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:28.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:28.868+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:28.867+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:28.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:28.918+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:28.930+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:28.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.108 seconds
[2024-05-10T16:28:42.447+0000] {processor.py:161} INFO - Started process (PID=94753) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:42.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:42.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:42.449+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:42.493+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:42.487+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:42.494+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:42.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:28:50.265+0000] {processor.py:161} INFO - Started process (PID=94836) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:50.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:50.267+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:50.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:50.309+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:50.302+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:50.310+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:50.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:28:59.623+0000] {processor.py:161} INFO - Started process (PID=94919) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:59.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:28:59.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:59.625+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:59.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:59.660+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:28:59.668+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:28:59.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:29:12.171+0000] {processor.py:161} INFO - Started process (PID=95008) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:12.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:29:12.174+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:12.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:12.221+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:12.213+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:29:12.222+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:12.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:29:23.664+0000] {processor.py:161} INFO - Started process (PID=95091) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:23.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:29:23.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:23.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:23.721+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:23.709+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:29:23.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:23.755+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T16:29:34.668+0000] {processor.py:161} INFO - Started process (PID=95181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:34.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:29:34.672+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:34.671+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:34.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:34.727+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:29:34.746+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:34.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.128 seconds
[2024-05-10T16:29:47.964+0000] {processor.py:161} INFO - Started process (PID=95265) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:47.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:29:47.967+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:47.966+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:48.006+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:47.998+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:29:48.007+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:48.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:29:59.843+0000] {processor.py:161} INFO - Started process (PID=95348) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:59.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:29:59.848+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:59.847+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:59.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:29:59.899+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:29:59.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:29:59.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T16:30:15.962+0000] {processor.py:161} INFO - Started process (PID=95437) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:15.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:30:15.966+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:15.965+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:16.029+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:16.020+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:30:16.030+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:16.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T16:30:30.338+0000] {processor.py:161} INFO - Started process (PID=95520) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:30.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:30:30.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:30.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:30.387+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:30.379+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:30:30.389+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:30.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:30:38.888+0000] {processor.py:161} INFO - Started process (PID=95610) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:38.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:30:38.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:38.891+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:38.946+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:38.937+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:30:38.949+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:38.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T16:30:49.028+0000] {processor.py:161} INFO - Started process (PID=95693) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:49.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:30:49.032+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:49.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:49.106+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:49.094+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:30:49.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:49.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.116 seconds
[2024-05-10T16:30:58.386+0000] {processor.py:161} INFO - Started process (PID=95776) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:58.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:30:58.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:58.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:58.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:30:58.435+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:30:58.446+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:30:58.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:31:06.422+0000] {processor.py:161} INFO - Started process (PID=95865) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:06.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:06.426+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:06.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:06.471+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:06.465+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:06.473+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:06.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:31:13.581+0000] {processor.py:161} INFO - Started process (PID=95949) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:13.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:13.584+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:13.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:13.644+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:13.635+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:13.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:13.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T16:31:22.312+0000] {processor.py:161} INFO - Started process (PID=96032) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:22.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:22.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:22.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:22.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:22.351+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:22.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:22.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:31:32.721+0000] {processor.py:161} INFO - Started process (PID=96121) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:32.725+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:32.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:32.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:32.851+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:32.836+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:32.856+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:32.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.228 seconds
[2024-05-10T16:31:44.448+0000] {processor.py:161} INFO - Started process (PID=96204) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:44.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:44.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:44.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:44.540+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:44.526+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:44.547+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:44.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.141 seconds
[2024-05-10T16:31:53.573+0000] {processor.py:161} INFO - Started process (PID=96287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:53.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:31:53.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:53.576+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:53.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:31:53.625+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:31:53.633+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:31:53.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T16:32:03.311+0000] {processor.py:161} INFO - Started process (PID=96377) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:03.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:03.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:03.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:03.372+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:03.361+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:03.373+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:03.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T16:32:12.975+0000] {processor.py:161} INFO - Started process (PID=96460) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:12.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:12.980+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:12.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:13.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:13.044+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:13.058+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:13.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.126 seconds
[2024-05-10T16:32:22.984+0000] {processor.py:161} INFO - Started process (PID=96543) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:22.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:22.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:22.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:23.069+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:23.059+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:23.071+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:23.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.130 seconds
[2024-05-10T16:32:32.259+0000] {processor.py:161} INFO - Started process (PID=96632) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:32.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:32.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:32.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:32.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:32.295+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:32.302+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:32.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T16:32:41.121+0000] {processor.py:161} INFO - Started process (PID=96715) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:41.123+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:41.125+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:41.125+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:41.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:41.170+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:41.178+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:41.194+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:32:51.041+0000] {processor.py:161} INFO - Started process (PID=96798) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:51.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:32:51.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:51.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:51.086+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:32:51.080+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:32:51.087+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:32:51.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:33:00.177+0000] {processor.py:161} INFO - Started process (PID=96882) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:00.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:00.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:00.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:00.224+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:00.218+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:00.225+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:00.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:33:10.242+0000] {processor.py:161} INFO - Started process (PID=96971) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:10.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:10.244+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:10.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:10.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:10.283+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:10.291+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:10.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:33:19.147+0000] {processor.py:161} INFO - Started process (PID=97054) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:19.148+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:19.150+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:19.150+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:19.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:19.201+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:19.211+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:19.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T16:33:28.412+0000] {processor.py:161} INFO - Started process (PID=97137) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:28.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:28.415+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:28.414+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:28.460+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:28.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:28.461+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:28.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:33:39.183+0000] {processor.py:161} INFO - Started process (PID=97226) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:39.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:39.186+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:39.186+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:39.269+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:39.259+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:39.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:39.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.122 seconds
[2024-05-10T16:33:49.701+0000] {processor.py:161} INFO - Started process (PID=97310) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:49.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:33:49.704+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:49.703+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:49.753+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:33:49.746+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:33:49.754+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:33:49.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:34:00.642+0000] {processor.py:161} INFO - Started process (PID=97393) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:00.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:00.651+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:00.650+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:00.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:00.718+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:00.731+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:00.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.138 seconds
[2024-05-10T16:34:11.100+0000] {processor.py:161} INFO - Started process (PID=97482) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:11.102+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:11.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:11.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:11.150+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:11.143+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:11.152+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:11.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:34:20.992+0000] {processor.py:161} INFO - Started process (PID=97565) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:20.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:20.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:20.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:21.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:21.044+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:21.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:21.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:34:29.439+0000] {processor.py:161} INFO - Started process (PID=97648) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:29.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:29.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:29.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:29.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:29.476+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:29.485+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:29.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:34:40.196+0000] {processor.py:161} INFO - Started process (PID=97737) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:40.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:40.201+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:40.200+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:40.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:40.253+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:40.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:40.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.102 seconds
[2024-05-10T16:34:49.992+0000] {processor.py:161} INFO - Started process (PID=97820) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:49.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:49.995+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:49.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:50.046+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:50.039+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:50.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:50.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:34:57.606+0000] {processor.py:161} INFO - Started process (PID=97904) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:57.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:34:57.609+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:57.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:57.655+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:34:57.647+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:34:57.656+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:34:57.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:35:06.480+0000] {processor.py:161} INFO - Started process (PID=97993) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:06.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:06.484+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:06.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:06.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:06.529+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:06.539+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:06.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:35:18.008+0000] {processor.py:161} INFO - Started process (PID=98077) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:18.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:18.013+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:18.012+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:18.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:18.066+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:18.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:18.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T16:35:29.088+0000] {processor.py:161} INFO - Started process (PID=98160) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:29.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:29.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:29.090+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:29.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:29.137+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:29.148+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:29.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T16:35:38.414+0000] {processor.py:161} INFO - Started process (PID=98249) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:38.415+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:38.417+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:38.417+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:38.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:38.457+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:38.466+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:38.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:35:46.786+0000] {processor.py:161} INFO - Started process (PID=98333) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:46.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:46.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:46.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:46.847+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:46.837+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:46.850+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:46.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T16:35:56.208+0000] {processor.py:161} INFO - Started process (PID=98416) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:56.210+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:35:56.212+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:56.211+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:56.269+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:35:56.261+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:35:56.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:35:56.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T16:36:06.161+0000] {processor.py:161} INFO - Started process (PID=98505) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:06.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:06.165+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:06.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:06.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:06.208+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:06.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:06.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:36:14.286+0000] {processor.py:161} INFO - Started process (PID=98588) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:14.288+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:14.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:14.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:14.337+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:14.330+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:14.339+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:14.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:36:25.123+0000] {processor.py:161} INFO - Started process (PID=98671) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:25.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:25.127+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:25.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:25.175+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:25.169+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:25.176+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:25.192+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:36:37.775+0000] {processor.py:161} INFO - Started process (PID=98760) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:37.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:37.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:37.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:37.846+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:37.831+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:37.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:37.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T16:36:50.002+0000] {processor.py:161} INFO - Started process (PID=98843) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:50.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:50.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:50.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:50.049+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:50.042+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:50.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:50.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:36:57.627+0000] {processor.py:161} INFO - Started process (PID=98926) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:57.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:36:57.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:57.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:57.682+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:36:57.674+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:36:57.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:36:57.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:37:10.832+0000] {processor.py:161} INFO - Started process (PID=99016) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:10.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:10.836+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:10.835+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:10.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:10.915+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:10.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:10.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.149 seconds
[2024-05-10T16:37:23.692+0000] {processor.py:161} INFO - Started process (PID=99100) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:23.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:23.696+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:23.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:23.746+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:23.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:23.747+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:23.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:37:31.649+0000] {processor.py:161} INFO - Started process (PID=99183) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:31.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:31.653+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:31.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:31.703+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:31.697+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:31.704+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:31.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:37:39.484+0000] {processor.py:161} INFO - Started process (PID=99272) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:39.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:39.487+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:39.486+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:39.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:39.524+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:39.532+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:39.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:37:46.565+0000] {processor.py:161} INFO - Started process (PID=99355) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:46.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:46.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:46.567+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:46.614+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:46.606+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:46.615+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:46.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:37:54.997+0000] {processor.py:161} INFO - Started process (PID=99438) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:54.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:37:54.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:54.999+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:55.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:37:55.033+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:37:55.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:37:55.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:38:01.711+0000] {processor.py:161} INFO - Started process (PID=99521) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:01.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:01.714+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:01.714+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:01.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:01.748+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:01.757+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:01.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:38:09.368+0000] {processor.py:161} INFO - Started process (PID=99610) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:09.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:09.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:09.371+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:09.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:09.406+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:09.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:09.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:38:17.063+0000] {processor.py:161} INFO - Started process (PID=99693) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:17.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:17.067+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:17.066+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:17.118+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:17.111+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:17.119+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:17.133+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:38:25.752+0000] {processor.py:161} INFO - Started process (PID=99777) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:25.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:25.755+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:25.754+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:25.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:25.797+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:25.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:25.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:38:33.158+0000] {processor.py:161} INFO - Started process (PID=99866) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:33.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:33.161+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:33.160+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:33.202+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:33.195+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:33.203+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:33.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:38:41.286+0000] {processor.py:161} INFO - Started process (PID=99949) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:41.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:41.289+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:41.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:41.337+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:41.329+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:41.339+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:41.354+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:38:48.538+0000] {processor.py:161} INFO - Started process (PID=333) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:48.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:48.541+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:48.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:48.582+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:48.575+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:48.583+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:48.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:38:55.423+0000] {processor.py:161} INFO - Started process (PID=416) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:55.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:38:55.426+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:55.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:55.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:38:55.461+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:38:55.470+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:38:55.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:39:02.847+0000] {processor.py:161} INFO - Started process (PID=499) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:02.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:02.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:02.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:02.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:02.886+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:02.894+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:02.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:39:10.453+0000] {processor.py:161} INFO - Started process (PID=588) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:10.454+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:10.456+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:10.455+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:10.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:10.496+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:10.504+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:10.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:39:18.656+0000] {processor.py:161} INFO - Started process (PID=671) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:18.658+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:18.660+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:18.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:18.720+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:18.711+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:18.722+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:18.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T16:39:26.596+0000] {processor.py:161} INFO - Started process (PID=755) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:26.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:26.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:26.599+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:26.643+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:26.635+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:26.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:26.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:39:36.469+0000] {processor.py:161} INFO - Started process (PID=844) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:36.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:36.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:36.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:36.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:36.516+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:36.526+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:36.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:39:46.371+0000] {processor.py:161} INFO - Started process (PID=928) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:46.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:46.374+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:46.373+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:46.424+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:46.415+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:46.425+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:46.441+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:39:54.547+0000] {processor.py:161} INFO - Started process (PID=1012) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:54.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:39:54.550+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:54.550+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:54.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:39:54.589+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:39:54.597+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:39:54.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:40:02.086+0000] {processor.py:161} INFO - Started process (PID=1096) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:02.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:02.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:02.088+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:02.131+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:02.126+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:02.133+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:02.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:40:11.347+0000] {processor.py:161} INFO - Started process (PID=1186) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:11.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:11.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:11.350+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:11.400+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:11.391+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:11.401+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:11.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:40:20.272+0000] {processor.py:161} INFO - Started process (PID=1269) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:20.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:20.275+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:20.274+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:20.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:20.313+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:20.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:20.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:40:30.137+0000] {processor.py:161} INFO - Started process (PID=1352) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:30.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:30.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:30.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:30.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:30.182+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:30.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:30.204+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:40:37.569+0000] {processor.py:161} INFO - Started process (PID=1442) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:37.570+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:37.572+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:37.572+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:37.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:37.610+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:37.618+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:37.635+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:40:47.219+0000] {processor.py:161} INFO - Started process (PID=1525) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:47.221+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:47.223+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:47.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:47.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:47.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:47.283+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:47.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T16:40:58.877+0000] {processor.py:161} INFO - Started process (PID=1608) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:58.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:40:58.881+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:58.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:58.925+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:40:58.917+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:40:58.926+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:40:58.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:41:06.871+0000] {processor.py:161} INFO - Started process (PID=1699) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:06.872+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:06.873+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:06.873+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:06.913+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:06.907+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:06.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:06.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:41:14.421+0000] {processor.py:161} INFO - Started process (PID=1782) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:14.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:14.424+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:14.423+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:14.478+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:14.470+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:14.480+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:14.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T16:41:27.351+0000] {processor.py:161} INFO - Started process (PID=1865) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:27.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:27.356+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:27.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:27.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:27.406+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:27.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:27.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T16:41:37.424+0000] {processor.py:161} INFO - Started process (PID=1955) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:37.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:37.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:37.426+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:37.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:37.470+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:37.481+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:37.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:41:45.690+0000] {processor.py:161} INFO - Started process (PID=2038) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:45.692+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:45.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:45.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:45.746+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:45.738+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:45.748+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:45.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:41:53.799+0000] {processor.py:161} INFO - Started process (PID=2121) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:53.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:41:53.802+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:53.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:53.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:41:53.844+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:41:53.854+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:41:53.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:42:02.822+0000] {processor.py:161} INFO - Started process (PID=2204) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:02.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:02.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:02.825+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:02.870+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:02.863+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:02.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:02.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:42:11.593+0000] {processor.py:161} INFO - Started process (PID=2293) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:11.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:11.597+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:11.596+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:11.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:11.657+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:11.668+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:11.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T16:42:20.160+0000] {processor.py:161} INFO - Started process (PID=2376) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:20.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:20.166+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:20.165+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:20.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:20.219+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:20.230+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:20.246+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T16:42:28.537+0000] {processor.py:161} INFO - Started process (PID=2459) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:28.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:28.541+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:28.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:28.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:28.596+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:28.606+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:28.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T16:42:36.699+0000] {processor.py:161} INFO - Started process (PID=2548) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:36.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:36.702+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:36.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:36.746+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:36.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:36.747+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:36.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:42:45.470+0000] {processor.py:161} INFO - Started process (PID=2632) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:45.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:45.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:45.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:45.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:45.520+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:45.529+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:45.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T16:42:52.843+0000] {processor.py:161} INFO - Started process (PID=2715) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:52.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:52.846+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:52.845+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:52.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:52.883+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:52.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:52.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:42:59.758+0000] {processor.py:161} INFO - Started process (PID=2795) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:59.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:42:59.760+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:59.760+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:59.837+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:42:59.828+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:42:59.839+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:42:59.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T16:43:11.401+0000] {processor.py:161} INFO - Started process (PID=2884) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:11.403+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:11.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:11.404+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:11.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:11.444+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:11.453+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:11.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:43:22.612+0000] {processor.py:161} INFO - Started process (PID=2967) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:22.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:22.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:22.615+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:22.666+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:22.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:22.667+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:22.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:43:32.321+0000] {processor.py:161} INFO - Started process (PID=3051) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:32.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:32.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:32.324+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:32.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:32.362+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:32.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:32.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:43:39.846+0000] {processor.py:161} INFO - Started process (PID=3140) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:39.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:39.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:39.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:39.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:39.884+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:39.892+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:39.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:43:47.274+0000] {processor.py:161} INFO - Started process (PID=3223) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:47.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:47.276+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:47.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:47.314+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:47.308+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:47.315+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:47.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:43:55.497+0000] {processor.py:161} INFO - Started process (PID=3306) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:55.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:43:55.500+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:55.499+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:55.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:43:55.542+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:43:55.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:43:55.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:44:04.473+0000] {processor.py:161} INFO - Started process (PID=3396) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:04.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:04.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:04.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:04.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:04.520+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:04.529+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:04.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:44:14.361+0000] {processor.py:161} INFO - Started process (PID=3479) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:14.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:14.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:14.364+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:14.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:14.404+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:14.414+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:14.431+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:44:24.283+0000] {processor.py:161} INFO - Started process (PID=3562) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:24.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:24.290+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:24.289+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:24.369+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:24.361+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:24.371+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:24.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T16:44:35.739+0000] {processor.py:161} INFO - Started process (PID=3650) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:35.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:35.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:35.741+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:35.795+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:35.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:35.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:35.818+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T16:44:46.461+0000] {processor.py:161} INFO - Started process (PID=3734) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:46.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:46.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:46.463+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:46.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:46.499+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:46.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:46.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:44:54.378+0000] {processor.py:161} INFO - Started process (PID=3818) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:54.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:44:54.428+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:54.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:54.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:44:54.474+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:44:54.481+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:44:54.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.147 seconds
[2024-05-10T16:45:01.397+0000] {processor.py:161} INFO - Started process (PID=3901) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:01.399+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:01.401+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:01.400+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:01.454+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:01.447+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:01.455+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:01.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:45:12.211+0000] {processor.py:161} INFO - Started process (PID=3990) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:12.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:12.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:12.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:12.264+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:12.256+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:12.266+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:12.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:45:21.937+0000] {processor.py:161} INFO - Started process (PID=4074) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:21.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:21.941+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:21.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:21.994+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:21.986+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:21.996+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:22.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T16:45:30.477+0000] {processor.py:161} INFO - Started process (PID=4157) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:30.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:30.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:30.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:30.537+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:30.528+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:30.538+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:30.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:45:38.285+0000] {processor.py:161} INFO - Started process (PID=4246) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:38.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:38.288+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:38.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:38.337+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:38.329+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:38.338+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:38.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:45:49.122+0000] {processor.py:161} INFO - Started process (PID=4329) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:49.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:49.137+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:49.136+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:49.257+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:49.242+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:49.260+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:49.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.202 seconds
[2024-05-10T16:45:57.925+0000] {processor.py:161} INFO - Started process (PID=4412) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:57.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:45:57.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:57.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:57.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:45:57.968+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:45:57.977+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:45:57.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:46:06.816+0000] {processor.py:161} INFO - Started process (PID=4501) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:06.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:06.820+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:06.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:06.884+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:06.876+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:06.886+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:06.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.107 seconds
[2024-05-10T16:46:17.893+0000] {processor.py:161} INFO - Started process (PID=4585) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:17.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:17.897+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:17.896+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:17.945+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:17.937+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:17.946+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:17.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:46:27.109+0000] {processor.py:161} INFO - Started process (PID=4669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:27.110+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:27.112+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:27.112+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:27.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:27.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:27.159+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:27.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:46:34.716+0000] {processor.py:161} INFO - Started process (PID=4758) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:34.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:34.719+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:34.719+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:34.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:34.756+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:34.766+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:34.782+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:46:42.597+0000] {processor.py:161} INFO - Started process (PID=4841) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:42.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:42.600+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:42.600+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:42.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:42.640+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:42.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:42.663+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:46:49.755+0000] {processor.py:161} INFO - Started process (PID=4924) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:49.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:49.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:49.758+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:49.808+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:49.800+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:49.809+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:49.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:46:59.242+0000] {processor.py:161} INFO - Started process (PID=5007) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:59.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:46:59.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:59.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:59.343+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:46:59.325+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:46:59.346+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:46:59.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.160 seconds
[2024-05-10T16:47:10.173+0000] {processor.py:161} INFO - Started process (PID=5098) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:10.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:47:10.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:10.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:10.223+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:10.216+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:47:10.225+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:10.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:47:21.262+0000] {processor.py:161} INFO - Started process (PID=5181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:21.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:47:21.265+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:21.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:21.314+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:21.307+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:47:21.316+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:21.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:47:31.375+0000] {processor.py:161} INFO - Started process (PID=5264) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:31.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:47:31.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:31.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:31.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:31.425+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:47:31.435+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:31.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:47:42.354+0000] {processor.py:161} INFO - Started process (PID=5353) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:42.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:47:42.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:42.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:42.407+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:42.399+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:47:42.408+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:42.423+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:47:55.690+0000] {processor.py:161} INFO - Started process (PID=5436) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:55.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:47:55.692+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:55.692+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:55.742+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:47:55.735+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:47:55.743+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:47:55.757+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:48:04.131+0000] {processor.py:161} INFO - Started process (PID=5519) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:04.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:04.135+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:04.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:04.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:04.182+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:04.192+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:04.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T16:48:15.168+0000] {processor.py:161} INFO - Started process (PID=5608) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:15.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:15.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:15.170+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:15.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:15.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:15.212+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:15.225+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T16:48:23.125+0000] {processor.py:161} INFO - Started process (PID=5692) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:23.126+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:23.128+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:23.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:23.180+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:23.172+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:23.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:23.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:48:31.324+0000] {processor.py:161} INFO - Started process (PID=5775) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:31.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:31.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:31.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:31.371+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:31.363+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:31.372+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:31.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:48:38.520+0000] {processor.py:161} INFO - Started process (PID=5865) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:38.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:38.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:38.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:38.575+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:38.568+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:38.576+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:38.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:48:48.488+0000] {processor.py:161} INFO - Started process (PID=5948) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:48.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:48.491+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:48.490+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:48.542+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:48.534+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:48.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:48.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:48:56.070+0000] {processor.py:161} INFO - Started process (PID=6031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:56.071+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:48:56.073+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:56.073+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:56.114+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:48:56.108+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:48:56.115+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:48:56.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:49:02.628+0000] {processor.py:161} INFO - Started process (PID=6114) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:02.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:02.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:02.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:02.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:02.673+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:02.682+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:02.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:49:10.937+0000] {processor.py:161} INFO - Started process (PID=6204) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:10.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:10.940+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:10.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:10.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:10.980+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:10.990+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:11.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:49:19.079+0000] {processor.py:161} INFO - Started process (PID=6287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:19.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:19.083+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:19.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:19.139+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:19.131+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:19.140+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:19.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T16:49:30.299+0000] {processor.py:161} INFO - Started process (PID=6370) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:30.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:30.302+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:30.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:30.345+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:30.338+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:30.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:30.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:49:38.038+0000] {processor.py:161} INFO - Started process (PID=6460) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:38.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:38.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:38.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:38.085+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:38.078+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:38.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:38.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:49:45.672+0000] {processor.py:161} INFO - Started process (PID=6543) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:45.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:45.675+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:45.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:45.722+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:45.715+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:45.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:45.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:49:53.335+0000] {processor.py:161} INFO - Started process (PID=6626) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:53.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:49:53.338+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:53.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:53.384+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:49:53.377+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:49:53.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:49:53.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:50:02.148+0000] {processor.py:161} INFO - Started process (PID=6709) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:02.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:02.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:02.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:02.195+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:02.188+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:02.197+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:02.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:50:11.693+0000] {processor.py:161} INFO - Started process (PID=6798) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:11.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:11.696+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:11.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:11.741+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:11.735+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:11.743+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:11.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T16:50:19.794+0000] {processor.py:161} INFO - Started process (PID=6881) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:19.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:19.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:19.797+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:19.846+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:19.839+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:19.848+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:19.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:50:27.872+0000] {processor.py:161} INFO - Started process (PID=6964) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:27.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:27.875+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:27.875+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:27.922+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:27.915+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:27.923+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:27.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:50:36.537+0000] {processor.py:161} INFO - Started process (PID=7053) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:36.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:36.541+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:36.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:36.586+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:36.580+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:36.588+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:36.605+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:50:45.262+0000] {processor.py:161} INFO - Started process (PID=7136) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:45.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:45.266+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:45.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:45.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:45.312+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:45.322+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:45.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T16:50:52.456+0000] {processor.py:161} INFO - Started process (PID=7219) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:52.457+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:52.459+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:52.458+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:52.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:52.500+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:52.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:52.523+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:50:59.181+0000] {processor.py:161} INFO - Started process (PID=7302) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:59.182+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:50:59.183+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:59.183+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:59.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:50:59.223+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:50:59.231+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:50:59.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:51:06.312+0000] {processor.py:161} INFO - Started process (PID=7391) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:06.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:06.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:06.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:06.356+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:06.349+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:06.357+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:06.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:51:14.665+0000] {processor.py:161} INFO - Started process (PID=7474) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:14.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:14.668+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:14.668+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:14.719+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:14.712+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:14.720+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:14.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:51:25.595+0000] {processor.py:161} INFO - Started process (PID=7557) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:25.596+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:25.598+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:25.597+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:25.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:25.638+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:25.648+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:25.664+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:51:32.155+0000] {processor.py:161} INFO - Started process (PID=7640) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:32.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:32.159+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:32.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:32.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:32.197+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:32.206+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:32.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:51:39.008+0000] {processor.py:161} INFO - Started process (PID=7729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:39.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:39.011+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:39.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:39.057+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:39.050+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:39.058+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:39.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:51:46.772+0000] {processor.py:161} INFO - Started process (PID=7812) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:46.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:46.776+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:46.776+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:46.829+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:46.823+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:46.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:46.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T16:51:54.331+0000] {processor.py:161} INFO - Started process (PID=7896) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:54.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:51:54.333+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:54.333+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:54.390+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:51:54.382+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:51:54.391+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:51:54.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T16:52:00.526+0000] {processor.py:161} INFO - Started process (PID=7979) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:00.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:00.529+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:00.529+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:00.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:00.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:00.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:00.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:52:07.186+0000] {processor.py:161} INFO - Started process (PID=8068) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:07.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:07.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:07.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:07.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:07.227+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:07.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:07.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:52:14.781+0000] {processor.py:161} INFO - Started process (PID=8151) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:14.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:14.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:14.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:14.832+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:14.824+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:14.833+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:14.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:52:23.813+0000] {processor.py:161} INFO - Started process (PID=8234) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:23.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:23.817+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:23.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:23.870+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:23.863+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:23.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:23.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T16:52:31.106+0000] {processor.py:161} INFO - Started process (PID=8317) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:31.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:31.109+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:31.108+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:31.157+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:31.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:31.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:31.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:52:37.604+0000] {processor.py:161} INFO - Started process (PID=8406) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:37.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:37.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:37.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:37.648+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:37.641+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:37.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:37.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:52:45.377+0000] {processor.py:161} INFO - Started process (PID=8489) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:45.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:45.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:45.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:45.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:45.423+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:45.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:45.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T16:52:53.110+0000] {processor.py:161} INFO - Started process (PID=8572) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:53.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:52:53.113+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:53.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:53.160+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:52:53.153+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:52:53.162+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:52:53.176+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:53:00.706+0000] {processor.py:161} INFO - Started process (PID=8655) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:00.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:00.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:00.708+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:00.750+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:00.743+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:00.751+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:00.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T16:53:07.715+0000] {processor.py:161} INFO - Started process (PID=8744) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:07.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:07.717+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:07.717+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:07.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:07.752+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:07.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:07.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:53:14.297+0000] {processor.py:161} INFO - Started process (PID=8827) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:14.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:14.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:14.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:14.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:14.336+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:14.343+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:14.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:53:22.767+0000] {processor.py:161} INFO - Started process (PID=8910) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:22.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:22.770+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:22.769+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:22.819+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:22.810+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:22.820+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:22.834+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:53:30.337+0000] {processor.py:161} INFO - Started process (PID=8993) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:30.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:30.340+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:30.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:30.383+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:30.377+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:30.384+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:30.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:53:37.801+0000] {processor.py:161} INFO - Started process (PID=9082) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:37.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:37.804+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:37.803+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:37.851+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:37.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:37.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:37.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:53:44.449+0000] {processor.py:161} INFO - Started process (PID=9165) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:44.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:44.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:44.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:44.491+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:44.486+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:44.492+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:44.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:53:51.458+0000] {processor.py:161} INFO - Started process (PID=9248) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:51.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:51.461+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:51.461+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:51.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:51.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:51.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:51.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:53:58.996+0000] {processor.py:161} INFO - Started process (PID=9331) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:58.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:53:58.999+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:58.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:59.044+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:53:59.037+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:53:59.046+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:53:59.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T16:54:06.983+0000] {processor.py:161} INFO - Started process (PID=9420) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:06.985+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:06.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:06.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:07.029+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:07.022+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:07.030+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:07.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:54:14.003+0000] {processor.py:161} INFO - Started process (PID=9503) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:14.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:14.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:14.005+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:14.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:14.040+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:14.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:14.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:54:21.140+0000] {processor.py:161} INFO - Started process (PID=9586) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:21.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:21.142+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:21.142+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:21.184+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:21.176+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:21.186+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:21.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:54:28.428+0000] {processor.py:161} INFO - Started process (PID=9669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:28.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:28.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:28.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:28.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:28.465+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:28.474+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:28.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:54:36.542+0000] {processor.py:161} INFO - Started process (PID=9759) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:36.544+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:36.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:36.545+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:36.594+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:36.586+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:36.596+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:36.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T16:54:43.986+0000] {processor.py:161} INFO - Started process (PID=9842) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:43.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:43.989+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:43.989+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:44.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:44.022+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:44.031+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:44.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T16:54:50.367+0000] {processor.py:161} INFO - Started process (PID=9925) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:50.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:50.370+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:50.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:50.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:50.408+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:50.415+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:50.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:54:57.000+0000] {processor.py:161} INFO - Started process (PID=10008) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:57.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:54:57.002+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:57.002+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:57.046+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:54:57.039+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:54:57.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:54:57.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:55:03.565+0000] {processor.py:161} INFO - Started process (PID=10091) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:03.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:03.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:03.568+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:03.610+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:03.603+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:03.611+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:03.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:55:11.396+0000] {processor.py:161} INFO - Started process (PID=10180) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:11.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:11.399+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:11.399+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:11.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:11.435+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:11.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:11.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:55:20.187+0000] {processor.py:161} INFO - Started process (PID=10263) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:20.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:20.190+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:20.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:20.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:20.229+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:20.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:20.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:55:27.447+0000] {processor.py:161} INFO - Started process (PID=10347) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:27.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:27.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:27.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:27.492+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:27.485+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:27.493+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:27.509+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:55:34.059+0000] {processor.py:161} INFO - Started process (PID=10430) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:34.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:34.062+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:34.062+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:34.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:34.102+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:34.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:34.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:55:40.841+0000] {processor.py:161} INFO - Started process (PID=10519) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:40.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:40.844+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:40.844+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:40.888+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:40.881+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:40.889+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:40.903+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T16:55:47.923+0000] {processor.py:161} INFO - Started process (PID=10602) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:47.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:47.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:47.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:47.974+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:47.965+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:47.975+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:47.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T16:55:55.056+0000] {processor.py:161} INFO - Started process (PID=10685) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:55.057+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:55:55.059+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:55.058+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:55.107+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:55:55.100+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:55:55.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:55:55.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:56:01.535+0000] {processor.py:161} INFO - Started process (PID=10768) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:01.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:01.538+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:01.537+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:01.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:01.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:01.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:01.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:56:08.976+0000] {processor.py:161} INFO - Started process (PID=10857) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:08.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:08.978+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:08.978+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:09.021+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:09.014+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:09.023+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:09.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:56:15.661+0000] {processor.py:161} INFO - Started process (PID=10940) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:15.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:15.664+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:15.664+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:15.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:15.703+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:15.712+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:15.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:56:24.926+0000] {processor.py:161} INFO - Started process (PID=11023) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:24.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:24.929+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:24.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:24.993+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:24.984+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:24.994+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:25.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T16:56:32.289+0000] {processor.py:161} INFO - Started process (PID=11106) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:32.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:32.292+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:32.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:32.329+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:32.323+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:32.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:32.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T16:56:39.174+0000] {processor.py:161} INFO - Started process (PID=11195) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:39.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:39.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:39.176+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:39.218+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:39.211+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:39.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:39.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T16:56:46.006+0000] {processor.py:161} INFO - Started process (PID=11278) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:46.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:46.009+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:46.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:46.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:46.048+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:46.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:46.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:56:52.864+0000] {processor.py:161} INFO - Started process (PID=11361) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:52.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:56:52.867+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:52.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:52.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:56:52.905+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:56:52.913+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:56:52.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:57:02.608+0000] {processor.py:161} INFO - Started process (PID=11444) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:02.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:02.613+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:02.612+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:02.663+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:02.655+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:02.664+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:02.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T16:57:09.776+0000] {processor.py:161} INFO - Started process (PID=11534) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:09.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:09.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:09.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:09.824+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:09.816+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:09.825+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:09.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:57:16.274+0000] {processor.py:161} INFO - Started process (PID=11617) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:16.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:16.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:16.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:16.320+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:16.312+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:16.321+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:16.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:57:24.542+0000] {processor.py:161} INFO - Started process (PID=11700) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:24.544+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:24.545+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:24.545+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:24.583+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:24.577+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:24.584+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:24.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:57:32.900+0000] {processor.py:161} INFO - Started process (PID=11784) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:32.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:32.903+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:32.903+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:32.950+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:32.942+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:32.951+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:32.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:57:40.547+0000] {processor.py:161} INFO - Started process (PID=11873) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:40.549+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:40.550+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:40.550+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:40.598+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:40.591+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:40.600+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:40.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:57:47.355+0000] {processor.py:161} INFO - Started process (PID=11956) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:47.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:47.358+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:47.358+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:47.397+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:47.391+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:47.398+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:47.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T16:57:53.986+0000] {processor.py:161} INFO - Started process (PID=12040) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:53.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:57:53.989+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:53.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:54.028+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:57:54.021+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:57:54.029+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:57:54.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T16:58:01.009+0000] {processor.py:161} INFO - Started process (PID=12123) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:01.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:01.012+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:01.011+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:01.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:01.047+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:01.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:01.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T16:58:09.318+0000] {processor.py:161} INFO - Started process (PID=12212) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:09.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:09.326+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:09.325+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:09.397+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:09.387+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:09.399+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:09.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.125 seconds
[2024-05-10T16:58:16.828+0000] {processor.py:161} INFO - Started process (PID=12295) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:16.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:16.831+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:16.831+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:16.874+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:16.868+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:16.875+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:16.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:58:24.470+0000] {processor.py:161} INFO - Started process (PID=12378) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:24.471+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:24.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:24.472+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:24.518+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:24.512+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:24.519+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:24.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:58:32.697+0000] {processor.py:161} INFO - Started process (PID=12461) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:32.699+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:32.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:32.700+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:32.748+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:32.741+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:32.749+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:32.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:58:40.904+0000] {processor.py:161} INFO - Started process (PID=12550) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:40.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:40.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:40.907+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:40.954+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:40.947+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:40.956+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:40.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T16:58:49.764+0000] {processor.py:161} INFO - Started process (PID=12633) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:49.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:49.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:49.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:49.828+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:49.821+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:49.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:49.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T16:58:57.078+0000] {processor.py:161} INFO - Started process (PID=12716) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:57.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:58:57.081+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:57.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:57.129+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:58:57.121+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:58:57.130+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:58:57.144+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T16:59:04.399+0000] {processor.py:161} INFO - Started process (PID=12799) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:04.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:04.402+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:04.402+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:04.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:04.444+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:04.453+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:04.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:59:11.497+0000] {processor.py:161} INFO - Started process (PID=12888) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:11.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:11.500+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:11.500+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:11.546+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:11.539+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:11.547+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:11.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T16:59:19.960+0000] {processor.py:161} INFO - Started process (PID=12971) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:19.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:19.963+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:19.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:20.008+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:20.002+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:20.009+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:20.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:59:29.337+0000] {processor.py:161} INFO - Started process (PID=13054) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:29.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:29.340+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:29.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:29.392+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:29.384+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:29.393+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:29.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T16:59:36.476+0000] {processor.py:161} INFO - Started process (PID=13143) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:36.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:36.479+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:36.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:36.524+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:36.517+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:36.525+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:36.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T16:59:42.991+0000] {processor.py:161} INFO - Started process (PID=13226) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:42.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:42.994+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:42.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:43.038+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:43.031+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:43.039+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:43.052+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T16:59:49.430+0000] {processor.py:161} INFO - Started process (PID=13309) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:49.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:49.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:49.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:49.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:49.471+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:49.478+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:49.494+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T16:59:56.528+0000] {processor.py:161} INFO - Started process (PID=13392) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:56.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T16:59:56.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:56.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:56.650+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:59:56.644+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:59:56.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T16:59:56.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.145 seconds
[2024-05-10T17:00:06.461+0000] {processor.py:161} INFO - Started process (PID=13481) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:06.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:06.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:06.463+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:06.500+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:06.494+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:06.501+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:06.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.060 seconds
[2024-05-10T17:00:13.201+0000] {processor.py:161} INFO - Started process (PID=13565) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:13.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:13.203+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:13.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:13.242+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:13.236+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:13.243+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:13.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:00:20.314+0000] {processor.py:161} INFO - Started process (PID=13648) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:20.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:20.317+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:20.316+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:20.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:20.352+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:20.360+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:20.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:00:27.554+0000] {processor.py:161} INFO - Started process (PID=13731) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:27.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:27.557+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:27.557+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:27.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:27.597+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:27.607+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:27.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:00:34.213+0000] {processor.py:161} INFO - Started process (PID=13814) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:34.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:34.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:34.216+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:34.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:34.253+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:34.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:34.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:00:41.265+0000] {processor.py:161} INFO - Started process (PID=13903) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:41.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:41.268+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:41.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:41.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:41.300+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:41.307+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:41.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:00:47.585+0000] {processor.py:161} INFO - Started process (PID=13986) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:47.587+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:47.588+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:47.588+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:47.631+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:47.624+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:47.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:47.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:00:54.389+0000] {processor.py:161} INFO - Started process (PID=14069) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:54.390+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:00:54.392+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:54.391+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:54.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:00:54.424+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:00:54.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:00:54.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T17:01:01.329+0000] {processor.py:161} INFO - Started process (PID=14152) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:01.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:01.332+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:01.331+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:01.374+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:01.367+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:01.376+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:01.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:01:08.383+0000] {processor.py:161} INFO - Started process (PID=14241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:08.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:08.386+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:08.386+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:08.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:08.425+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:08.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:08.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:01:16.301+0000] {processor.py:161} INFO - Started process (PID=14324) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:16.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:16.310+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:16.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:16.354+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:16.348+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:16.355+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:16.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T17:01:24.434+0000] {processor.py:161} INFO - Started process (PID=14407) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:24.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:24.437+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:24.437+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:24.486+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:24.479+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:24.487+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:24.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:01:31.920+0000] {processor.py:161} INFO - Started process (PID=14490) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:31.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:31.923+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:31.922+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:31.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:31.958+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:31.965+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:31.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:01:39.765+0000] {processor.py:161} INFO - Started process (PID=14579) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:39.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:39.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:39.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:39.809+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:39.802+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:39.810+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:39.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:01:48.014+0000] {processor.py:161} INFO - Started process (PID=14662) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:48.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:48.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:48.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:48.058+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:48.051+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:48.059+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:48.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:01:55.597+0000] {processor.py:161} INFO - Started process (PID=14745) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:55.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:01:55.600+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:55.599+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:55.644+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:01:55.637+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:01:55.646+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:01:55.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:02:05.890+0000] {processor.py:161} INFO - Started process (PID=14834) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:05.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:05.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:05.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:05.936+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:05.928+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:05.937+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:05.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:02:15.665+0000] {processor.py:161} INFO - Started process (PID=14918) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:15.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:15.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:15.667+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:15.708+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:15.701+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:15.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:15.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T17:02:26.503+0000] {processor.py:161} INFO - Started process (PID=15001) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:26.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:26.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:26.505+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:26.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:26.540+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:26.548+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:26.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:02:36.567+0000] {processor.py:161} INFO - Started process (PID=15090) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:36.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:36.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:36.570+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:36.620+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:36.612+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:36.622+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:36.635+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:02:48.228+0000] {processor.py:161} INFO - Started process (PID=15173) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:48.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:48.231+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:48.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:48.277+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:48.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:48.278+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:48.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:02:54.663+0000] {processor.py:161} INFO - Started process (PID=15256) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:54.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:02:54.666+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:54.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:54.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:02:54.705+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:02:54.713+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:02:54.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:03:01.967+0000] {processor.py:161} INFO - Started process (PID=15339) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:01.968+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:01.970+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:01.969+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:02.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:02.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:02.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:02.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:03:09.013+0000] {processor.py:161} INFO - Started process (PID=15428) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:09.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:09.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:09.015+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:09.058+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:09.051+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:09.059+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:09.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:03:15.565+0000] {processor.py:161} INFO - Started process (PID=15511) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:15.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:15.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:15.568+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:15.610+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:15.603+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:15.611+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:15.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:03:25.347+0000] {processor.py:161} INFO - Started process (PID=15594) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:25.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:25.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:25.349+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:25.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:25.388+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:25.396+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:25.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:03:33.387+0000] {processor.py:161} INFO - Started process (PID=15677) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:33.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:33.390+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:33.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:33.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:33.434+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:33.444+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:33.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:03:39.886+0000] {processor.py:161} INFO - Started process (PID=15766) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:39.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:39.888+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:39.888+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:39.932+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:39.924+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:39.933+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:39.947+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:03:46.194+0000] {processor.py:161} INFO - Started process (PID=15849) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:46.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:46.196+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:46.196+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:46.237+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:46.231+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:46.239+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:46.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T17:03:53.877+0000] {processor.py:161} INFO - Started process (PID=15932) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:53.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:03:53.881+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:53.881+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:53.932+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:03:53.924+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:03:53.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:03:53.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T17:04:01.472+0000] {processor.py:161} INFO - Started process (PID=16016) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:01.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:01.475+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:01.474+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:01.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:01.512+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:01.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:01.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:04:09.069+0000] {processor.py:161} INFO - Started process (PID=16105) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:09.071+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:09.072+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:09.072+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:09.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:09.112+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:09.119+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:09.133+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:04:16.186+0000] {processor.py:161} INFO - Started process (PID=16188) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:16.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:16.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:16.189+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:16.226+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:16.220+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:16.228+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:16.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T17:04:23.582+0000] {processor.py:161} INFO - Started process (PID=16271) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:23.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:23.596+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:23.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:23.639+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:23.632+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:23.640+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:23.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T17:04:33.236+0000] {processor.py:161} INFO - Started process (PID=16354) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:33.238+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:33.240+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:33.239+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:33.288+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:33.281+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:33.289+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:33.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T17:04:40.224+0000] {processor.py:161} INFO - Started process (PID=16443) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:40.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:40.227+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:40.227+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:40.279+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:40.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:40.281+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:40.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:04:47.354+0000] {processor.py:161} INFO - Started process (PID=16526) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:47.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:47.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:47.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:47.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:47.396+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:47.404+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:47.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:04:53.754+0000] {processor.py:161} INFO - Started process (PID=16609) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:53.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:04:53.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:53.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:53.794+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:04:53.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:04:53.796+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:04:53.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.063 seconds
[2024-05-10T17:05:00.637+0000] {processor.py:161} INFO - Started process (PID=16692) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:00.654+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:00.675+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:00.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:00.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:00.774+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:00.783+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:00.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.174 seconds
[2024-05-10T17:05:08.462+0000] {processor.py:161} INFO - Started process (PID=16781) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:08.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:08.465+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:08.464+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:08.507+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:08.500+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:08.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:08.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:05:14.976+0000] {processor.py:161} INFO - Started process (PID=16864) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:14.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:14.979+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:14.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:15.023+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:15.016+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:15.024+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:15.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:05:23.407+0000] {processor.py:161} INFO - Started process (PID=16947) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:23.409+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:23.411+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:23.410+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:23.455+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:23.449+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:23.456+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:23.470+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:05:30.678+0000] {processor.py:161} INFO - Started process (PID=17030) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:30.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:30.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:30.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:30.747+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:30.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:30.786+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:30.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.179 seconds
[2024-05-10T17:05:38.710+0000] {processor.py:161} INFO - Started process (PID=17120) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:38.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:38.713+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:38.713+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:38.763+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:38.755+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:38.765+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:38.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:05:45.211+0000] {processor.py:161} INFO - Started process (PID=17203) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:45.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:45.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:45.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:45.255+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:45.250+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:45.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:45.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:05:53.466+0000] {processor.py:161} INFO - Started process (PID=17286) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:53.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:05:53.469+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:53.468+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:53.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:05:53.502+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:05:53.510+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:05:53.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:06:00.877+0000] {processor.py:161} INFO - Started process (PID=17369) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:00.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:00.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:00.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:00.925+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:00.917+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:00.926+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:00.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:06:09.049+0000] {processor.py:161} INFO - Started process (PID=17458) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:09.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:09.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:09.051+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:09.094+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:09.087+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:09.095+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:09.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:06:16.028+0000] {processor.py:161} INFO - Started process (PID=17541) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:16.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:16.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:16.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:16.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:16.068+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:16.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:16.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:06:22.944+0000] {processor.py:161} INFO - Started process (PID=17624) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:22.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:22.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:22.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:22.992+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:22.986+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:22.994+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:23.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:06:30.320+0000] {processor.py:161} INFO - Started process (PID=17707) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:30.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:30.322+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:30.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:30.363+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:30.357+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:30.364+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:30.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T17:06:37.280+0000] {processor.py:161} INFO - Started process (PID=17796) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:37.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:37.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:37.282+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:37.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:37.317+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:37.325+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:37.338+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:06:44.088+0000] {processor.py:161} INFO - Started process (PID=17879) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:44.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:44.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:44.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:44.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:44.126+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:44.134+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:44.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:06:50.902+0000] {processor.py:161} INFO - Started process (PID=17962) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:50.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:50.905+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:50.904+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:50.952+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:50.944+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:50.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:50.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:06:58.425+0000] {processor.py:161} INFO - Started process (PID=18045) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:58.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:06:58.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:58.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:58.486+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:06:58.479+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:06:58.487+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:06:58.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T17:07:06.611+0000] {processor.py:161} INFO - Started process (PID=18134) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:06.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:06.613+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:06.613+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:06.649+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:06.643+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:06.650+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:06.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.059 seconds
[2024-05-10T17:07:13.168+0000] {processor.py:161} INFO - Started process (PID=18218) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:13.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:13.171+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:13.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:13.211+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:13.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:13.212+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:13.226+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T17:07:20.333+0000] {processor.py:161} INFO - Started process (PID=18301) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:20.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:20.338+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:20.336+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:20.384+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:20.377+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:20.385+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:20.401+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:07:27.887+0000] {processor.py:161} INFO - Started process (PID=18384) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:27.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:27.889+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:27.889+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:27.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:27.923+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:27.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:27.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:07:34.708+0000] {processor.py:161} INFO - Started process (PID=18467) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:34.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:34.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:34.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:34.762+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:34.754+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:34.763+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:34.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:07:41.892+0000] {processor.py:161} INFO - Started process (PID=18556) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:41.894+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:41.895+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:41.895+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:41.939+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:41.931+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:41.941+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:41.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:07:48.546+0000] {processor.py:161} INFO - Started process (PID=18639) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:48.547+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:48.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:48.548+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:48.588+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:48.582+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:48.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:48.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T17:07:56.505+0000] {processor.py:161} INFO - Started process (PID=18722) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:56.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:07:56.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:56.507+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:56.550+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:07:56.544+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:07:56.552+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:07:56.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:08:04.863+0000] {processor.py:161} INFO - Started process (PID=18806) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:04.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:04.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:04.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:04.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:04.903+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:04.912+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:04.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:08:11.452+0000] {processor.py:161} INFO - Started process (PID=18895) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:11.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:11.454+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:11.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:11.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:11.493+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:11.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:11.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:08:18.882+0000] {processor.py:161} INFO - Started process (PID=18978) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:18.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:18.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:18.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:18.927+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:18.921+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:18.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:18.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:08:26.707+0000] {processor.py:161} INFO - Started process (PID=19062) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:26.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:26.711+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:26.710+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:26.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:26.750+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:26.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:26.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:08:33.908+0000] {processor.py:161} INFO - Started process (PID=19145) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:33.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:33.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:33.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:33.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:33.949+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:33.957+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:33.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:08:40.477+0000] {processor.py:161} INFO - Started process (PID=19234) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:40.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:40.480+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:40.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:40.521+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:40.515+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:40.522+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:40.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:08:47.694+0000] {processor.py:161} INFO - Started process (PID=19317) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:47.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:47.696+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:47.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:47.739+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:47.733+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:47.740+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:47.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:08:54.083+0000] {processor.py:161} INFO - Started process (PID=19400) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:54.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:08:54.086+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:54.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:54.127+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:08:54.121+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:08:54.129+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:08:54.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:09:02.657+0000] {processor.py:161} INFO - Started process (PID=19483) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:02.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:02.662+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:02.661+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:02.719+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:02.711+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:02.722+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:02.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T17:09:09.762+0000] {processor.py:161} INFO - Started process (PID=19572) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:09.763+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:09.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:09.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:09.811+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:09.802+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:09.812+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:09.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:09:16.255+0000] {processor.py:161} INFO - Started process (PID=19655) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:16.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:16.258+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:16.258+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:16.296+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:16.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:16.297+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:16.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:09:25.292+0000] {processor.py:161} INFO - Started process (PID=19738) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:25.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:25.295+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:25.295+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:25.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:25.333+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:25.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:25.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:09:32.995+0000] {processor.py:161} INFO - Started process (PID=19822) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:32.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:32.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:32.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:33.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:33.034+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:33.042+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:33.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:09:40.642+0000] {processor.py:161} INFO - Started process (PID=19912) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:40.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:40.645+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:40.644+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:40.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:40.676+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:40.686+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:40.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:09:48.276+0000] {processor.py:161} INFO - Started process (PID=19996) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:48.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:48.278+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:48.278+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:48.326+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:48.318+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:48.327+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:48.340+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:09:56.402+0000] {processor.py:161} INFO - Started process (PID=20079) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:56.404+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:09:56.406+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:56.405+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:56.456+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:09:56.450+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:09:56.457+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:09:56.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:10:06.373+0000] {processor.py:161} INFO - Started process (PID=20162) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:06.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:06.376+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:06.376+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:06.420+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:06.412+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:06.421+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:06.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:10:13.857+0000] {processor.py:161} INFO - Started process (PID=20251) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:13.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:13.861+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:13.860+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:13.911+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:13.903+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:13.912+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:13.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:10:22.781+0000] {processor.py:161} INFO - Started process (PID=20334) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:22.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:22.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:22.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:22.834+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:22.827+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:22.836+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:22.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:10:31.980+0000] {processor.py:161} INFO - Started process (PID=20417) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:31.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:31.983+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:31.983+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:32.033+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:32.025+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:32.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:32.052+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:10:42.460+0000] {processor.py:161} INFO - Started process (PID=20507) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:42.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:42.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:42.462+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:42.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:42.499+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:42.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:42.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:10:51.622+0000] {processor.py:161} INFO - Started process (PID=20591) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:51.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:10:51.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:51.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:51.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:10:51.676+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:10:51.686+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:10:51.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T17:11:00.429+0000] {processor.py:161} INFO - Started process (PID=20674) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:00.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:00.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:00.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:00.496+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:00.490+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:00.498+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:00.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T17:11:08.563+0000] {processor.py:161} INFO - Started process (PID=20764) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:08.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:08.566+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:08.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:08.612+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:08.605+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:08.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:08.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:11:15.655+0000] {processor.py:161} INFO - Started process (PID=20847) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:15.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:15.658+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:15.657+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:15.702+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:15.695+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:15.703+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:15.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:11:22.694+0000] {processor.py:161} INFO - Started process (PID=20930) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:22.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:22.697+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:22.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:22.738+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:22.732+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:22.740+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:22.755+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:11:30.015+0000] {processor.py:161} INFO - Started process (PID=21013) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:30.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:30.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:30.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:30.060+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:30.054+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:30.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:30.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:11:36.788+0000] {processor.py:161} INFO - Started process (PID=21096) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:36.790+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:36.791+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:36.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:36.836+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:36.829+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:36.837+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:36.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:11:43.800+0000] {processor.py:161} INFO - Started process (PID=21185) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:43.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:43.803+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:43.803+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:43.852+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:43.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:43.853+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:43.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:11:50.806+0000] {processor.py:161} INFO - Started process (PID=21268) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:50.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:50.809+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:50.809+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:50.851+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:50.845+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:50.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:50.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:11:57.253+0000] {processor.py:161} INFO - Started process (PID=21351) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:57.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:11:57.257+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:57.256+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:57.301+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:11:57.295+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:11:57.302+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:11:57.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:12:06.044+0000] {processor.py:161} INFO - Started process (PID=21434) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:06.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:06.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:06.046+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:06.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:06.082+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:06.090+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:06.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:12:12.336+0000] {processor.py:161} INFO - Started process (PID=21523) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:12.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:12.339+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:12.338+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:12.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:12.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:12.381+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:12.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:12:20.906+0000] {processor.py:161} INFO - Started process (PID=21606) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:20.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:20.909+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:20.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:20.955+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:20.947+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:20.956+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:20.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:12:28.296+0000] {processor.py:161} INFO - Started process (PID=21689) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:28.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:28.299+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:28.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:28.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:28.335+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:28.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:28.355+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.067 seconds
[2024-05-10T17:12:35.942+0000] {processor.py:161} INFO - Started process (PID=21772) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:35.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:35.946+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:35.945+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:35.991+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:35.983+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:35.992+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:36.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:12:43.155+0000] {processor.py:161} INFO - Started process (PID=21861) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:43.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:43.158+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:43.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:43.198+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:43.192+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:43.199+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:43.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:12:50.183+0000] {processor.py:161} INFO - Started process (PID=21944) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:50.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:50.186+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:50.186+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:50.230+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:50.223+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:50.232+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:50.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:12:57.889+0000] {processor.py:161} INFO - Started process (PID=22027) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:57.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:12:57.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:57.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:57.941+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:12:57.934+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:12:57.942+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:12:57.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:13:05.968+0000] {processor.py:161} INFO - Started process (PID=22110) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:05.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:05.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:05.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:06.023+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:06.016+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:06.025+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:06.043+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:13:13.279+0000] {processor.py:161} INFO - Started process (PID=22200) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:13.280+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:13.282+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:13.281+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:13.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:13.321+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:13.331+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:13.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:13:21.162+0000] {processor.py:161} INFO - Started process (PID=22283) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:21.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:21.166+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:21.165+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:21.220+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:21.213+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:21.222+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:21.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T17:13:30.202+0000] {processor.py:161} INFO - Started process (PID=22366) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:30.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:30.205+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:30.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:30.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:30.247+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:30.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:30.272+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:13:37.969+0000] {processor.py:161} INFO - Started process (PID=22455) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:37.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:37.972+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:37.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:38.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:38.012+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:38.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:38.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:13:45.936+0000] {processor.py:161} INFO - Started process (PID=22538) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:45.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:45.939+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:45.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:45.983+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:45.976+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:45.985+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:46.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:13:54.186+0000] {processor.py:161} INFO - Started process (PID=22621) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:54.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:13:54.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:54.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:54.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:13:54.225+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:13:54.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:13:54.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:14:01.789+0000] {processor.py:161} INFO - Started process (PID=22705) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:01.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:01.795+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:01.794+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:01.850+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:01.843+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:01.852+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:01.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T17:14:12.515+0000] {processor.py:161} INFO - Started process (PID=22794) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:12.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:12.518+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:12.517+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:12.566+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:12.559+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:12.568+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:12.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T17:14:22.820+0000] {processor.py:161} INFO - Started process (PID=22878) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:22.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:22.823+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:22.823+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:22.875+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:22.868+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:22.876+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:22.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:14:31.017+0000] {processor.py:161} INFO - Started process (PID=22961) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:31.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:31.021+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:31.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:31.070+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:31.063+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:31.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:31.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:14:39.663+0000] {processor.py:161} INFO - Started process (PID=23050) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:39.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:39.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:39.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:39.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:39.715+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:39.727+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:39.750+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.103 seconds
[2024-05-10T17:14:49.995+0000] {processor.py:161} INFO - Started process (PID=23133) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:49.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:49.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:49.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:50.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:50.046+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:50.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:50.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T17:14:58.518+0000] {processor.py:161} INFO - Started process (PID=23217) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:58.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:14:58.521+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:58.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:58.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:14:58.562+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:14:58.571+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:14:58.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:15:06.532+0000] {processor.py:161} INFO - Started process (PID=23300) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:06.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:06.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:06.535+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:06.578+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:06.571+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:06.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:06.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:15:14.258+0000] {processor.py:161} INFO - Started process (PID=23389) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:14.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:14.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:14.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:14.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:14.318+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:14.331+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:14.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T17:15:26.882+0000] {processor.py:161} INFO - Started process (PID=23472) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:26.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:26.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:26.884+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:26.939+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:26.932+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:26.941+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:26.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T17:15:36.001+0000] {processor.py:161} INFO - Started process (PID=23556) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:36.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:36.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:36.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:36.054+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:36.047+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:36.056+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:36.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T17:15:44.324+0000] {processor.py:161} INFO - Started process (PID=23645) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:44.325+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:44.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:44.326+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:44.367+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:44.360+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:44.368+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:44.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:15:52.555+0000] {processor.py:161} INFO - Started process (PID=23729) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:52.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:15:52.558+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:52.558+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:52.628+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:15:52.618+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:15:52.630+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:15:52.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T17:16:02.304+0000] {processor.py:161} INFO - Started process (PID=23812) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:02.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:02.308+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:02.307+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:02.375+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:02.366+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:02.377+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:02.397+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T17:16:13.112+0000] {processor.py:161} INFO - Started process (PID=23902) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:13.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:13.116+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:13.115+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:13.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:13.157+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:13.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:13.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:16:21.678+0000] {processor.py:161} INFO - Started process (PID=23985) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:21.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:21.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:21.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:21.725+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:21.719+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:21.727+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:21.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:16:31.136+0000] {processor.py:161} INFO - Started process (PID=24068) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:31.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:31.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:31.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:31.193+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:31.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:31.195+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:31.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:16:40.903+0000] {processor.py:161} INFO - Started process (PID=24158) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:40.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:40.907+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:40.906+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:40.962+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:40.954+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:40.964+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:40.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T17:16:51.014+0000] {processor.py:161} INFO - Started process (PID=24241) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:51.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:16:51.018+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:51.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:51.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:16:51.064+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:16:51.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:16:51.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T17:17:00.700+0000] {processor.py:161} INFO - Started process (PID=24324) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:00.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:17:00.703+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:00.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:00.746+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:00.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:17:00.747+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:00.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:17:12.267+0000] {processor.py:161} INFO - Started process (PID=24413) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:12.268+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:17:12.270+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:12.269+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:12.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:12.317+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:17:12.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:12.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:17:26.722+0000] {processor.py:161} INFO - Started process (PID=24497) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:26.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:17:26.725+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:26.724+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:26.771+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:26.764+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:17:26.772+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:26.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:17:37.046+0000] {processor.py:161} INFO - Started process (PID=24580) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:37.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:17:37.050+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:37.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:37.109+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:37.099+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:17:37.111+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:37.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T17:17:49.037+0000] {processor.py:161} INFO - Started process (PID=24669) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:49.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:17:49.122+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:49.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:49.204+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:17:49.197+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:17:49.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:17:49.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.196 seconds
[2024-05-10T17:18:02.071+0000] {processor.py:161} INFO - Started process (PID=24752) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:02.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:02.076+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:02.075+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:02.129+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:02.121+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:02.130+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:02.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T17:18:15.377+0000] {processor.py:161} INFO - Started process (PID=24841) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:15.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:15.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:15.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:15.429+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:15.423+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:15.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:15.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T17:18:27.747+0000] {processor.py:161} INFO - Started process (PID=24925) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:27.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:27.751+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:27.750+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:27.812+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:27.803+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:27.813+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:27.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T17:18:37.282+0000] {processor.py:161} INFO - Started process (PID=25009) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:37.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:37.285+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:37.285+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:37.331+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:37.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:37.333+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:37.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:18:45.427+0000] {processor.py:161} INFO - Started process (PID=25098) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:45.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:45.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:45.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:45.476+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:45.469+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:45.478+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:45.492+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:18:53.960+0000] {processor.py:161} INFO - Started process (PID=25181) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:53.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:18:53.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:53.964+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:54.015+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:18:54.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:18:54.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:18:54.040+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T17:19:04.514+0000] {processor.py:161} INFO - Started process (PID=25265) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:04.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:04.518+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:04.517+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:04.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:04.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:04.581+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:04.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T17:19:12.415+0000] {processor.py:161} INFO - Started process (PID=25354) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:12.416+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:12.418+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:12.417+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:12.461+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:12.454+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:12.462+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:12.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:19:20.821+0000] {processor.py:161} INFO - Started process (PID=25437) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:20.822+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:20.824+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:20.823+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:20.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:20.862+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:20.871+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:20.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:19:29.649+0000] {processor.py:161} INFO - Started process (PID=25520) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:29.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:29.653+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:29.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:29.703+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:29.696+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:29.705+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:29.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:19:38.336+0000] {processor.py:161} INFO - Started process (PID=25609) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:38.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:38.339+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:38.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:38.397+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:38.389+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:38.399+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:38.419+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T17:19:46.769+0000] {processor.py:161} INFO - Started process (PID=25692) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:46.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:46.772+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:46.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:46.822+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:46.814+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:46.824+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:46.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:19:54.842+0000] {processor.py:161} INFO - Started process (PID=25775) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:54.844+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:19:54.847+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:54.846+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:54.895+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:19:54.888+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:19:54.896+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:19:54.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:20:02.812+0000] {processor.py:161} INFO - Started process (PID=25859) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:02.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:02.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:02.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:02.870+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:02.862+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:02.871+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:02.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T17:20:13.541+0000] {processor.py:161} INFO - Started process (PID=25948) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:13.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:13.545+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:13.545+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:13.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:13.604+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:13.618+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:13.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.117 seconds
[2024-05-10T17:20:24.266+0000] {processor.py:161} INFO - Started process (PID=26031) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:24.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:24.273+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:24.272+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:24.328+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:24.322+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:24.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:24.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T17:20:32.206+0000] {processor.py:161} INFO - Started process (PID=26114) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:32.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:32.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:32.208+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:32.257+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:32.249+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:32.258+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:32.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:20:41.098+0000] {processor.py:161} INFO - Started process (PID=26203) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:41.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:41.102+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:41.101+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:41.167+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:41.150+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:41.169+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:41.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.108 seconds
[2024-05-10T17:20:51.170+0000] {processor.py:161} INFO - Started process (PID=26287) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:51.172+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:51.174+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:51.173+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:51.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:51.220+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:51.230+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:51.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T17:20:59.454+0000] {processor.py:161} INFO - Started process (PID=26370) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:59.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:20:59.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:59.456+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:59.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:20:59.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:20:59.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:20:59.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:21:08.562+0000] {processor.py:161} INFO - Started process (PID=26460) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:08.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:08.565+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:08.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:08.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:08.608+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:08.616+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:08.629+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:21:16.242+0000] {processor.py:161} INFO - Started process (PID=26544) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:16.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:16.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:16.245+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:16.312+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:16.301+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:16.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:16.344+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.113 seconds
[2024-05-10T17:21:28.611+0000] {processor.py:161} INFO - Started process (PID=26627) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:28.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:28.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:28.615+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:28.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:28.662+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:28.670+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:28.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T17:21:37.447+0000] {processor.py:161} INFO - Started process (PID=26710) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:37.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:37.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:37.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:37.501+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:37.493+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:37.502+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:37.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:21:46.158+0000] {processor.py:161} INFO - Started process (PID=26800) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:46.160+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:46.164+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:46.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:46.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:46.203+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:46.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:46.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T17:21:54.028+0000] {processor.py:161} INFO - Started process (PID=26883) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:54.030+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:21:54.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:54.031+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:54.077+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:21:54.070+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:21:54.078+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:21:54.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:22:03.136+0000] {processor.py:161} INFO - Started process (PID=26966) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:03.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:03.143+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:03.142+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:03.225+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:03.216+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:03.227+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:03.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.135 seconds
[2024-05-10T17:22:14.150+0000] {processor.py:161} INFO - Started process (PID=27056) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:14.153+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:14.155+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:14.154+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:14.213+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:14.205+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:14.217+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:14.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T17:22:22.846+0000] {processor.py:161} INFO - Started process (PID=27139) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:22.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:22.849+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:22.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:22.895+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:22.887+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:22.897+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:22.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:22:31.390+0000] {processor.py:161} INFO - Started process (PID=27223) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:31.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:31.394+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:31.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:31.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:31.433+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:31.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:31.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T17:22:40.589+0000] {processor.py:161} INFO - Started process (PID=27312) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:40.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:40.593+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:40.592+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:40.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:40.633+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:40.641+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:40.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:22:50.867+0000] {processor.py:161} INFO - Started process (PID=27395) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:50.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:50.869+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:50.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:50.912+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:50.906+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:50.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:50.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:22:58.106+0000] {processor.py:161} INFO - Started process (PID=27479) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:58.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:22:58.109+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:58.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:58.159+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:22:58.152+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:22:58.161+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:22:58.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.082 seconds
[2024-05-10T17:23:06.559+0000] {processor.py:161} INFO - Started process (PID=27562) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:06.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:06.563+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:06.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:06.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:06.600+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:06.609+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:06.623+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:23:14.961+0000] {processor.py:161} INFO - Started process (PID=27651) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:14.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:14.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:14.963+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:15.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:15.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:15.017+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:15.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T17:23:27.618+0000] {processor.py:161} INFO - Started process (PID=27734) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:27.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:27.622+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:27.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:27.672+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:27.662+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:27.674+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:27.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:23:36.223+0000] {processor.py:161} INFO - Started process (PID=27818) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:36.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:36.226+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:36.225+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:36.268+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:36.261+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:36.270+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:36.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:23:43.843+0000] {processor.py:161} INFO - Started process (PID=27907) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:43.844+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:43.846+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:43.846+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:43.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:43.894+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:43.903+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:43.920+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.089 seconds
[2024-05-10T17:23:52.747+0000] {processor.py:161} INFO - Started process (PID=27990) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:52.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:23:52.751+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:52.750+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:52.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:23:52.796+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:23:52.807+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:23:52.830+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T17:24:01.527+0000] {processor.py:161} INFO - Started process (PID=28073) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:01.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:01.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:01.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:01.576+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:01.568+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:01.577+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:01.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:24:10.131+0000] {processor.py:161} INFO - Started process (PID=28162) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:10.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:10.135+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:10.135+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:10.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:10.190+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:10.206+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:10.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.131 seconds
[2024-05-10T17:24:18.749+0000] {processor.py:161} INFO - Started process (PID=28245) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:18.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:18.753+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:18.752+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:18.802+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:18.795+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:18.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:18.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:24:29.528+0000] {processor.py:161} INFO - Started process (PID=28328) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:29.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:29.532+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:29.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:29.589+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:29.580+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:29.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:29.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T17:24:38.837+0000] {processor.py:161} INFO - Started process (PID=28417) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:38.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:38.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:38.840+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:38.890+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:38.881+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:38.892+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:38.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:24:47.072+0000] {processor.py:161} INFO - Started process (PID=28500) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:47.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:47.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:47.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:47.118+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:47.113+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:47.120+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:47.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:24:55.841+0000] {processor.py:161} INFO - Started process (PID=28584) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:55.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:24:55.845+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:55.845+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:55.904+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:24:55.895+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:24:55.905+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:24:55.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.104 seconds
[2024-05-10T17:25:07.024+0000] {processor.py:161} INFO - Started process (PID=28667) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:07.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:07.028+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:07.027+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:07.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:07.076+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:07.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:07.105+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T17:25:15.812+0000] {processor.py:161} INFO - Started process (PID=28756) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:15.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:15.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:15.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:15.860+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:15.853+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:15.861+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:15.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:25:24.779+0000] {processor.py:161} INFO - Started process (PID=28839) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:24.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:24.784+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:24.783+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:24.841+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:24.833+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:24.842+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:24.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.100 seconds
[2024-05-10T17:25:33.942+0000] {processor.py:161} INFO - Started process (PID=28922) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:33.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:33.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:33.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:34.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:33.995+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:34.005+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:34.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.093 seconds
[2024-05-10T17:25:43.501+0000] {processor.py:161} INFO - Started process (PID=29012) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:43.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:43.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:43.504+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:43.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:43.561+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:43.572+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:43.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.109 seconds
[2024-05-10T17:25:52.519+0000] {processor.py:161} INFO - Started process (PID=29095) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:52.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:25:52.522+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:52.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:52.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:25:52.563+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:25:52.572+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:25:52.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:26:00.746+0000] {processor.py:161} INFO - Started process (PID=29179) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:00.747+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:00.749+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:00.748+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:00.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:00.789+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:00.799+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:00.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:26:09.744+0000] {processor.py:161} INFO - Started process (PID=29268) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:09.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:09.748+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:09.747+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:09.806+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:09.798+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:09.808+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:09.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T17:26:19.991+0000] {processor.py:161} INFO - Started process (PID=29352) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:19.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:19.995+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:19.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:20.061+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:20.052+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:20.063+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:20.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T17:26:30.414+0000] {processor.py:161} INFO - Started process (PID=29435) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:30.416+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:30.417+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:30.417+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:30.470+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:30.462+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:30.471+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:30.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.086 seconds
[2024-05-10T17:26:38.882+0000] {processor.py:161} INFO - Started process (PID=29525) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:38.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:38.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:38.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:38.934+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:38.926+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:38.936+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:38.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:26:48.352+0000] {processor.py:161} INFO - Started process (PID=29608) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:48.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:48.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:48.356+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:48.416+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:48.409+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:48.418+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:48.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T17:26:57.701+0000] {processor.py:161} INFO - Started process (PID=29691) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:57.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:26:57.704+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:57.704+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:57.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:26:57.748+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:26:57.757+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:26:57.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.085 seconds
[2024-05-10T17:27:05.923+0000] {processor.py:161} INFO - Started process (PID=29774) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:05.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:05.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:05.926+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:05.983+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:05.975+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:05.985+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:06.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T17:27:14.013+0000] {processor.py:161} INFO - Started process (PID=29863) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:14.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:14.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:14.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:14.065+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:14.058+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:14.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:14.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:27:24.339+0000] {processor.py:161} INFO - Started process (PID=29946) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:24.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:24.344+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:24.343+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:24.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:24.401+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:24.415+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:24.438+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.114 seconds
[2024-05-10T17:27:34.480+0000] {processor.py:161} INFO - Started process (PID=30029) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:34.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:34.483+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:34.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:34.531+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:34.524+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:34.532+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:34.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T17:27:42.506+0000] {processor.py:161} INFO - Started process (PID=30119) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:42.508+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:42.509+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:42.509+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:42.554+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:42.548+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:42.556+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:42.571+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:27:51.413+0000] {processor.py:161} INFO - Started process (PID=30202) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:51.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:51.416+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:51.415+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:51.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:51.450+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:51.458+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:51.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:27:58.811+0000] {processor.py:161} INFO - Started process (PID=30285) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:58.812+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:27:58.814+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:58.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:58.859+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:27:58.852+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:27:58.860+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:27:58.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:28:08.653+0000] {processor.py:161} INFO - Started process (PID=30368) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:08.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:08.657+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:08.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:08.721+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:08.713+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:08.723+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:08.745+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.110 seconds
[2024-05-10T17:28:18.443+0000] {processor.py:161} INFO - Started process (PID=30458) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:18.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:18.446+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:18.446+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:18.494+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:18.487+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:18.495+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:18.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:28:28.255+0000] {processor.py:161} INFO - Started process (PID=30541) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:28.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:28.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:28.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:28.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:28.314+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:28.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:28.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T17:28:37.356+0000] {processor.py:161} INFO - Started process (PID=30624) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:37.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:37.359+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:37.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:37.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:37.404+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:37.413+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:37.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.088 seconds
[2024-05-10T17:28:48.914+0000] {processor.py:161} INFO - Started process (PID=30713) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:48.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:48.919+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:48.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:48.979+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:48.970+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:48.980+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:48.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.096 seconds
[2024-05-10T17:28:58.913+0000] {processor.py:161} INFO - Started process (PID=30797) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:58.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:28:58.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:58.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:58.957+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:28:58.951+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:28:58.959+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:28:58.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.073 seconds
[2024-05-10T17:29:06.814+0000] {processor.py:161} INFO - Started process (PID=30880) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:06.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:06.817+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:06.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:06.860+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:06.853+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:06.861+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:06.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:29:13.766+0000] {processor.py:161} INFO - Started process (PID=30969) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:13.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:13.769+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:13.769+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:13.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:13.809+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:13.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:13.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:29:23.633+0000] {processor.py:161} INFO - Started process (PID=31052) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:23.635+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:23.637+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:23.637+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:23.697+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:23.690+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:23.699+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:23.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.098 seconds
[2024-05-10T17:29:33.139+0000] {processor.py:161} INFO - Started process (PID=31135) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:33.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:33.144+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:33.142+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:33.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:33.181+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:33.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:33.203+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:29:43.039+0000] {processor.py:161} INFO - Started process (PID=31225) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:43.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:43.043+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:43.042+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:43.091+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:43.085+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:43.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:43.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:29:53.715+0000] {processor.py:161} INFO - Started process (PID=31308) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:53.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:29:53.718+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:53.718+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:53.776+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:29:53.768+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:29:53.778+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:29:53.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T17:30:04.531+0000] {processor.py:161} INFO - Started process (PID=31392) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:04.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:04.534+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:04.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:04.576+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:04.569+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:04.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:04.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:30:12.470+0000] {processor.py:161} INFO - Started process (PID=31482) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:12.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:12.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:12.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:12.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:12.512+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:12.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:12.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:30:21.356+0000] {processor.py:161} INFO - Started process (PID=31565) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:21.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:21.361+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:21.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:21.415+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:21.407+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:21.416+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:21.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T17:30:29.963+0000] {processor.py:161} INFO - Started process (PID=31649) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:29.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:29.966+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:29.965+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:30.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:30.004+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:30.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:30.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:30:40.773+0000] {processor.py:161} INFO - Started process (PID=31738) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:40.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:40.779+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:40.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:40.833+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:40.826+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:40.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:40.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.095 seconds
[2024-05-10T17:30:49.427+0000] {processor.py:161} INFO - Started process (PID=31822) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:49.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:49.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:49.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:49.476+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:49.469+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:49.477+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:49.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:30:57.847+0000] {processor.py:161} INFO - Started process (PID=31905) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:57.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:30:57.850+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:57.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:57.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:30:57.887+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:30:57.895+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:30:57.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.081 seconds
[2024-05-10T17:31:06.967+0000] {processor.py:161} INFO - Started process (PID=31989) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:06.968+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:06.970+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:06.969+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:07.007+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:07.001+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:07.008+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:07.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.064 seconds
[2024-05-10T17:31:15.859+0000] {processor.py:161} INFO - Started process (PID=32078) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:15.861+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:15.862+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:15.861+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:15.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:15.894+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:15.903+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:15.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.065 seconds
[2024-05-10T17:31:23.776+0000] {processor.py:161} INFO - Started process (PID=32162) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:23.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:23.781+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:23.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:23.835+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:23.829+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:23.837+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:23.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.094 seconds
[2024-05-10T17:31:32.241+0000] {processor.py:161} INFO - Started process (PID=32245) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:32.243+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:32.245+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:32.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:32.297+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:32.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:32.298+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:32.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T17:31:40.959+0000] {processor.py:161} INFO - Started process (PID=32334) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:40.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:40.962+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:40.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:41.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:41.003+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:41.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:41.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T17:31:49.365+0000] {processor.py:161} INFO - Started process (PID=32417) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:49.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:49.370+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:49.369+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:49.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:49.424+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:49.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:49.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.108 seconds
[2024-05-10T17:31:57.031+0000] {processor.py:161} INFO - Started process (PID=32500) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:57.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:31:57.034+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:57.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:57.080+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:31:57.073+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:31:57.081+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:31:57.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.072 seconds
[2024-05-10T17:32:08.552+0000] {processor.py:161} INFO - Started process (PID=32583) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:08.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:08.556+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:08.555+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:08.615+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:08.605+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:08.616+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:08.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.105 seconds
[2024-05-10T17:32:18.180+0000] {processor.py:161} INFO - Started process (PID=32672) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:18.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:18.183+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:18.182+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:18.235+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:18.227+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:18.236+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:18.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:32:27.107+0000] {processor.py:161} INFO - Started process (PID=32755) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:27.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:27.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:27.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:27.156+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:27.149+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:27.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:27.175+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:32:34.089+0000] {processor.py:161} INFO - Started process (PID=32838) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:34.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:34.093+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:34.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:34.154+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:34.146+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:34.155+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:34.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.097 seconds
[2024-05-10T17:32:41.681+0000] {processor.py:161} INFO - Started process (PID=32927) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:41.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:41.684+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:41.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:41.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:41.723+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:41.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:41.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:32:49.101+0000] {processor.py:161} INFO - Started process (PID=33010) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:49.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:49.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:49.104+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:49.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:49.143+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:49.153+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:49.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:32:57.032+0000] {processor.py:161} INFO - Started process (PID=33093) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:57.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:32:57.035+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:57.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:57.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:32:57.077+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:32:57.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:32:57.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:33:05.664+0000] {processor.py:161} INFO - Started process (PID=33176) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:05.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:05.667+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:05.667+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:05.715+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:05.708+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:05.716+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:05.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:33:12.863+0000] {processor.py:161} INFO - Started process (PID=33265) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:12.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:12.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:12.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:12.914+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:12.907+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:12.916+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:12.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:33:19.882+0000] {processor.py:161} INFO - Started process (PID=33348) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:19.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:19.885+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:19.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:19.933+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:19.925+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:19.935+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:19.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:33:29.019+0000] {processor.py:161} INFO - Started process (PID=33431) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:29.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:29.022+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:29.021+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:29.070+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:29.061+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:29.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:29.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T17:33:37.374+0000] {processor.py:161} INFO - Started process (PID=33514) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:37.376+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:37.377+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:37.377+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:37.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:37.407+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:37.413+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:37.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.059 seconds
[2024-05-10T17:33:44.608+0000] {processor.py:161} INFO - Started process (PID=33604) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:44.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:44.611+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:44.611+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:44.653+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:44.647+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:44.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:44.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:33:52.733+0000] {processor.py:161} INFO - Started process (PID=33687) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:52.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:33:52.736+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:52.736+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:52.787+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:33:52.780+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:33:52.789+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:33:52.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.080 seconds
[2024-05-10T17:34:04.576+0000] {processor.py:161} INFO - Started process (PID=33770) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:04.577+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:04.578+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:04.578+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:04.630+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:04.621+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:04.632+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:04.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.087 seconds
[2024-05-10T17:34:13.096+0000] {processor.py:161} INFO - Started process (PID=33860) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:13.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:13.100+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:13.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:13.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:13.139+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:13.147+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:13.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.076 seconds
[2024-05-10T17:34:23.032+0000] {processor.py:161} INFO - Started process (PID=33943) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:23.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:23.035+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:23.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:23.085+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:23.077+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:23.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:23.101+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.078 seconds
[2024-05-10T17:34:33.587+0000] {processor.py:161} INFO - Started process (PID=34026) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:33.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:33.591+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:33.590+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:33.652+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:33.645+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:33.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:33.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.111 seconds
[2024-05-10T17:34:45.998+0000] {processor.py:161} INFO - Started process (PID=34115) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:45.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:46.001+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:46.001+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:46.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:46.041+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:46.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:46.063+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:34:55.451+0000] {processor.py:161} INFO - Started process (PID=34198) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:55.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:34:55.454+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:55.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:55.536+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:34:55.522+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:34:55.538+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:34:55.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T17:35:06.209+0000] {processor.py:161} INFO - Started process (PID=34282) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:06.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:35:06.212+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:06.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:06.255+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:06.248+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:35:06.257+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:06.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.069 seconds
[2024-05-10T17:35:19.489+0000] {processor.py:161} INFO - Started process (PID=34372) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:19.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:35:19.492+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:19.491+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:19.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:19.528+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:35:19.536+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:19.551+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
[2024-05-10T17:35:34.352+0000] {processor.py:161} INFO - Started process (PID=34455) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:34.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:35:34.354+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:34.354+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:34.404+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:34.396+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:35:34.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:34.419+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.077 seconds
[2024-05-10T17:35:44.428+0000] {processor.py:161} INFO - Started process (PID=34545) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:44.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:35:44.431+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:44.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:44.473+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:44.466+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:35:44.474+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:44.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.068 seconds
[2024-05-10T17:35:53.451+0000] {processor.py:161} INFO - Started process (PID=34628) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:53.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:35:53.455+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:53.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:53.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:35:53.500+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:35:53.510+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:35:53.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.090 seconds
[2024-05-10T17:36:05.169+0000] {processor.py:161} INFO - Started process (PID=34712) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:05.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:05.171+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:05.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:05.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:05.202+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:05.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:05.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.061 seconds
[2024-05-10T17:36:12.817+0000] {processor.py:161} INFO - Started process (PID=34802) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:12.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:12.821+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:12.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:12.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:12.860+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:12.867+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:12.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.075 seconds
[2024-05-10T17:36:20.209+0000] {processor.py:161} INFO - Started process (PID=34885) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:20.210+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:20.213+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:20.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:20.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:20.256+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:20.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:20.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:36:31.207+0000] {processor.py:161} INFO - Started process (PID=34968) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:31.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:31.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:31.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:31.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:31.254+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:31.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:31.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.091 seconds
[2024-05-10T17:36:42.863+0000] {processor.py:161} INFO - Started process (PID=35058) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:42.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:42.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:42.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:42.923+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:42.915+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:42.926+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:42.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T17:36:51.285+0000] {processor.py:161} INFO - Started process (PID=35142) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:51.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:36:51.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:51.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:51.333+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:36:51.325+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:36:51.336+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:36:51.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.074 seconds
[2024-05-10T17:37:01.752+0000] {processor.py:161} INFO - Started process (PID=35225) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:01.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:01.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:01.755+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:01.829+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:01.817+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:01.831+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:01.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.124 seconds
[2024-05-10T17:37:11.430+0000] {processor.py:161} INFO - Started process (PID=35315) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:11.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:11.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:11.432+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:11.489+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:11.481+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:11.491+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:11.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.092 seconds
[2024-05-10T17:37:18.912+0000] {processor.py:161} INFO - Started process (PID=35398) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:18.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:18.915+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:18.914+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:18.964+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:18.956+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:18.965+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:18.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.083 seconds
[2024-05-10T17:37:28.882+0000] {processor.py:161} INFO - Started process (PID=35481) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:28.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:28.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:28.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:28.951+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:28.941+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:28.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:28.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.106 seconds
[2024-05-10T17:37:37.560+0000] {processor.py:161} INFO - Started process (PID=35564) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:37.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:37.564+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:37.564+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:37.613+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:37.605+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:37.614+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:37.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.079 seconds
[2024-05-10T17:37:46.011+0000] {processor.py:161} INFO - Started process (PID=35653) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:46.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:46.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:46.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:46.064+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:46.056+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:46.065+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:46.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.084 seconds
[2024-05-10T17:37:54.940+0000] {processor.py:161} INFO - Started process (PID=35736) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:54.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:37:54.944+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:54.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:55.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:37:54.995+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:37:55.006+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:37:55.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.101 seconds
[2024-05-10T17:38:04.495+0000] {processor.py:161} INFO - Started process (PID=35820) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:04.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:38:04.498+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:04.498+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:04.542+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:04.534+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:38:04.543+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:04.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:38:13.258+0000] {processor.py:161} INFO - Started process (PID=35909) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:13.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:38:13.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:13.260+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:13.300+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:13.294+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:38:13.302+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:13.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.066 seconds
[2024-05-10T17:38:20.206+0000] {processor.py:161} INFO - Started process (PID=35993) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:20.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:38:20.209+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:20.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:20.254+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:20.246+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:38:20.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:20.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.070 seconds
[2024-05-10T17:38:29.084+0000] {processor.py:161} INFO - Started process (PID=36076) to work on /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:29.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all.py for tasks to queue
[2024-05-10T17:38:29.087+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:29.087+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:29.129+0000] {logging_mixin.py:188} INFO - [2024-05-10T17:38:29.123+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all.py", line 7, in <module>
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T17:38:29.130+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all.py
[2024-05-10T17:38:29.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all.py took 0.071 seconds
