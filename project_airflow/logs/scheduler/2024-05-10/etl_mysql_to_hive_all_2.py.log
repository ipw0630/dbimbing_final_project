[2024-05-10T15:44:12.558+0000] {processor.py:161} INFO - Started process (PID=70420) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:12.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:12.561+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:12.561+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:12.606+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:12.599+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all()
    ^^^^^^^^^^^^^^^^^^^^^
NameError: name 'etl_mysql_to_hive_all' is not defined
[2024-05-10T15:44:12.608+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:12.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T15:44:13.439+0000] {processor.py:161} INFO - Started process (PID=70426) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:13.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:13.443+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:13.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:13.479+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:13.472+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all()
    ^^^^^^^^^^^^^^^^^^^^^
NameError: name 'etl_mysql_to_hive_all' is not defined
[2024-05-10T15:44:13.481+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:13.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.095 seconds
[2024-05-10T15:44:21.458+0000] {processor.py:161} INFO - Started process (PID=70509) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:21.462+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:21.464+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.464+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:21.572+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:21.798+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.796+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:etl_mysql_to_hive_all_2
[2024-05-10T15:44:21.815+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.815+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:etl_mysql_to_hive_all_2
[2024-05-10T15:44:21.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.829+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:etl_mysql_to_hive_all_2
[2024-05-10T15:44:21.832+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.831+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:21.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.858+0000] {dag.py:3118} INFO - Creating ORM DAG for etl_mysql_to_hive_all_2
[2024-05-10T15:44:21.881+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:21.880+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:44:21.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.464 seconds
[2024-05-10T15:44:30.489+0000] {processor.py:161} INFO - Started process (PID=70599) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:30.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:30.493+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:30.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:30.565+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:30.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:30.576+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:30.627+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:30.626+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:44:30.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.189 seconds
[2024-05-10T15:44:48.250+0000] {processor.py:161} INFO - Started process (PID=70683) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:48.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:48.254+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.253+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:48.348+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:48.356+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.355+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:48.402+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:48.401+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:44:48.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.208 seconds
[2024-05-10T15:44:57.353+0000] {processor.py:161} INFO - Started process (PID=70773) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:57.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:44:57.355+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:57.403+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:44:57.429+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.428+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:44:57.459+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:44:57.459+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:44:57.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.145 seconds
[2024-05-10T15:45:10.886+0000] {processor.py:161} INFO - Started process (PID=70861) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:10.889+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:45:10.894+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:10.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:11.023+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:11.106+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:11.105+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:11.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:11.186+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:45:11.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.389 seconds
[2024-05-10T15:45:28.642+0000] {processor.py:161} INFO - Started process (PID=70950) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:28.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:45:28.646+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:28.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:28.714+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:28.752+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:28.752+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:28.805+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:28.804+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:45:28.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.209 seconds
[2024-05-10T15:45:39.033+0000] {processor.py:161} INFO - Started process (PID=71033) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:39.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:45:39.036+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.036+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:39.088+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:39.119+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.119+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:39.155+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:39.155+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:45:39.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.162 seconds
[2024-05-10T15:45:51.779+0000] {processor.py:161} INFO - Started process (PID=71116) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:51.780+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:45:51.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:51.781+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:51.850+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:45:51.880+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:51.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:45:51.923+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:45:51.922+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:45:51.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.186 seconds
[2024-05-10T15:46:01.980+0000] {processor.py:161} INFO - Started process (PID=71206) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:01.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:01.986+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:01.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:02.050+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:02.076+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:02.075+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:02.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:02.120+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:02.153+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.186 seconds
[2024-05-10T15:46:10.291+0000] {processor.py:161} INFO - Started process (PID=71290) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:10.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:10.293+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.293+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:10.341+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:10.362+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.361+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:10.388+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:10.388+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:10.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.129 seconds
[2024-05-10T15:46:20.406+0000] {processor.py:161} INFO - Started process (PID=71373) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:20.408+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:20.410+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.409+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:20.472+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:20.499+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.499+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:20.539+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:20.539+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:20.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.181 seconds
[2024-05-10T15:46:27.890+0000] {processor.py:161} INFO - Started process (PID=71462) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:27.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:27.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:27.893+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:27.948+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:27.973+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:27.973+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:28.009+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:28.008+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:28.050+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.172 seconds
[2024-05-10T15:46:39.456+0000] {processor.py:161} INFO - Started process (PID=71545) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:39.458+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:39.459+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:39.459+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:39.522+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:39.552+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:39.551+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:39.590+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:39.590+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:39.636+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.195 seconds
[2024-05-10T15:46:48.509+0000] {processor.py:161} INFO - Started process (PID=71628) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:48.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:48.512+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:48.563+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:48.587+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.587+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:48.617+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:48.617+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:48.642+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:46:57.359+0000] {processor.py:161} INFO - Started process (PID=71717) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:57.361+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:46:57.362+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.362+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:57.415+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:46:57.436+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.436+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:46:57.463+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:46:57.462+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:46:57.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.139 seconds
[2024-05-10T15:47:08.079+0000] {processor.py:161} INFO - Started process (PID=71800) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:08.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:08.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:08.148+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:08.189+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.188+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:08.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:08.236+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:08.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.215 seconds
[2024-05-10T15:47:18.737+0000] {processor.py:161} INFO - Started process (PID=71884) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:18.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:18.740+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:18.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:18.793+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:18.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:18.815+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:18.843+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:18.843+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:18.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:47:26.959+0000] {processor.py:161} INFO - Started process (PID=71974) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:26.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:26.963+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:26.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:27.020+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:27.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:27.047+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:27.083+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:27.082+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:27.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.162 seconds
[2024-05-10T15:47:36.417+0000] {processor.py:161} INFO - Started process (PID=72057) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:36.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:36.420+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:36.471+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:36.495+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.495+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:36.526+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:36.526+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:36.555+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.148 seconds
[2024-05-10T15:47:44.764+0000] {processor.py:161} INFO - Started process (PID=72140) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:44.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:44.768+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:44.767+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:44.833+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:44.859+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:44.859+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:44.896+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:44.896+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:44.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.176 seconds
[2024-05-10T15:47:53.261+0000] {processor.py:161} INFO - Started process (PID=72223) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:53.262+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:47:53.264+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.263+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:53.314+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:47:53.336+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.336+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:47:53.367+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:47:53.366+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:47:53.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.143 seconds
[2024-05-10T15:48:00.725+0000] {processor.py:161} INFO - Started process (PID=72312) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:00.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:00.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:00.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:00.780+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:00.804+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:00.804+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:00.836+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:00.836+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:00.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.151 seconds
[2024-05-10T15:48:08.827+0000] {processor.py:161} INFO - Started process (PID=72395) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:08.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:08.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:08.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:08.883+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:08.906+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:08.906+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:08.936+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:08.936+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:08.964+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.147 seconds
[2024-05-10T15:48:16.492+0000] {processor.py:161} INFO - Started process (PID=72478) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:16.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:16.495+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.495+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:16.547+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:16.575+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.575+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:16.608+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:16.608+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:16.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.154 seconds
[2024-05-10T15:48:25.037+0000] {processor.py:161} INFO - Started process (PID=72561) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:25.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:25.040+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.039+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:25.093+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:25.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.120+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:25.157+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:25.157+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:25.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.162 seconds
[2024-05-10T15:48:33.431+0000] {processor.py:161} INFO - Started process (PID=72651) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:33.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:33.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:33.492+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:33.518+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.517+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:33.553+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:33.553+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:33.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.161 seconds
[2024-05-10T15:48:41.441+0000] {processor.py:161} INFO - Started process (PID=72735) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:41.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:41.444+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.444+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:41.493+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:41.517+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.516+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:41.547+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:41.547+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:41.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:48:51.158+0000] {processor.py:161} INFO - Started process (PID=72818) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:51.160+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:48:51.161+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.161+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:51.210+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:48:51.235+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.235+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:48:51.287+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:48:51.287+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:48:51.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.173 seconds
[2024-05-10T15:49:00.767+0000] {processor.py:161} INFO - Started process (PID=72907) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:00.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:00.770+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:00.770+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:00.826+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:00.854+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:00.853+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:00.884+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:00.884+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:00.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.159 seconds
[2024-05-10T15:49:09.662+0000] {processor.py:161} INFO - Started process (PID=72990) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:09.663+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:09.665+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:09.664+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:09.708+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:09.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:09.728+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:09.756+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:09.756+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:09.782+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.128 seconds
[2024-05-10T15:49:19.448+0000] {processor.py:161} INFO - Started process (PID=73074) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:19.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:19.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:19.496+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:19.519+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:19.548+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:19.548+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:19.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.144 seconds
[2024-05-10T15:49:29.800+0000] {processor.py:161} INFO - Started process (PID=73163) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:29.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:29.804+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:29.803+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:29.880+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:29.919+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:29.918+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:29.980+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:29.979+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:30.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.247 seconds
[2024-05-10T15:49:40.118+0000] {processor.py:161} INFO - Started process (PID=73247) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:40.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:40.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:40.167+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:40.187+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.187+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:40.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:40.213+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:40.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.128 seconds
[2024-05-10T15:49:49.952+0000] {processor.py:161} INFO - Started process (PID=73330) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:49.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:49.956+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:49.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:50.022+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:50.053+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:50.053+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:50.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:50.096+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:50.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.187 seconds
[2024-05-10T15:49:57.616+0000] {processor.py:161} INFO - Started process (PID=73420) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:57.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:49:57.619+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:57.619+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:57.670+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:49:57.691+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:57.690+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:49:57.720+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:49:57.719+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:49:57.745+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.137 seconds
[2024-05-10T15:50:05.899+0000] {processor.py:161} INFO - Started process (PID=73503) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:05.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:05.902+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:05.902+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:05.954+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:05.976+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:05.975+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:06.003+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:06.003+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:06.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:50:15.517+0000] {processor.py:161} INFO - Started process (PID=73587) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:15.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:15.519+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:15.561+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:15.581+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.580+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:15.608+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:15.607+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:15.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.121 seconds
[2024-05-10T15:50:24.047+0000] {processor.py:161} INFO - Started process (PID=73671) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:24.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:24.051+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.050+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:24.098+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:24.119+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.118+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:24.146+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:24.146+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:24.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.134 seconds
[2024-05-10T15:50:32.147+0000] {processor.py:161} INFO - Started process (PID=73762) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:32.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:32.150+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.149+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:32.203+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:32.224+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.224+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:32.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:32.253+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:32.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:50:40.750+0000] {processor.py:161} INFO - Started process (PID=73845) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:40.752+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:40.754+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:40.753+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:40.814+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:40.842+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:40.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:40.878+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:40.877+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:40.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.169 seconds
[2024-05-10T15:50:50.596+0000] {processor.py:161} INFO - Started process (PID=73928) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:50.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:50.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:50.599+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:50.645+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:50.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:50.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:50.712+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:50.711+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:50.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.151 seconds
[2024-05-10T15:50:59.232+0000] {processor.py:161} INFO - Started process (PID=74018) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:59.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:50:59.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:59.297+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:50:59.329+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.328+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:50:59.368+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:50:59.367+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:50:59.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.178 seconds
[2024-05-10T15:51:08.682+0000] {processor.py:161} INFO - Started process (PID=74101) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:08.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:08.685+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:08.731+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:08.751+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.750+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:08.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:08.776+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:08.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.127 seconds
[2024-05-10T15:51:17.039+0000] {processor.py:161} INFO - Started process (PID=74184) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:17.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:17.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:17.091+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:17.117+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.116+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:17.144+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:17.144+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:17.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.138 seconds
[2024-05-10T15:51:26.363+0000] {processor.py:161} INFO - Started process (PID=74273) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:26.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:26.365+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:26.412+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:26.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.434+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:26.461+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:26.460+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:26.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.131 seconds
[2024-05-10T15:51:35.536+0000] {processor.py:161} INFO - Started process (PID=74357) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:35.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:35.540+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:35.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:35.618+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:35.650+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:35.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:35.700+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:35.699+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:35.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.213 seconds
[2024-05-10T15:51:43.207+0000] {processor.py:161} INFO - Started process (PID=74440) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:43.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:43.210+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:43.257+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:43.279+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.279+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:43.309+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:43.309+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:43.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.138 seconds
[2024-05-10T15:51:51.148+0000] {processor.py:161} INFO - Started process (PID=74523) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:51.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:51.151+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:51.194+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:51.214+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.214+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:51.240+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:51.240+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:51.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.126 seconds
[2024-05-10T15:51:59.623+0000] {processor.py:161} INFO - Started process (PID=74613) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:59.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:51:59.626+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:59.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:59.677+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:51:59.702+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:59.702+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:51:59.737+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:51:59.737+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:51:59.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.150 seconds
[2024-05-10T15:52:10.864+0000] {processor.py:161} INFO - Started process (PID=74696) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:10.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:10.867+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:10.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:10.920+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:10.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:10.941+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:10.971+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:10.971+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:10.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.143 seconds
[2024-05-10T15:52:21.003+0000] {processor.py:161} INFO - Started process (PID=74780) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:21.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:21.006+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.006+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:21.054+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:21.078+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.078+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:21.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:21.110+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:21.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.142 seconds
[2024-05-10T15:52:28.174+0000] {processor.py:161} INFO - Started process (PID=74870) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:28.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:28.177+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.177+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:28.231+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:28.254+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:28.283+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:28.282+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:28.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.146 seconds
[2024-05-10T15:52:38.027+0000] {processor.py:161} INFO - Started process (PID=74953) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:38.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:38.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.029+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:38.087+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:38.111+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.111+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:38.149+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:38.149+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:38.182+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.164 seconds
[2024-05-10T15:52:46.759+0000] {processor.py:161} INFO - Started process (PID=75036) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:46.761+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:46.762+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:46.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:46.807+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:46.829+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:46.829+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:46.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:46.857+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:47.033+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.282 seconds
[2024-05-10T15:52:54.045+0000] {processor.py:161} INFO - Started process (PID=75119) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:54.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:52:54.047+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:54.102+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:52:54.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:52:54.166+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:52:54.165+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:52:54.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.157 seconds
[2024-05-10T15:53:01.637+0000] {processor.py:161} INFO - Started process (PID=75207) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:01.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:53:01.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:01.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:01.693+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:01.715+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:01.715+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:01.745+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:01.744+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:53:01.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.145 seconds
[2024-05-10T15:53:11.312+0000] {processor.py:161} INFO - Started process (PID=75290) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:11.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:53:11.315+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:11.372+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:11.396+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.396+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:11.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:11.427+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:53:11.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.151 seconds
[2024-05-10T15:53:23.487+0000] {processor.py:161} INFO - Started process (PID=75373) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:23.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:53:23.491+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:23.491+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:23.553+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:23.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:23.579+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:23.868+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:23.868+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:53:23.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.436 seconds
[2024-05-10T15:53:34.483+0000] {processor.py:161} INFO - Started process (PID=75462) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:34.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:53:34.488+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:34.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:34.569+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:34.605+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:34.605+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:34.648+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:34.647+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:53:34.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.224 seconds
[2024-05-10T15:53:48.902+0000] {processor.py:161} INFO - Started process (PID=75546) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:48.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:53:48.905+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:48.904+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:48.960+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:53:48.992+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:48.991+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:53:49.030+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:53:49.030+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:53:49.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.166 seconds
[2024-05-10T15:54:04.767+0000] {processor.py:161} INFO - Started process (PID=75636) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:04.771+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:04.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:04.773+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:04.839+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:05.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:05.096+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:05.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:05.175+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:54:05.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.512 seconds
[2024-05-10T15:54:18.925+0000] {processor.py:161} INFO - Started process (PID=75719) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:18.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:18.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:18.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:18.979+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:19.005+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:19.004+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:19.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:19.252+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:54:19.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.366 seconds
[2024-05-10T15:54:31.001+0000] {processor.py:161} INFO - Started process (PID=75808) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:31.002+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:31.004+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:31.056+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:31.079+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.078+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:31.109+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:31.108+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:54:31.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.144 seconds
[2024-05-10T15:54:41.105+0000] {processor.py:161} INFO - Started process (PID=75891) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:41.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:41.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:41.183+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:41.220+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.219+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:41.302+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:41.302+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:54:41.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.277 seconds
[2024-05-10T15:54:50.292+0000] {processor.py:161} INFO - Started process (PID=75974) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:50.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:50.296+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.295+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:50.353+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:50.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.378+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:50.414+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:50.414+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:54:50.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.171 seconds
[2024-05-10T15:54:59.822+0000] {processor.py:161} INFO - Started process (PID=76063) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:59.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:54:59.827+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:59.827+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:59.902+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:54:59.941+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:59.940+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:54:59.990+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:54:59.989+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:00.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.218 seconds
[2024-05-10T15:55:08.708+0000] {processor.py:161} INFO - Started process (PID=76146) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:08.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:08.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:08.710+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:08.759+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:08.783+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:08.783+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:08.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:08.816+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:08.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.148 seconds
[2024-05-10T15:55:18.454+0000] {processor.py:161} INFO - Started process (PID=76229) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:18.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:18.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.456+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:18.506+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:18.527+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.527+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:18.558+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:18.558+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:18.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.144 seconds
[2024-05-10T15:55:28.064+0000] {processor.py:161} INFO - Started process (PID=76318) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:28.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:28.070+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:28.129+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:28.159+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.159+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:28.200+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:28.199+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:28.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.183 seconds
[2024-05-10T15:55:37.250+0000] {processor.py:161} INFO - Started process (PID=76401) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:37.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:37.253+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.253+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:37.318+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:37.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.350+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:37.387+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:37.387+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:37.419+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.180 seconds
[2024-05-10T15:55:45.273+0000] {processor.py:161} INFO - Started process (PID=76484) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:45.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:45.276+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.276+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:45.348+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:45.376+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.375+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:45.419+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:45.418+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:45.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.192 seconds
[2024-05-10T15:55:54.342+0000] {processor.py:161} INFO - Started process (PID=76567) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:54.344+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:55:54.346+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.345+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:54.388+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:55:54.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.413+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:55:54.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:55:54.441+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:55:54.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.131 seconds
[2024-05-10T15:56:01.488+0000] {processor.py:161} INFO - Started process (PID=76657) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:01.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:01.492+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.491+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:01.540+0000] {processor.py:840} INFO - DAG(s) 'etl_mysql_to_hive_all_2' retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:01.562+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.561+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-05-10T15:56:01.590+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:01.590+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_mysql_to_hive_all_2 to None, run_after=None
[2024-05-10T15:56:01.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.136 seconds
[2024-05-10T15:56:11.312+0000] {processor.py:161} INFO - Started process (PID=76740) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:11.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:11.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:11.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:11.385+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:11.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:11.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:11.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.107 seconds
[2024-05-10T15:56:23.056+0000] {processor.py:161} INFO - Started process (PID=76823) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:23.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:23.064+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:23.059+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:23.147+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:23.131+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:23.149+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:23.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.124 seconds
[2024-05-10T15:56:31.536+0000] {processor.py:161} INFO - Started process (PID=76912) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:31.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:31.538+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:31.538+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:31.593+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:31.586+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:31.594+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:31.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T15:56:41.110+0000] {processor.py:161} INFO - Started process (PID=76995) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:41.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:41.113+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:41.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:41.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:41.165+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:41.178+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:41.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.098 seconds
[2024-05-10T15:56:49.726+0000] {processor.py:161} INFO - Started process (PID=77079) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:49.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:49.730+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:49.729+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:49.782+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:49.774+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:49.784+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:49.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.086 seconds
[2024-05-10T15:56:56.493+0000] {processor.py:161} INFO - Started process (PID=77162) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:56.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:56:56.496+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:56.495+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:56.576+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:56:56.564+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:56:56.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:56:56.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.122 seconds
[2024-05-10T15:57:06.736+0000] {processor.py:161} INFO - Started process (PID=77251) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:06.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:06.739+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:06.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:06.796+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:06.787+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:06.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:06.815+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.089 seconds
[2024-05-10T15:57:16.243+0000] {processor.py:161} INFO - Started process (PID=77335) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:16.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:16.247+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:16.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:16.314+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:16.303+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:16.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:16.342+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.111 seconds
[2024-05-10T15:57:27.205+0000] {processor.py:161} INFO - Started process (PID=77424) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:27.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:27.208+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:27.207+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:27.254+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:27.246+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:27.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:27.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T15:57:37.252+0000] {processor.py:161} INFO - Started process (PID=77508) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:37.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:37.255+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:37.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:37.318+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:37.308+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:37.320+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:37.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.093 seconds
[2024-05-10T15:57:46.089+0000] {processor.py:161} INFO - Started process (PID=77591) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:46.091+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:46.093+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:46.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:46.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:46.160+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:46.172+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:46.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.125 seconds
[2024-05-10T15:57:57.886+0000] {processor.py:161} INFO - Started process (PID=77680) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:57.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:57:57.891+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:57.890+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:57.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:57:57.974+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:57:57.991+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:57:58.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.151 seconds
[2024-05-10T15:58:10.300+0000] {processor.py:161} INFO - Started process (PID=77763) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:10.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:10.304+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:10.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:10.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:10.365+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:10.381+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:10.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.120 seconds
[2024-05-10T15:58:20.012+0000] {processor.py:161} INFO - Started process (PID=77846) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:20.014+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:20.015+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:20.015+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:20.064+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:20.055+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:20.065+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:20.079+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T15:58:29.158+0000] {processor.py:161} INFO - Started process (PID=77935) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:29.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:29.161+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:29.161+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:29.232+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:29.219+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:29.234+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:29.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.110 seconds
[2024-05-10T15:58:40.564+0000] {processor.py:161} INFO - Started process (PID=78019) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:40.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:40.567+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:40.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:40.616+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:40.608+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:40.618+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:40.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T15:58:48.377+0000] {processor.py:161} INFO - Started process (PID=78102) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:48.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:48.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:48.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:48.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:48.421+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:48.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:48.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T15:58:56.156+0000] {processor.py:161} INFO - Started process (PID=78185) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:56.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:58:56.159+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:56.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:56.217+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:58:56.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:58:56.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:58:56.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.091 seconds
[2024-05-10T15:59:04.981+0000] {processor.py:161} INFO - Started process (PID=78274) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:04.982+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:04.984+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:04.983+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:05.035+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:05.026+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:05.036+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:05.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T15:59:14.818+0000] {processor.py:161} INFO - Started process (PID=78357) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:14.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:14.822+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:14.821+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:14.875+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:14.867+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:14.876+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:14.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T15:59:23.138+0000] {processor.py:161} INFO - Started process (PID=78440) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:23.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:23.141+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:23.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:23.193+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:23.185+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:23.194+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:23.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.077 seconds
[2024-05-10T15:59:31.591+0000] {processor.py:161} INFO - Started process (PID=78529) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:31.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:31.595+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:31.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:31.650+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:31.640+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:31.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:31.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.085 seconds
[2024-05-10T15:59:42.560+0000] {processor.py:161} INFO - Started process (PID=78613) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:42.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:42.562+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:42.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:42.607+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:42.599+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:42.609+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:42.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.073 seconds
[2024-05-10T15:59:52.757+0000] {processor.py:161} INFO - Started process (PID=78696) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:52.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T15:59:52.761+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:52.760+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:52.844+0000] {logging_mixin.py:188} INFO - [2024-05-10T15:59:52.833+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T15:59:52.845+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T15:59:52.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.126 seconds
[2024-05-10T16:00:04.143+0000] {processor.py:161} INFO - Started process (PID=78785) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:04.145+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:04.147+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:04.146+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:04.213+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:04.202+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:04.215+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:04.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.109 seconds
[2024-05-10T16:00:12.878+0000] {processor.py:161} INFO - Started process (PID=78868) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:12.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:12.882+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:12.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:12.942+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:12.931+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:12.943+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:12.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:00:23.605+0000] {processor.py:161} INFO - Started process (PID=78951) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:23.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:23.609+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:23.608+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:23.657+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:23.648+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:23.658+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:23.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.072 seconds
[2024-05-10T16:00:33.181+0000] {processor.py:161} INFO - Started process (PID=79041) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:33.183+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:33.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:33.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:33.242+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:33.232+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:33.243+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:33.258+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:00:41.755+0000] {processor.py:161} INFO - Started process (PID=79125) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:41.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:41.759+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:41.759+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:41.814+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:41.804+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:41.816+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:41.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.090 seconds
[2024-05-10T16:00:50.905+0000] {processor.py:161} INFO - Started process (PID=79208) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:50.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:00:50.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:50.907+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:50.958+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:00:50.951+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:00:50.960+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:00:50.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:01:01.088+0000] {processor.py:161} INFO - Started process (PID=79297) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:01.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:01.092+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:01.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:01.157+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:01.148+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:01.158+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:01.174+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.098 seconds
[2024-05-10T16:01:10.107+0000] {processor.py:161} INFO - Started process (PID=79380) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:10.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:10.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:10.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:10.169+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:10.158+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:10.170+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:10.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:01:20.027+0000] {processor.py:161} INFO - Started process (PID=79463) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:20.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:20.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:20.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:20.090+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:20.081+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:20.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:20.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.090 seconds
[2024-05-10T16:01:27.985+0000] {processor.py:161} INFO - Started process (PID=79552) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:27.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:27.989+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:27.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:28.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:28.032+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:28.042+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:28.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T16:01:36.886+0000] {processor.py:161} INFO - Started process (PID=79635) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:36.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:36.889+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:36.889+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:36.943+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:36.934+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:36.944+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:36.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T16:01:46.945+0000] {processor.py:161} INFO - Started process (PID=79719) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:46.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:46.948+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:46.948+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:47.014+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:47.002+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:47.016+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:47.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.099 seconds
[2024-05-10T16:01:55.409+0000] {processor.py:161} INFO - Started process (PID=79802) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:55.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:01:55.413+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:55.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:55.477+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:01:55.464+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:01:55.479+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:01:55.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.100 seconds
[2024-05-10T16:02:03.921+0000] {processor.py:161} INFO - Started process (PID=79891) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:03.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:03.925+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:03.925+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:03.981+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:03.973+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:03.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:03.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:02:12.522+0000] {processor.py:161} INFO - Started process (PID=79974) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:12.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:12.525+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:12.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:12.578+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:12.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:12.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:12.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.081 seconds
[2024-05-10T16:02:23.674+0000] {processor.py:161} INFO - Started process (PID=80057) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:23.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:23.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:23.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:23.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:23.720+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:23.730+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:23.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.080 seconds
[2024-05-10T16:02:31.812+0000] {processor.py:161} INFO - Started process (PID=80146) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:31.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:31.816+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:31.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:31.864+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:31.856+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:31.865+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:31.878+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:02:42.771+0000] {processor.py:161} INFO - Started process (PID=80230) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:42.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:42.774+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:42.774+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:42.826+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:42.818+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:42.827+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:42.840+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:02:49.992+0000] {processor.py:161} INFO - Started process (PID=80313) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:49.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:49.995+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:49.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:50.046+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:50.039+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:50.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:50.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.080 seconds
[2024-05-10T16:02:58.513+0000] {processor.py:161} INFO - Started process (PID=80402) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:58.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:02:58.517+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:58.516+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:58.571+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:02:58.563+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:02:58.573+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:02:58.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.086 seconds
[2024-05-10T16:03:07.546+0000] {processor.py:161} INFO - Started process (PID=80485) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:07.548+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:07.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:07.549+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:07.599+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:07.592+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:07.601+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:07.614+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.076 seconds
[2024-05-10T16:03:16.749+0000] {processor.py:161} INFO - Started process (PID=80569) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:16.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:16.753+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:16.752+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:16.813+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:16.803+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:16.814+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:16.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.089 seconds
[2024-05-10T16:03:25.445+0000] {processor.py:161} INFO - Started process (PID=80653) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:25.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:25.449+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:25.448+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:25.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:25.496+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:25.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:25.520+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.086 seconds
[2024-05-10T16:03:33.529+0000] {processor.py:161} INFO - Started process (PID=80742) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:33.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:33.533+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:33.532+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:33.591+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:33.583+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:33.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:33.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.089 seconds
[2024-05-10T16:03:41.659+0000] {processor.py:161} INFO - Started process (PID=80825) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:41.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:41.664+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:41.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:41.715+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:41.708+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:41.717+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:41.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.085 seconds
[2024-05-10T16:03:49.389+0000] {processor.py:161} INFO - Started process (PID=80908) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:49.391+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:49.393+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:49.392+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:49.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:49.434+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:49.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:49.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.080 seconds
[2024-05-10T16:03:58.387+0000] {processor.py:161} INFO - Started process (PID=80997) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:58.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:03:58.390+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:58.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:58.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:03:58.431+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:03:58.440+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:03:58.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.076 seconds
[2024-05-10T16:04:07.412+0000] {processor.py:161} INFO - Started process (PID=81080) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:07.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:07.415+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:07.414+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:07.460+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:07.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:07.462+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:07.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:04:16.437+0000] {processor.py:161} INFO - Started process (PID=81163) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:16.438+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:16.441+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:16.440+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:16.503+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:16.493+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:16.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:16.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.104 seconds
[2024-05-10T16:04:24.396+0000] {processor.py:161} INFO - Started process (PID=81246) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:24.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:24.399+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:24.399+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:24.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:24.443+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:24.452+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:24.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.089 seconds
[2024-05-10T16:04:35.478+0000] {processor.py:161} INFO - Started process (PID=81335) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:35.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:35.481+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:35.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:35.535+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:35.526+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:35.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:35.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T16:04:45.337+0000] {processor.py:161} INFO - Started process (PID=81418) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:45.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:45.342+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:45.341+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:45.422+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:45.410+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:45.424+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:45.448+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.127 seconds
[2024-05-10T16:04:55.787+0000] {processor.py:161} INFO - Started process (PID=81501) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:55.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:04:55.793+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:55.792+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:55.896+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:04:55.883+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:04:55.900+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:04:55.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.163 seconds
[2024-05-10T16:05:08.469+0000] {processor.py:161} INFO - Started process (PID=81590) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:08.470+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:08.472+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:08.471+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:08.526+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:08.516+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:08.528+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:08.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.086 seconds
[2024-05-10T16:05:16.522+0000] {processor.py:161} INFO - Started process (PID=81673) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:16.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:16.525+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:16.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:16.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:16.568+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:16.579+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:16.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.096 seconds
[2024-05-10T16:05:26.574+0000] {processor.py:161} INFO - Started process (PID=81756) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:26.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:26.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:26.576+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:26.625+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:26.617+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:26.626+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:26.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.104 seconds
[2024-05-10T16:05:35.377+0000] {processor.py:161} INFO - Started process (PID=81845) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:35.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:35.380+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:35.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:35.428+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:35.420+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:35.429+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:35.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T16:05:43.467+0000] {processor.py:161} INFO - Started process (PID=81928) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:43.469+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:43.470+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:43.470+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:43.520+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:43.510+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:43.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:43.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.080 seconds
[2024-05-10T16:05:51.166+0000] {processor.py:161} INFO - Started process (PID=82011) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:51.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:05:51.168+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:51.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:51.221+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:05:51.212+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:05:51.223+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:05:51.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.089 seconds
[2024-05-10T16:06:01.826+0000] {processor.py:161} INFO - Started process (PID=82099) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:01.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:01.830+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:01.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:01.886+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:01.878+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:01.887+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:01.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T16:06:09.791+0000] {processor.py:161} INFO - Started process (PID=82183) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:09.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:09.794+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:09.793+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:09.843+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:09.834+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:09.844+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:09.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T16:06:19.502+0000] {processor.py:161} INFO - Started process (PID=82266) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:19.503+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:19.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:19.504+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:19.562+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:19.553+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:19.563+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:19.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.086 seconds
[2024-05-10T16:06:27.707+0000] {processor.py:161} INFO - Started process (PID=82349) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:27.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:27.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:27.710+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:27.758+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:27.749+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:27.759+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:27.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.072 seconds
[2024-05-10T16:06:37.072+0000] {processor.py:161} INFO - Started process (PID=82438) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:37.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:37.077+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:37.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:37.128+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:37.120+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:37.129+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:37.144+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.081 seconds
[2024-05-10T16:06:45.539+0000] {processor.py:161} INFO - Started process (PID=82522) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:45.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:45.542+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:45.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:45.589+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:45.581+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:45.590+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:45.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.072 seconds
[2024-05-10T16:06:54.375+0000] {processor.py:161} INFO - Started process (PID=82605) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:54.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:06:54.379+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:54.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:54.440+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:06:54.431+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:06:54.442+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:06:54.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.101 seconds
[2024-05-10T16:07:03.013+0000] {processor.py:161} INFO - Started process (PID=82694) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:03.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:03.019+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:03.018+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:03.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:03.068+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:03.086+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:03.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.130 seconds
[2024-05-10T16:07:12.182+0000] {processor.py:161} INFO - Started process (PID=82777) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:12.183+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:12.185+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:12.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:12.236+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:12.227+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:12.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:12.249+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.076 seconds
[2024-05-10T16:07:20.107+0000] {processor.py:161} INFO - Started process (PID=82860) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:20.109+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:20.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:20.110+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:20.176+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:20.162+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:20.180+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:20.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.102 seconds
[2024-05-10T16:07:28.768+0000] {processor.py:161} INFO - Started process (PID=82949) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:28.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:28.771+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:28.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:28.824+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:28.815+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:28.826+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:28.840+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:07:37.631+0000] {processor.py:161} INFO - Started process (PID=83033) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:37.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:37.634+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:37.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:37.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:37.686+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:37.697+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:37.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.091 seconds
[2024-05-10T16:07:46.705+0000] {processor.py:161} INFO - Started process (PID=83117) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:46.707+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:46.710+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:46.709+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:46.777+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:46.765+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:46.778+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:46.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.102 seconds
[2024-05-10T16:07:55.065+0000] {processor.py:161} INFO - Started process (PID=83200) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:55.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:07:55.068+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:55.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:55.121+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:07:55.112+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:07:55.122+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:07:55.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:08:04.447+0000] {processor.py:161} INFO - Started process (PID=83289) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:04.449+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:04.450+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:04.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:04.505+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:04.497+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:04.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:04.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:08:11.891+0000] {processor.py:161} INFO - Started process (PID=83372) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:11.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:11.893+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:11.893+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:11.937+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:11.929+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:11.938+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:11.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.066 seconds
[2024-05-10T16:08:21.672+0000] {processor.py:161} INFO - Started process (PID=83455) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:21.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:21.677+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:21.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:21.748+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:21.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:21.750+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:21.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.107 seconds
[2024-05-10T16:08:31.052+0000] {processor.py:161} INFO - Started process (PID=83544) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:31.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:31.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:31.055+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:31.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:31.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:31.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:31.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.078 seconds
[2024-05-10T16:08:41.745+0000] {processor.py:161} INFO - Started process (PID=83628) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:41.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:41.748+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:41.748+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:41.806+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:41.798+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:41.808+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:41.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:08:49.299+0000] {processor.py:161} INFO - Started process (PID=83711) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:49.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:49.303+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:49.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:49.363+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:49.353+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:49.365+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:49.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:08:57.126+0000] {processor.py:161} INFO - Started process (PID=83794) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:57.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:08:57.129+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:57.128+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:57.183+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:08:57.174+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:08:57.185+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:08:57.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T16:09:05.307+0000] {processor.py:161} INFO - Started process (PID=83883) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:05.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:05.311+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:05.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:05.375+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:05.364+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:05.376+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:05.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.096 seconds
[2024-05-10T16:09:12.995+0000] {processor.py:161} INFO - Started process (PID=83967) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:12.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:12.998+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:12.997+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:13.048+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:13.040+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:13.050+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:13.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.078 seconds
[2024-05-10T16:09:22.850+0000] {processor.py:161} INFO - Started process (PID=84050) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:22.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:22.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:22.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:22.908+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:22.899+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:22.910+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:22.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:09:30.449+0000] {processor.py:161} INFO - Started process (PID=84140) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:30.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:30.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:30.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:30.508+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:30.498+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:30.510+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:30.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:09:39.028+0000] {processor.py:161} INFO - Started process (PID=84223) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:39.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:39.031+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:39.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:39.074+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:39.067+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:39.075+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:39.086+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.066 seconds
[2024-05-10T16:09:46.168+0000] {processor.py:161} INFO - Started process (PID=84306) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:46.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:46.170+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:46.170+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:46.216+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:46.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:46.218+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:46.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.073 seconds
[2024-05-10T16:09:55.191+0000] {processor.py:161} INFO - Started process (PID=84389) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:55.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:09:55.193+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:55.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:55.243+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:09:55.234+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:09:55.244+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:09:55.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.081 seconds
[2024-05-10T16:10:02.429+0000] {processor.py:161} INFO - Started process (PID=84478) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:02.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:02.433+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:02.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:02.490+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:02.482+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:02.492+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:02.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:10:11.038+0000] {processor.py:161} INFO - Started process (PID=84561) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:11.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:11.040+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:11.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:11.082+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:11.075+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:11.083+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:11.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:10:19.598+0000] {processor.py:161} INFO - Started process (PID=84644) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:19.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:19.601+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:19.600+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:19.694+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:19.653+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:19.695+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:19.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.123 seconds
[2024-05-10T16:10:29.807+0000] {processor.py:161} INFO - Started process (PID=84733) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:29.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:29.810+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:29.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:29.866+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:29.856+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:29.867+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:29.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.091 seconds
[2024-05-10T16:10:38.498+0000] {processor.py:161} INFO - Started process (PID=84817) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:38.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:38.500+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:38.500+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:38.549+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:38.540+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:38.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:38.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:10:46.891+0000] {processor.py:161} INFO - Started process (PID=84900) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:46.893+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:46.895+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:46.894+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:46.944+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:46.938+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:46.946+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:46.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.077 seconds
[2024-05-10T16:10:54.637+0000] {processor.py:161} INFO - Started process (PID=84983) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:54.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:10:54.640+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:54.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:54.689+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:10:54.681+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:10:54.690+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:10:54.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.077 seconds
[2024-05-10T16:11:03.431+0000] {processor.py:161} INFO - Started process (PID=85072) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:03.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:11:03.434+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:03.434+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:03.479+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:03.472+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:03.480+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:03.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.071 seconds
[2024-05-10T16:11:18.326+0000] {processor.py:161} INFO - Started process (PID=85155) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:18.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:11:18.330+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:18.330+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:18.381+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:18.373+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:18.382+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:18.396+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.080 seconds
[2024-05-10T16:11:30.394+0000] {processor.py:161} INFO - Started process (PID=85244) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:30.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:11:30.396+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:30.396+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:30.442+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:30.433+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:30.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:30.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.070 seconds
[2024-05-10T16:11:41.561+0000] {processor.py:161} INFO - Started process (PID=85328) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:41.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:11:41.566+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:41.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:41.646+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:41.631+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:41.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:41.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.128 seconds
[2024-05-10T16:11:52.793+0000] {processor.py:161} INFO - Started process (PID=85411) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:52.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:11:52.797+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:52.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:52.853+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:11:52.843+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 56, in <module>
    etl_mysql_to_hive_all_2()
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py", line 4081, in factory
    f(**f_kwargs)
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 13, in etl_mysql_to_hive_all_2
    list_table = Variable.get("mysql_to_hive", deserialize_json=True)['tables']
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/variable.py", line 146, in get
    obj = json.loads(var_val)
          ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
               ^^^^^^^^^^^^^^^^^^^^^^
json.decoder.JSONDecodeError: Expecting ':' delimiter: line 1 column 22 (char 21)
[2024-05-10T16:11:52.855+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:11:52.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.091 seconds
[2024-05-10T16:12:04.760+0000] {processor.py:161} INFO - Started process (PID=85499) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:04.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:04.764+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:04.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:04.789+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:04.788+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:04.791+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:04.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:12:16.325+0000] {processor.py:161} INFO - Started process (PID=85582) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:16.327+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:16.328+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:16.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:16.350+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:16.349+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:16.352+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:16.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:12:27.035+0000] {processor.py:161} INFO - Started process (PID=85666) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:27.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:27.039+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:27.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:27.065+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:27.063+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:27.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:27.094+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.069 seconds
[2024-05-10T16:12:37.603+0000] {processor.py:161} INFO - Started process (PID=85755) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:37.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:37.608+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:37.607+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:37.647+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:37.645+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:37.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:37.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.095 seconds
[2024-05-10T16:12:46.083+0000] {processor.py:161} INFO - Started process (PID=85838) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:46.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:46.086+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:46.085+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:46.110+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:46.108+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:46.111+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:46.139+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.068 seconds
[2024-05-10T16:12:53.923+0000] {processor.py:161} INFO - Started process (PID=85921) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:53.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:12:53.926+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:53.926+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:53.955+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:12:53.953+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:12:53.957+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:12:53.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.081 seconds
[2024-05-10T16:13:02.975+0000] {processor.py:161} INFO - Started process (PID=86012) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:02.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:02.980+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:02.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:03.009+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:03.007+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:03.011+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:03.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:13:13.167+0000] {processor.py:161} INFO - Started process (PID=86095) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:13.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:13.171+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:13.170+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:13.199+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:13.197+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:13.200+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:13.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:13:24.101+0000] {processor.py:161} INFO - Started process (PID=86178) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:24.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:24.116+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:24.115+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:24.149+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:24.147+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:24.151+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:24.185+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.112 seconds
[2024-05-10T16:13:31.855+0000] {processor.py:161} INFO - Started process (PID=86267) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:31.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:31.858+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:31.858+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:31.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:31.882+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:31.885+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:31.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:13:39.423+0000] {processor.py:161} INFO - Started process (PID=86350) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:39.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:39.426+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:39.426+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:39.454+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:39.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:39.456+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:39.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.070 seconds
[2024-05-10T16:13:48.785+0000] {processor.py:161} INFO - Started process (PID=86433) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:48.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:48.787+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:48.787+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:48.813+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:48.811+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:48.814+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:48.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.073 seconds
[2024-05-10T16:13:57.016+0000] {processor.py:161} INFO - Started process (PID=86516) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:57.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:13:57.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:57.019+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:57.043+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:13:57.042+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:13:57.044+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:13:57.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:14:05.811+0000] {processor.py:161} INFO - Started process (PID=86605) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:05.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:05.814+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:05.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:05.839+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:05.838+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:05.840+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:05.867+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:14:13.310+0000] {processor.py:161} INFO - Started process (PID=86688) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:13.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:13.314+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:13.313+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:13.344+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:13.343+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:13.346+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:13.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T16:14:24.374+0000] {processor.py:161} INFO - Started process (PID=86771) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:24.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:24.377+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:24.377+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:24.411+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:24.409+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:24.412+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:24.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:14:33.007+0000] {processor.py:161} INFO - Started process (PID=86861) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:33.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:33.010+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:33.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:33.035+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:33.033+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:33.036+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:33.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.071 seconds
[2024-05-10T16:14:42.338+0000] {processor.py:161} INFO - Started process (PID=86944) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:42.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:42.341+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:42.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:42.362+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:42.360+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:42.363+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:42.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.056 seconds
[2024-05-10T16:14:50.231+0000] {processor.py:161} INFO - Started process (PID=87028) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:50.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:50.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:50.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:50.260+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:50.259+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:50.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:50.287+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.066 seconds
[2024-05-10T16:14:58.790+0000] {processor.py:161} INFO - Started process (PID=87111) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:58.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:14:58.795+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:58.794+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:58.832+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:14:58.831+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:14:58.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:14:58.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.118 seconds
[2024-05-10T16:15:08.328+0000] {processor.py:161} INFO - Started process (PID=87201) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:08.329+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:08.331+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:08.330+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:08.353+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:08.352+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:08.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:08.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.057 seconds
[2024-05-10T16:15:16.013+0000] {processor.py:161} INFO - Started process (PID=87285) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:16.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:16.016+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:16.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:16.041+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:16.040+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:16.043+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:16.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.068 seconds
[2024-05-10T16:15:23.960+0000] {processor.py:161} INFO - Started process (PID=87369) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:23.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:23.963+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:23.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:23.987+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:23.986+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:23.988+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:24.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.064 seconds
[2024-05-10T16:15:32.178+0000] {processor.py:161} INFO - Started process (PID=87458) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:32.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:32.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:32.181+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:32.206+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:32.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:32.207+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:32.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.070 seconds
[2024-05-10T16:15:42.797+0000] {processor.py:161} INFO - Started process (PID=87541) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:42.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:42.800+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:42.799+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:42.821+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:42.819+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:42.822+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:42.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.056 seconds
[2024-05-10T16:15:50.822+0000] {processor.py:161} INFO - Started process (PID=87624) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:50.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:50.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:50.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:50.847+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:50.846+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:50.847+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:50.871+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.055 seconds
[2024-05-10T16:15:58.049+0000] {processor.py:161} INFO - Started process (PID=87708) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:58.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:15:58.052+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:58.051+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:58.078+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:15:58.076+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:15:58.080+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:15:58.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.069 seconds
[2024-05-10T16:16:05.653+0000] {processor.py:161} INFO - Started process (PID=87797) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:05.654+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:05.656+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:05.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:05.681+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:05.679+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:05.682+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:05.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.067 seconds
[2024-05-10T16:16:13.415+0000] {processor.py:161} INFO - Started process (PID=87880) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:13.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:13.419+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:13.418+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:13.446+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:13.444+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:13.447+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:13.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:16:22.624+0000] {processor.py:161} INFO - Started process (PID=87963) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:22.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:22.628+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:22.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:22.660+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:22.658+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:22.661+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:22.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T16:16:30.307+0000] {processor.py:161} INFO - Started process (PID=88052) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:30.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:30.310+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:30.309+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:30.338+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:30.336+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:30.339+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:30.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.069 seconds
[2024-05-10T16:16:39.149+0000] {processor.py:161} INFO - Started process (PID=88135) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:39.150+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:39.160+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:39.159+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:39.188+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:39.184+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:39.190+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:39.231+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.094 seconds
[2024-05-10T16:16:49.478+0000] {processor.py:161} INFO - Started process (PID=88218) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:49.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:49.481+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:49.481+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:49.506+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:49.504+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:49.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:49.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:16:58.313+0000] {processor.py:161} INFO - Started process (PID=88301) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:58.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:16:58.316+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:58.315+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:58.346+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:16:58.344+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:16:58.348+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:16:58.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.073 seconds
[2024-05-10T16:17:06.435+0000] {processor.py:161} INFO - Started process (PID=88390) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:06.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:06.439+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:06.439+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:06.467+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:06.465+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:06.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:06.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.104 seconds
[2024-05-10T16:17:13.177+0000] {processor.py:161} INFO - Started process (PID=88473) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:13.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:13.181+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:13.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:13.211+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:13.209+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:13.212+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:13.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T16:17:20.225+0000] {processor.py:161} INFO - Started process (PID=88556) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:20.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:20.228+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:20.227+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:20.247+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:20.246+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:20.248+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:20.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.057 seconds
[2024-05-10T16:17:31.227+0000] {processor.py:161} INFO - Started process (PID=88645) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:31.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:31.234+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:31.233+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:31.265+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:31.262+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:31.267+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:31.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.104 seconds
[2024-05-10T16:17:41.589+0000] {processor.py:161} INFO - Started process (PID=88728) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:41.591+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:41.592+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:41.592+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:41.620+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:41.619+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:41.621+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:41.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.070 seconds
[2024-05-10T16:17:49.699+0000] {processor.py:161} INFO - Started process (PID=88812) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:49.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:49.702+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:49.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:49.728+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:49.726+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:49.729+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:49.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.084 seconds
[2024-05-10T16:17:59.064+0000] {processor.py:161} INFO - Started process (PID=88895) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:59.068+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:17:59.070+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:59.069+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:59.104+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:17:59.102+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:17:59.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:17:59.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:18:08.010+0000] {processor.py:161} INFO - Started process (PID=88985) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:08.011+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:08.012+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:08.012+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:08.034+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:08.033+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:08.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:08.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.055 seconds
[2024-05-10T16:18:17.719+0000] {processor.py:161} INFO - Started process (PID=89070) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:17.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:17.723+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:17.722+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:17.749+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:17.748+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:17.750+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:17.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.073 seconds
[2024-05-10T16:18:26.065+0000] {processor.py:161} INFO - Started process (PID=89155) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:26.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:26.068+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:26.067+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:26.097+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:26.096+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:26.098+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:26.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.071 seconds
[2024-05-10T16:18:36.894+0000] {processor.py:161} INFO - Started process (PID=89245) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:36.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:36.898+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:36.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:36.928+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:36.926+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:36.929+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:36.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.100 seconds
[2024-05-10T16:18:45.654+0000] {processor.py:161} INFO - Started process (PID=89330) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:45.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:45.656+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:45.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:45.675+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:45.673+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:45.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:45.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.061 seconds
[2024-05-10T16:18:53.302+0000] {processor.py:161} INFO - Started process (PID=89414) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:53.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:18:53.305+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:53.305+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:53.326+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:18:53.325+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:18:53.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:18:53.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.061 seconds
[2024-05-10T16:19:00.740+0000] {processor.py:161} INFO - Started process (PID=89504) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:00.741+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:00.743+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:00.742+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:00.767+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:00.766+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:00.769+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:00.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.067 seconds
[2024-05-10T16:19:10.875+0000] {processor.py:161} INFO - Started process (PID=89588) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:10.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:10.878+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:10.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:10.903+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:10.901+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:10.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:10.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.067 seconds
[2024-05-10T16:19:22.173+0000] {processor.py:161} INFO - Started process (PID=89672) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:22.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:22.175+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:22.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:22.198+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:22.196+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:22.199+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:22.226+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.066 seconds
[2024-05-10T16:19:30.640+0000] {processor.py:161} INFO - Started process (PID=89762) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:30.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:30.643+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:30.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:30.669+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:30.668+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:30.670+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:30.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.068 seconds
[2024-05-10T16:19:39.424+0000] {processor.py:161} INFO - Started process (PID=89846) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:39.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:39.427+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:39.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:39.451+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:39.450+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:39.452+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:39.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.062 seconds
[2024-05-10T16:19:46.534+0000] {processor.py:161} INFO - Started process (PID=89930) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:46.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:46.539+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:46.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:46.568+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:46.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:46.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:46.605+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.085 seconds
[2024-05-10T16:19:55.213+0000] {processor.py:161} INFO - Started process (PID=90014) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:55.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:19:55.216+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:55.215+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:55.241+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:19:55.239+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:19:55.242+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:19:55.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T16:20:02.367+0000] {processor.py:161} INFO - Started process (PID=90104) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:02.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:02.370+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:02.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:02.403+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:02.401+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:02.404+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:02.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:20:10.449+0000] {processor.py:161} INFO - Started process (PID=90188) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:10.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:10.453+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:10.452+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:10.481+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:10.479+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:10.482+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:10.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.068 seconds
[2024-05-10T16:20:19.416+0000] {processor.py:161} INFO - Started process (PID=90272) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:19.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:19.421+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:19.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:19.452+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:19.450+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:19.453+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:19.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.101 seconds
[2024-05-10T16:20:30.143+0000] {processor.py:161} INFO - Started process (PID=90362) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:30.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:30.159+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:30.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:30.213+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:30.211+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:30.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:30.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.181 seconds
[2024-05-10T16:20:39.490+0000] {processor.py:161} INFO - Started process (PID=90447) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:39.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:39.492+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:39.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:39.513+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:39.511+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:39.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:39.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.057 seconds
[2024-05-10T16:20:47.986+0000] {processor.py:161} INFO - Started process (PID=90531) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:47.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:47.989+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:47.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:48.013+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:48.012+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:48.014+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:48.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.069 seconds
[2024-05-10T16:20:55.386+0000] {processor.py:161} INFO - Started process (PID=90616) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:55.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:20:55.389+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:55.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:55.412+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:20:55.411+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:20:55.413+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:20:55.438+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.061 seconds
[2024-05-10T16:21:04.052+0000] {processor.py:161} INFO - Started process (PID=90706) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:04.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:04.055+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:04.055+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:04.084+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:04.082+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:04.085+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:04.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.085 seconds
[2024-05-10T16:21:13.728+0000] {processor.py:161} INFO - Started process (PID=90791) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:13.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:13.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:13.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:13.767+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:13.764+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:13.768+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:13.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:21:26.435+0000] {processor.py:161} INFO - Started process (PID=90875) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:26.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:26.438+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:26.437+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:26.465+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:26.463+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:26.466+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:26.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
[2024-05-10T16:21:34.341+0000] {processor.py:161} INFO - Started process (PID=90966) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:34.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:34.347+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:34.346+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:34.378+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:34.376+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:34.380+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:34.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.097 seconds
[2024-05-10T16:21:44.353+0000] {processor.py:161} INFO - Started process (PID=91050) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:44.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:44.357+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:44.356+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:44.387+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:44.385+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:44.388+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:44.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.076 seconds
[2024-05-10T16:21:52.576+0000] {processor.py:161} INFO - Started process (PID=91135) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:52.577+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:21:52.579+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:52.578+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:52.603+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:21:52.602+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:21:52.604+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:21:52.631+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.064 seconds
[2024-05-10T16:22:00.065+0000] {processor.py:161} INFO - Started process (PID=91219) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:00.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:00.068+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:00.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:00.093+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:00.091+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:00.094+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:00.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.072 seconds
[2024-05-10T16:22:08.726+0000] {processor.py:161} INFO - Started process (PID=91309) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:08.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:08.729+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:08.729+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:08.752+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:08.751+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:08.754+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:08.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:22:19.056+0000] {processor.py:161} INFO - Started process (PID=91393) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:19.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:19.062+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:19.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:19.089+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:19.088+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:19.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:19.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.082 seconds
[2024-05-10T16:22:27.426+0000] {processor.py:161} INFO - Started process (PID=91477) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:27.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:27.430+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:27.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:27.457+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:27.456+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:27.458+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:27.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.071 seconds
[2024-05-10T16:22:36.070+0000] {processor.py:161} INFO - Started process (PID=91568) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:36.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:36.075+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:36.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:36.107+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:36.105+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:36.108+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:36.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.094 seconds
[2024-05-10T16:22:45.428+0000] {processor.py:161} INFO - Started process (PID=91652) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:45.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:45.432+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:45.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:45.458+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:45.456+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:45.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:45.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.085 seconds
[2024-05-10T16:22:56.703+0000] {processor.py:161} INFO - Started process (PID=91737) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:56.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:22:56.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:56.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:56.739+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:22:56.737+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:22:56.740+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:22:56.773+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.083 seconds
[2024-05-10T16:23:05.127+0000] {processor.py:161} INFO - Started process (PID=91828) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:05.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:05.132+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:05.131+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:05.163+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:05.161+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:05.164+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:05.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.079 seconds
[2024-05-10T16:23:12.967+0000] {processor.py:161} INFO - Started process (PID=91912) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:12.969+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:12.971+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:12.970+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:12.996+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:12.994+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:12.997+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:13.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.075 seconds
[2024-05-10T16:23:21.302+0000] {processor.py:161} INFO - Started process (PID=91996) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:21.304+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:21.306+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:21.305+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:21.327+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:21.326+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:21.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:21.357+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:23:30.069+0000] {processor.py:161} INFO - Started process (PID=92080) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:30.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:30.072+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:30.072+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:30.108+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:30.106+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:30.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:30.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.081 seconds
[2024-05-10T16:23:38.630+0000] {processor.py:161} INFO - Started process (PID=92171) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:38.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:38.633+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:38.632+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:38.659+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:38.658+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:38.660+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:38.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:23:46.321+0000] {processor.py:161} INFO - Started process (PID=92255) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:46.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:46.324+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:46.323+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:46.347+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:46.346+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:46.349+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:46.376+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.066 seconds
[2024-05-10T16:23:53.552+0000] {processor.py:161} INFO - Started process (PID=92339) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:53.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:23:53.555+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:53.555+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:53.577+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:23:53.575+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:23:53.578+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:23:53.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.061 seconds
[2024-05-10T16:24:04.879+0000] {processor.py:161} INFO - Started process (PID=92429) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:04.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:04.883+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:04.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:04.914+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:04.912+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:04.915+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:04.953+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.091 seconds
[2024-05-10T16:24:14.243+0000] {processor.py:161} INFO - Started process (PID=92514) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:14.245+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:14.246+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:14.246+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:14.272+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:14.270+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:14.273+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:14.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.067 seconds
[2024-05-10T16:24:23.042+0000] {processor.py:161} INFO - Started process (PID=92598) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:23.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:23.045+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:23.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:23.065+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:23.063+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:23.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:23.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.060 seconds
[2024-05-10T16:24:30.993+0000] {processor.py:161} INFO - Started process (PID=92688) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:30.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:30.997+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:30.996+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:31.020+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:31.018+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:31.021+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:31.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.065 seconds
[2024-05-10T16:24:44.003+0000] {processor.py:161} INFO - Started process (PID=92772) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:44.006+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:44.008+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:44.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:44.042+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:44.040+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:44.044+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:44.089+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.105 seconds
[2024-05-10T16:24:53.227+0000] {processor.py:161} INFO - Started process (PID=92857) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:53.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:24:53.232+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:53.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:53.262+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:24:53.260+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:24:53.264+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:24:53.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.087 seconds
[2024-05-10T16:25:01.518+0000] {processor.py:161} INFO - Started process (PID=92948) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:01.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:25:01.523+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:01.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:01.562+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:01.559+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:25:01.564+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:01.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.110 seconds
[2024-05-10T16:25:13.702+0000] {processor.py:161} INFO - Started process (PID=93032) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:13.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:25:13.706+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:13.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:13.732+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:25:13.731+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:25:13.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:25:13.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.070 seconds
[2024-05-10T16:27:15.691+0000] {processor.py:161} INFO - Started process (PID=94053) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:15.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:15.695+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:15.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:15.726+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:15.724+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:15.728+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:15.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.095 seconds
[2024-05-10T16:27:16.597+0000] {processor.py:161} INFO - Started process (PID=94059) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:16.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:16.603+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:16.602+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:16.678+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:16.675+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:16.684+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:16.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.153 seconds
[2024-05-10T16:27:26.769+0000] {processor.py:161} INFO - Started process (PID=94144) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:26.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:26.778+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:26.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:26.825+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:26.822+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:26.827+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:26.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.118 seconds
[2024-05-10T16:27:36.702+0000] {processor.py:161} INFO - Started process (PID=94234) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:36.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:36.705+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:36.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:36.727+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:36.726+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:36.729+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:36.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.061 seconds
[2024-05-10T16:27:45.913+0000] {processor.py:161} INFO - Started process (PID=94319) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:45.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:45.916+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:45.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:45.947+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:45.945+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:45.949+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:45.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.092 seconds
[2024-05-10T16:27:58.567+0000] {processor.py:161} INFO - Started process (PID=94404) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:58.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:27:58.570+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:58.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:58.595+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:27:58.594+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:27:58.596+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:27:58.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.064 seconds
[2024-05-10T16:28:07.483+0000] {processor.py:161} INFO - Started process (PID=94494) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:07.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:28:07.486+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:07.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:07.507+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:07.506+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:28:07.508+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:07.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.056 seconds
[2024-05-10T16:28:17.957+0000] {processor.py:161} INFO - Started process (PID=94578) to work on /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:17.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_mysql_to_hive_all_2.py for tasks to queue
[2024-05-10T16:28:17.960+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:17.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:17.988+0000] {logging_mixin.py:188} INFO - [2024-05-10T16:28:17.986+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_mysql_to_hive_all_2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_mysql_to_hive_all_2.py", line 34
    (% for row in ti.xcom_pull(task_ids='transform_{table}_with_python') %)
     ^
SyntaxError: f-string: invalid syntax
[2024-05-10T16:28:17.989+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_mysql_to_hive_all_2.py
[2024-05-10T16:28:18.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_mysql_to_hive_all_2.py took 0.074 seconds
